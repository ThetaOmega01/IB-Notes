\documentclass[a4paper]{article}
\renewcommand{\epsilon}{\varepsilon}
\newcommand{\triposcourse}{Numerical Analysis}
\input{../header.tex}
\counterwithin{equation}{section}
\graphicspath{ {./images/} }
\pgfplotsset{compat=1.17}
\begin{document}
\maketitle
\tableofcontents
\clearpage

% \include{NA_Intro.tex}

\section{Polynomial interpolation}
\subsection{Lagrange and Newton polynomials}
Let $f:[a, b] \rightarrow \mathbb{R}$ be a real-valued continuous function defined on some interval $[a, b]$ and let $(x_{i})_{i=0}^{n}$ be $n+1$ distinct points in $[a, b]$. We wish to construct a polynomial $p$ of degree $n$ which interpolates $f$ at these points, i.e., satisfies
$$
p(x_{i})=f(x_{i}), \quad i=\overline{0, n}, \quad p \in \mathcal{P}_{n} .
$$
\begin{theorem}[Existence and uniqueness]
    Given $f \in C[a, b]$ and $n+1$ distinct points $(x_{i})_{i=0}^{n} \in[a, b]$, there is exactly one polynomial $p \in \mathcal{P}_{n}$ such that $p(x_{i})=f(x_{i})$ for all $i$.
\end{theorem}
\begin{proof}
    (Existence) There is at least one polynomial interpolant $p \in \mathcal{P}_{n}$, the one in the \textbf{Lagrange form},
    \begin{equation}\label{eqn:Lagrange 1}
        p(x)=\sum_{i=0}^{n} f(x_{i}) \ell_{i}(x) \quad \text { with } \quad \ell_{i}(x):=\prod_{\substack{j=0 \\ j \neq i}}^{n} \frac{x-x_{j}}{x_{i}-x_{j}}, \quad i=\overline{0, n},
    \end{equation}
    the $\ell_{i}$ s are called the fundamental Lagrange polynomials. Each $\ell_{i}$ is the product of $n$ linear factors, hence $\ell_{i} \in \mathcal{P}_{n}$. It also equals 1 at $x_{i}$ and vanishes at $x_{j} \neq x_{i}$, i.e., $\ell_{i}(x_{j})=\delta_{i j}$. Therefore $p \in \mathcal{P}_{n}$ and
    $$
    p(x_{j})=\sum_{i=0}^{n} f(x_{i}) \ell_{i}(x_{j})=f(x_{j}) .
    $$
    (Uniqueness) There is at most one polynomial interpolant $p \in \mathcal{P}_{n}$ to $f$ on $(x_{i})_{i=0}^{n}$. For if there are two, $p, q \in \mathcal{P}_{n}$, then the polynomial $r:=p-q$ is of degree $n$ and vanishes at $n+1$ points, whence $r \equiv 0$.
\end{proof}
\begin{remark}
    Let us introduce the so-called \textbf{nodal polynomial}
    $$
    \omega(x):=\prod_{i=0}^{n}(x-x_{i}) .
    $$
    Then, in the expression for $\ell_{i}$, the numerator is simply $\omega(x) /(x-x_{i})$ while the denominator is equal to $\omega^{\prime}(x_{i})$. With that we arrive to a \textbf{compact Lagrange form}
    \begin{equation}\label{eqn:Lagrange 2}
        p(x)=\sum_{i=0}^{n} f(x_{i}) \ell_{i}(x)=\sum_{i=0}^{n} \frac{f(x_{i})}{\omega^{\prime}(x_{i})} \frac{\omega(x)}{x-x_{i}}
    \end{equation}
    The Lagrange forms for the interpolating polynomials are easy to manipulate, but they are unsuitable for numerical evaluation. An alternative is the \textbf{Newton form} which has an \textit{adaptive} nature.
\end{remark}

\begin{method}[The Newton form]
    For $k=\overline{0,n}$, let $p_{k} \in \mathcal{P}_{k}$ be the polynomial interpolant to $f$ on $x_{0}, \ldots, x_{k}$. Then two subsequent $p_{k-1}$ and $p_{k}$ interpolate the same values $f(x_{i})$ for $i \leq k-1$, hence their difference is a polynomial of degree $k$ that vanishes at $k$ points $x_{0}, \ldots, x_{k-1}$. Thus
    \begin{equation}\label{eqn:Newton 1}
        p_{k}(x)-p_{k-1}(x)=A_{k} \prod_{i=0}^{k-1}(x-x_{i}),
    \end{equation}
    with some constant $A_{k}$ which is seen to be equal to the leading coefficient of $p_{k}$. It follows that $p:=p_{n}$ can be built step by step as one constructs the sequence $(p_{0}, p_{1}, \ldots)$, with $p_{k}$ obtained from $p_{k-1}$ by addition the term from the right-hand side of above, so that finally
    $$
    p(x):=p_{n}(x)=p_{0}(x)+\sum_{k=1}^{n}[p_{k}(x)-p_{k-1}(x)]=\sum_{k=0}^{n} A_{k} \prod_{i=0}^{k-1}(x-x_{i})
    $$
\end{method}
\begin{definition}[Divided difference]
    Given $f \in C[a, b]$ and $k+1$ distinct points $(x_{i})_{i=0}^{k} \in[a, b]$, the \textbf{divided difference} $f[x_{0}, \ldots, x_{k}]$ \textbf{of order} $k$ is the leading coefficient of the polynomial $p_{k} \in \mathcal{P}_{k}$ which interpolates $f$ at these points. By definition, it is a symmetric function of the variables $[x_{0}, \ldots, x_{k}]$, and if $f(x)=x^{m}, m \leq k$, then $f[x_{0}, \ldots, x_{k}]=\delta_{k m}$
\end{definition}
With this definition we arrive at the Newton formula for the interpolating polynomial.
\begin{theorem}[Newton formula]
    Given $n+1$ distinct points $(x_{i})_{i=0}^{n}$, let $p_{n} \in \mathcal{P}_{n}$ be the polynomial that interpolates $f$ at these points. Then it may be written in the \textbf{Newton form}
   \begin{align*}
    p_{n}(x)&=f[x_{0}]+f[x_{0}, x_{1}](x-x_{0})+f[x_{0}, x_{1}, x_{2}](x-x_{0})(x-x_{1})+\cdots \\
    &\cdots+f[x_{0}, x_{1}, \ldots, x_{n}](x-x_{0})(x-x_{1}) \cdots(x-x_{n-1}),
   \end{align*}
   or, more compactly,
   \begin{equation}\label{eqn:Divided difference}
    p_{n}(x)=\sum_{k=0}^{n} f[x_{0}, \ldots, x_{k}] \prod_{i=0}^{k-1}(x-x_{i})
   \end{equation}
\end{theorem}
To make this formula of any use, we need an expression for $f[x_{0}, \ldots, x_{k}]$. One such can be derived from the Lagrange formula (\ref{eqn:Lagrange 2}) by identifying the leading coefficient of $p$. This turns to be
\[
    f[x_{0}, \ldots, x_{n}]=\sum_{i=0}^{n} \frac{f(x_{i})}{\omega^{\prime}(x_{i})}, \quad \omega(x):=\prod_{i=0}^{n}(x-x_{i}) .
\]
However, this expression has computational disadvantages as the Lagrange form itself. A useful way to calculate divided difference is again an adaptive (or recurrence) approach.
\begin{theorem}[Recurrence relation]
    For distinct $x_{0}, x_{1}, \ldots, x_{k}$ (with $k \geq 1$), we have
    \begin{equation}\label{eqn:Recurrence relation}
        f[x_{0}, \ldots, x_{k}]=\frac{f[x_{1}, \ldots, x_{k}]-f[x_{0}, \ldots, x_{k-1}]}{x_{k}-x_{0}}
    \end{equation}
\end{theorem}
\begin{proof}
    Let $q_{0}, q_{1} \in \mathcal{P}_{k-1}$ be the polynomials such that 
    \begin{align*}
        & q_0 \text{ interpolates } f \text{ on } (x_0,x_1,\dots,x_{k-1})\\ 
        & q_1 \text{ interpolates } f \text{ on } ~ ~ ~ ~ ~ (x_1,\dots,x_{k-1},x_k)
    \end{align*}
    and consider the polynomial
    \[
        p(x):=\frac{x-x_{0}}{x_{k}-x_{0}} q_{1}(x)+\frac{x_{k}-x}{x_{k}-x_{0}} q_{0}(x), \quad p \in \mathcal{P}_{k}.
    \]
    One readily sees that $p(x_{i})=f(x_{i})$ for all $i$, hence, $p$ is the $k$-th degree interpolating polynomial for $f$. Moreover, the leading coefficient of $p$ is equal to the difference of those of $q_{1}$ and $q_{0}$ divided by $x_{k}-x_{0}$, and that is exactly what the recurrence (\ref{eqn:Recurrence relation}) says.
\end{proof}
\begin{method}
    The recursive formula (\ref{eqn:Recurrence relation}) allows for fast evaluation of the divided difference table
    \begin{center}
        \begin{tikzpicture}
            \matrix(ddtable)[matrix of nodes,
                nodes={anchor=center}
            ]{
                \hline
                $x_i$ & $f[*] = f(*)$ & $f[*,*]$ &[1em] $f[*,*,*]$ &[1em] $f[*,*,*,*]$ \\ \hline
                $x_0$ & $f[x_0]$ &  & &\\ 
                & & $f[x_0,x_1]$ & & \\ 
                $x_1$ & $f[x_1]$ & & $f[x_0,x_1,x_2]$ & \\ 
                & & $f[x_1, x_2]$ & & $f[x_0,x_1,x_2,x_3]$ \\ 
                $x_2$ & $f[x_2]$ & & $f[x_1, x_2, x_3]$ & \\
                & & $f[x_2, x_3]$ & &\\
                $x_3$ & $f[x_3]$ & & & \\ \hline
                & & & & \\
            };
            \draw[-to] (ddtable-2-1) -- (ddtable-2-2);
            \draw[-to] (ddtable-4-1) -- (ddtable-4-2);
            \draw[-to] (ddtable-6-1) -- (ddtable-6-2);
            \draw[-to] (ddtable-8-1) -- (ddtable-8-2);
            \draw[-to] (ddtable-2-2) -- (ddtable-3-3);
            \draw[-to] (ddtable-4-2) -- (ddtable-3-3);
            \draw[-to] (ddtable-4-2) -- (ddtable-5-3);
            \draw[-to] (ddtable-6-2) -- (ddtable-5-3);
            \draw[-to] (ddtable-6-2) -- (ddtable-7-3);
            \draw[-to] (ddtable-8-2) -- (ddtable-7-3);
            \draw[-to] (ddtable-3-3) -- (ddtable-4-4);
            \draw[-to] (ddtable-5-3) -- (ddtable-4-4);
            \draw[-to] (ddtable-5-3) -- (ddtable-6-4);
            \draw[-to] (ddtable-7-3) -- (ddtable-6-4);
            \draw[-to] (ddtable-4-4) -- (ddtable-5-5);
            \draw[-to] (ddtable-6-4) -- (ddtable-5-5);
        \end{tikzpicture}
      \end{center}
    \end{method}
This can be done in $\mathcal{O}(n^{2})$ operations, the outcome is the numbers $\{f[x_{0}, \ldots, x_{k}]\}_{k=0}^{n}$ at the head of the columns which can be used in the Newton form (\ref{eqn:Newton 1}).
\begin{method}[Horner scheme]
    Finally, evaluation of $p$ at a given point $x$ using Newton formula (provided that divided differences $A_{k}:=f[x_{0}, \ldots x_{k}]$ are known) requires just $n$ multiplications, as long as we do it by the \textbf{Horner scheme}
    \begin{align*}
        p_{n}(x)=\{\cdots\{\{A_{n} &\times(x-x_{n-1})+A_{n-1}\} \\ 
        &\times(x-x_{n-2})+A_{n-2}\} \\ 
        &\times(x-x_{n-3})+\cdots+A_{1}\} \times(x-x_{0})+A_{0}.
    \end{align*}
\end{method}
In practice, it may be written as 
\begin{alltt}
S <- f[\(\mathtt{x\sb{0}}\),..., \(\mathtt{x\sb{n}}\)]
for k = n - 1,..., 0
    S <- (\(\mathtt{\hat{x}}\) - \(\mathtt{x\sb{k}}\))S + f[\(\mathtt{x\sb{0}}\),..., \(\mathtt{x\sb{k}}\)]
end\end{alltt}
\subsection{Examples}
The next example derives interpolation polynomials of a function using different methods. 
\begin{example}
    Given the data

\begin{center}
\begin{tabular}{crrrr}
    \toprule
$x_{i}$ & 0 & 1 & 2 & 3 \\
\midrule
$f\left(x_{i}\right)$ & $-3$ & $-3$ & $-1$ & 9 \\\bottomrule
\end{tabular}
\end{center}

find the interpolating polynomial $p \in \mathcal{P}_{3}$ in both Lagrange and Newton forms.
\end{example}
1a) Fundamental Lagrange polynomials
\begin{IEEEeqnarray*}{cCcCrl}
    \ell_{0}(x)&=&\tfrac{(x-1)(x-2)(x-3)}{-6}&=& -&\tfrac{1}{6}\left(x^{3}-6 x^{2}+11 x-6\right), \\
    \ell_{1}(x)&=&\tfrac{x(x-2)(x-3)}{2}&=&&\tfrac{1}{2}\left(x^{3}-5 x^{2}+6 x\right), \\
    \ell_{2}(x)&=&\tfrac{x(x-1)(x-3)}{-2}&=&-&\tfrac{1}{2}\left(x^{3}-4 x^{2}+3 x\right), \\
    \ell_{3}(x)&=&\tfrac{x(x-1)(x-2)}{6}&=&&\tfrac{1}{6}\left(x^{3}-3 x^{2}+2 x\right) .
\end{IEEEeqnarray*}
1b) Lagrange form:
$$
\begin{aligned}
p(x) & =\textstyle(-3) \cdot \ell_{0}(x)+(-3) \cdot \ell_{1}(x)+(-1) \cdot \ell_{2}(x)+9 \cdot \ell_{3}(x) \\
& =\textstyle\left(\frac{1}{2}-\frac{3}{2}+\frac{1}{2}+\frac{3}{2}\right) x^{3}+\left(-3+\frac{15}{2}-2-\frac{9}{2}\right) x^{2}+\left(\frac{11}{2}-9+\frac{3}{2}+3\right) x-3 \\
& =\textstyle x^{3}-2 x^{2}+x-3 .
\end{aligned}
$$
2a) Divided differences:
\begin{center}
\begin{tikzpicture}
    \matrix(dd)[matrix of nodes,
        nodes={anchor=center}
    ]{
        $\widehat{0}$ & $-\overset{*}{3}$ &[.5em]  &[1em] &[1em]\\ 
        & & $\frac{(-3)-(-3)}{1-0} = \overset{*}{0}$ & & \\ 
        $\widehat{1}$ & $-3$ & & $\frac{2-0}{2-0}= \overset{*}{1}$ & \\ 
        & & $\frac{(-1)-(-3)}{2-1}=2$ & & $\frac{4-1}{3-0}=\overset{*}{1}$ \\ 
        $\widehat{2}$ & $-1$ & & $\frac{10-2}{3-1}=4$ & \\
        & & $\frac{9-(-1)}{3-2}=10$ & &\\
        $\widehat{3}$ & $9$ & & & \\
    };
    \draw (dd-1-1.north east) -- (dd-7-1.south east);
    \draw[-to] (dd-1-2) -- (dd-2-3);
    \draw[-to] (dd-3-2) -- (dd-2-3);
    \draw[-to] (dd-3-2) -- (dd-4-3);
    \draw[-to] (dd-5-2) -- (dd-4-3);
    \draw[-to] (dd-5-2) -- (dd-6-3);
    \draw[-to] (dd-7-2) -- (dd-6-3);
    \draw[-to] (dd-2-3) -- (dd-3-4);
    \draw[-to] (dd-4-3) -- (dd-3-4);
    \draw[-to] (dd-4-3) -- (dd-5-4);
    \draw[-to] (dd-6-3) -- (dd-5-4);
    \draw[-to] (dd-3-4) -- (dd-4-5);
    \draw[-to] (dd-5-4) -- (dd-4-5);
\end{tikzpicture}
\end{center}
2b) Newton form:
$$
p(x)=-\overset{*}{3}+\overset{*}{0} \cdot(x-\widehat{0})+\overset{*}{1} \cdot(x-\widehat{0})(x-\widehat{1})+\overset{*}{1} \cdot(x-\widehat{0})(x-\widehat{1})(x-\widehat{2}) .
$$
2c) Horner scheme:
$$
p(x)=\left\{[\overset{*}{1} \cdot(x-\widehat{2})+\overset{*}{1}] \cdot(x-\widehat{1})+\overset{*}{0}\right\} \cdot(x-\widehat{0})-\overset{*}{3}.
$$
\subsection{Some further formulas}
We estimate error terms and give an expression of $ f[x_0, \dots x_k] $. 
\begin{theorem}
    Let $p_{n} \in \mathcal{P}_{n}$ interpolate $f \in C[a, b]$ at $n+1$ distinct points $x_{0}, \ldots, x_{n}$. Then for any $x \notin\left(x_{i}\right)$
    \begin{equation}\label{eqn:error f-p}
        f(x)-p_{n}(x)=f[x_{0}, \ldots, x_{n}, x] \omega(x)
    \end{equation}
\end{theorem}
\begin{proof}
    Given $x_{0}, . ., x_{n}$, let $\bar{x}:=x_{n+1}$ be any other point. Then, by (\ref{eqn:Newton 1}), the corresponding polynomials $p_{n}$ and $p_{n+1}$ are related by
    $$
    p_{n+1}(x)=p_{n}(x)+f\left[x_{0}, \ldots, x_{n}, \bar{x}\right] \omega(x), \quad \omega(x):=\prod_{i=0}^{n}\left(x-x_{i}\right)
    $$
    In particular, putting $x=\bar{x}$, and noticing that $p_{n+1}(\bar{x})=f(\bar{x})$, we obtain
    $$
    f(\bar{x})=p_{n}(\bar{x})+f\left[x_{0}, \ldots, x_{n}, \bar{x}\right] \omega(\bar{x}),
    $$
    the latter equality being the same as (\ref{eqn:error f-p}).
\end{proof}

This theorem shows the error to be ``like the next term'' in the Newton form. However, we cannot evaluate the right-hand side of (\ref{eqn:error f-p}) without knowing the number $f(x)$. But as we now show we can relate it to the $(n+1)$-st derivative of $f$. For this we need a version of the Rolle's theorem:

\begin{lemma}
    If $g \in C^{k}[a, b]$ is zero at $k+\ell$ distinct points, then $g^{(k)}$ has at least $\ell$ distinct zeros in $[a, b]$.
\end{lemma}
\begin{proof}
    By Rolle's theorem, if $\phi \in C^{1}$ is zero at two points, then $\phi^{\prime}$ is zero at an intermediate point. So, we deduce that $g^{\prime}$ vanishes at least at $(k-1)+\ell$ distinct points. Next, applying Rolle to $g^{\prime}$, we conclude that $g^{\prime \prime}$ vanishes at $(k-2)+\ell$ points, and so on.
\end{proof}
\begin{theorem}
    Let $[\bar{a}, \bar{b}]$ be the smallest interval that contains $x_{0}, \ldots, x_{k}$ and let $f \in C^{k}[\bar{a}, \bar{b}]$. Then there exists $\xi \in[\bar{a}, \bar{b}]$ such that
    \begin{equation}\label{eqn:divided difference taylor}
        f[x_{0}, \ldots, x_{k}]=\frac{1}{k !} f^{(k)}(\xi)
    \end{equation}
\end{theorem}
\begin{proof}
    Let $p \in \mathcal{P}_{k}$ be the interpolating polynomial to $f$ on $\left(x_{i}\right)$. The error function $f-p$ has at least $k+1$ zeros in $[\bar{a}, \bar{b}]$ so, by Rolle's theorem, $f^{(k)}-p^{(k)}$ must vanish at some $\xi \in[\bar{a}, \bar{b}]$, i.e.,
    $$
    p^{(k)}(\xi)=f^{(k)}(\xi)
    $$
    On the other hand, if $p(x)=a_{k} x^{k}+$ (lower order terms), then (for any $\xi$)
    $$
    p^{(k)}(\xi)=k ! a_{n}=: k ! f[x_{0}, \ldots, x_{n}] .
    $$
\end{proof}

\section{Error bounds for polynomial interpolation}
Here we study the \textbf{interpolation error}
\[
    e_n(x) = f(x) - p_n(x),\quad p_n\in \mathcal{P}_n,
\]
for the class of differentiable functions $f$ that possess $n+1$ continuous derivatives on the interval $[a,b]$; we denote this class by $C^{n+1}[a,b]$. 
\begin{theorem}
    Let $f \in C^{n+1}[a, b]$, and let $p_n \in \mathcal{P}_n$ interpolate $f$ at $n+1$ points $\left(x_i\right)_{i=0}^n \in[a, b]$, and let $\omega(x):=\prod_{i=0}^n\left(x-x_i\right)$. Then for every $x \in[a, b]$ there exists $\xi \in[a, b]$ such that
    \begin{equation}\label{eqn:error bound}
        f(x)-p_n(x)=\frac{1}{(n+1) !} \omega(x) f^{(n+1)}(\xi)
    \end{equation}
\end{theorem} 
\begin{proof}
    If $x$ coincides with any $x_i$ from the interpolating set, then both sides vanish, hence the formula trivially holds. So, we let $x$ be any other fixed point, and consider the function
    \[
    \phi(t):=[f(t)-p(t)]-c_x \omega(t),
    \]
    with \textit{some} constant $c_x$. For \textit{any} $c_x, \phi(t)=0$ at $n+1$ points $t=x_2$, and we choose \textit{particular} $c_x$ so that $\phi(t)=0$ at $t=x$ as well, i.e.,
    \[
    c_x:=\frac{f(x)-p(x)}{\omega(x)} .
    \]
    Then $\phi$ has $n+2$ distinct zeros hence, by Rolle's theorem, $\phi^{(n+1)}(\xi)=0$ for some $\xi \in[a, b]$. So,
    \[
    0=\phi^{(n+1)}(\xi)=\left[f^{(n+1)}(\xi)-p^{(n+1)}(\xi)\right]-c_x \omega^{(n+1)}(\xi)=f^{(n+1)}(\xi)-c_x(n+1) !
    \]
    whence
    \[
    c_x:=\frac{f(x)-p(x)}{\omega(x)}=\frac{1}{(n+1) !} f^{(n+1)}(\xi),
    \]
    and that is the same as RHS.
\end{proof}

The equality (\ref{eqn:error bound}) with the value $f^{(n+1)}(\xi)$ for some $\xi$ is of hardly any use. Usually one has a bound for $f^{(n+1)}$ in terms of some norm, e.g., the $L_{\infty}$-norm (the max-norm)
\[
\|g\|_{\infty}:=\|g\|_{L_{\infty}[a, b]}:=\max _{t \in[a, b]}|g(t)| .
\]
Then estimate (\ref{eqn:error bound}) takes the form
\begin{equation}\label{eqn:2.2}
    \left|f(x)-p_n(x)\right| \leq \frac{1}{(n+1) !}|\omega(x)|\left\|f^{(n+1)}\right\|_{\infty}
\end{equation}
If we want to find the maximal error over the interval, then maximizing first the right- and then the left-hand side over $x \in[a, b]$ we get yet one more error bound for polynomial interpolation
\begin{equation}\label{eqn:2.3}
    \left\|f-p_{\Delta}\right\|_{\infty} \leq \frac{1}{(n+1) !}\left\|\omega_{\Delta}\right\|_{\infty}\left\|f^{(n+1)}\right\|_{\infty}
\end{equation}
Here we put the lower index in $\omega_{\Delta}$ in order to emphasize dependence of $\omega(x):=\prod_{i=0}^n\left(x-x_i\right)$ on the sequence of interpolating points $\Delta:=\left(x_i\right)_{i=0}^n$. The choice of $\Delta$ makes a big difference!

\begin{example}
    Consider interpolation of the function $\displaystyle f(x) = \frac{1}{1+x^2},\ x\in [-5,5]$. 
    \begin{center}
        \includegraphics[scale=.85]{NA1}
    \end{center}
\end{example}

\begin{definition}
    The Chebyshev polynomial of degree $n$ on $[-1,1]$ is defined by
    \[
    T_n(x)=\cos n \theta, \quad x=\cos \theta, \quad \theta \in[0, \pi] .
    \]
    (Or just $T_n(x)=\cos (n \arccos x)$, with $x \in[-1,1]$.)
\end{definition}

One sees at once that, on $[-1,1]$,
\begin{enumerate}
    \item  $T_n$ takes its maximal absolute value 1 with alternating signs $n+1$ times:
    \[
    \left\|T_n\right\|_{\infty}=1, \quad T_n\left(t_k\right)=(-1)^k, \quad t_k=\cos \tfrac{\pi k}{n}, \quad k=\overline{0, n}
    \]
    \item $T_n$ has $n$ distinct zeros: $\quad T_n\left(x_k^*\right)=0, \quad x_k^*=\cos \frac{2 k-1}{2 n} \pi, \quad k=\overline{1, n}$.
\end{enumerate}

\begin{lemma}
    The Chebyshev polynomials $T_n$ satisfy the recurrence relation
    \begin{align}
        & T_0(x) \equiv 1, \quad T_1(x)=x, \label{eqn:2.4}\\
        & T_{n+1}(x)=2 x T_n(x)-T_{n-1}(x), \quad n \geq 1 .\label{eqn:2.5}
    \end{align}
    In particular, $T_n$ is an algebraic polynomial of degree $n$ with the leading coefficient $2^{n-1}$.
\end{lemma}
\begin{proof}
    Expressions (\ref{eqn:2.4}) are straightforward, the recurrence follows via the substitution $x=\cos \theta$ into identity $\cos (n+1) \theta+\cos (n-1) \theta=2 \cos \theta \cos n \theta$.
\end{proof}

\begin{theorem}
    On the interoal $[-1,1]$, among all polynomials of degree $n$ with the leading coefficient equal to one, the Chebyshev polynomial $\gamma T_n$ has the smallest max-norm, i.e.,
    \[
    \inf _{\left(a_i\right)}\left\|x^n+a_{n-1} x^{n-1}+\cdots+a_0\right\|_{\infty}=\left\|\gamma T_n\right\|_{\infty}=\gamma, \quad \gamma:=1 / 2^{n-1} .
    \]
\end{theorem}
\begin{proof}
    Suppose there is a polynomial $q_n(x)=x^n+a_{n-1} x^{n-1}+\cdots+a_0$ such that $\|q\|_{\infty}<\gamma$, and set
    \[
    r:=\gamma T_n-q_n .
    \]
    The leading coefficients of both $q_n$ and $\gamma T_n$ are equal 1 , thus $r$ is of degree at most $n-1$.

    Further, at $n+1$ points $t_k:=\cos \frac{\pi k}{n}$, the Chebyshev polynomial $\gamma T_n$ takes the values $\pm \gamma$ alternatively, while by assumption $\left|q_n\left(t_k\right)\right|<\gamma$, hence $r=\gamma T_n-q_n$ alternates in sign at these $n+1$ points, therefore it has a zero in each of $n$ intervals $\left(t_k, t_{k+1}\right)$, i.e. at least $n$ zeros in the interval $[-1,1]$, a contradiction to $r \in \mathcal{P}_{n-1}$.
\end{proof}

\begin{corollary}
    For $\Delta=\left(x_i\right)_{i=0}^n \subset[-1,1]$, let $\omega_{\Delta}(x)=\prod_{i=0}^n\left(x-x_i\right)$. Then, for all $n$, we have
    \[
    \inf _{\Delta}\left\|\omega_{\Delta}\right\|_{\infty}=\left\|\omega_{{\Delta}_{*}}\right\|_{\infty}=1 / 2^n .
    \]
\end{corollary}

\begin{theorem}
    For $f \in C^{n+1}[-1,1]$, the best choice of interpolating points is 
    \[
        \Delta_*=\left(x_i^*\right)=\left(\cos \frac{2 i+1}{2 n+2} \pi\right)_{i=0}^n
    \]
    and
    \[
    \left\|f-p_{\Delta_*}\right\|_{\infty} \leq \frac{1}{2^n} \frac{1}{(n+1) !}\left\|f^{(n+1)}\right\|_{\infty}
    \]
\end{theorem}

\begin{example}
    For $f(x)=e^x$, and $x \in[-1,1]$, the error of approximation provided by interpolating polynomial of degree 9 with 10 Chebyshev knots is bounded by
    \[
    \left|e^x-p_9(x)\right| \leq \frac{1}{2^9} \frac{1}{10 !} e \leq 1.5 \cdot 10^{-9}
    \]
\end{example}

\section{Orthogonal polynomials}
\subsection{The three-term recurrence relation}
Consider $\mathbb{X}=C[a, b]$, the space of all continuous real-valued functions $f:[a, b] \rightarrow \mathbb{R}$, and define a \textbf{scalar} (or \textbf{inner}) \textbf{product} on $C[a, b]$ by
\begin{equation}\label{eqn:3.1}
    (f, g):=(f, g)_w:=\int_a^b f(x) g(x) w(x) d x .
\end{equation}
Here $w$, the so-called \textbf{weight function}, is a fixed positive function, such that the integral $\int h(x) w(x) d x$ exists for all $h \in C[a, b]$.

We denote by $\mathcal{P}_n$ the space of all algebraic polynomials of degree (at most) $n$, i.e., $p \in \mathcal{P}_n$ if $p(x)=\sum_{k=0}^n a_k x^k$. If the leading coefficent $a_n$ equals 1 , then $p$ is called a \textbf{monic} polynomial. Given a scalar product (\ref{eqn:3.1}), we say that $Q_n \in \mathcal{P}_n$ is the $n$\textbf{-th} \textbf{orthogonal polynomial} if
\[
\left(Q_n, p\right)=0 \quad \forall p \in \mathcal{P}_{n-1} .
\]
Different weights lead to different orthogonal polynomials.

\begin{lemma}
    For every $n \in \mathbb{N}$, there exists a unique monic orthogonal polynomial $Q_n \in \mathcal{P}_n$. Any $p \in \mathcal{P}_n$ is uniquely expressible as a linear combination
    \begin{equation}\label{eqn:3.2}
        p=\sum_{k=0}^n c_k Q_k, \quad c_k=\left(p, Q_k\right) /\left\|Q_k\right\|^2
    \end{equation}
\end{lemma}
\begin{proof}
    We apply the Gram-Schmidt orthogonalization algorithm for the linearly independent sequence of monomials $\left(1, x, \ldots, x^n, \ldots\right)$. Starting with $Q_0 \equiv 1$ we set
\[
Q_n(x):=x^n-\sum_{k=0}^{n-1} \frac{\left(x^n, Q_k\right)}{\left(Q_k, Q_k\right)} Q_k(x), \quad n=1,2, \ldots
\]
Then, from construction, $\left(Q_n, Q_k\right)=0$, i.e., $Q_n$ is orthogonal to each previous $Q_k$. Also, we have $x^n \in \operatorname{span}\left(Q_k\right)_{k=0}^n$, hence $\mathcal{P}_n=\operatorname{span}\left(Q_k\right)_{k=0}^n$. Therefore $Q_n \perp \mathcal{P}_{n-1}$, and any $p \in \mathcal{P}_n$ has an expansion (\ref{eqn:3.2}). Each coefficient $c_k$ in (\ref{eqn:3.2}) is uniquely determined by multiplying both sides (in the scalar product sense) with $Q_k$.

If $\widetilde{Q}_n$ is another $n$-th monic orthogonal polynomial, then $p:=Q_n-\widetilde{Q}_n$ belongs to $\mathcal{P}_{n-1}$, therefore $(p, p)=\left(Q_n-\widetilde{Q}_n, p\right)=0$. Hence $p \equiv 0$ (by the scalar product property), i.e., $\widetilde{Q}_n \equiv Q_n$.
\end{proof}
\begin{remark}
    For practical construction of orthogonal polynomials the Gram-Schmidt is not of much help, for it leads to loss of accuracy due to imprecisions in calculation of scalar products. A considerably better procedure is being provided by the next theorem.
\end{remark}

\begin{theorem}[The three-term recurrence relation]
    Monic orthogonal polynomials satisfy the relation
    \begin{equation}\label{eqn:3.3}
        Q_{n+1}(x)=\left(x-a_n\right) Q_n(x)-b_n Q_{n-1}(x), \quad n=0,1, \ldots
    \end{equation}
    where $Q_{-1}(x) \equiv 0, Q_0(x) \equiv 1$, and
    \begin{equation}\label{eqn:3.4}
        a_n=\frac{\left(x Q_n, Q_n\right)}{\left(Q_n, Q_n\right)}, \quad b_n=\frac{\left(Q_n, Q_n\right)}{\left(Q_{n-1}, Q_{n-1}\right)}>0
    \end{equation}
\end{theorem}
\begin{proof}
    Based on (\ref{eqn:3.2}), let us look at the coefficients of the expansion
    \[
    x Q_n(x)=\sum_{k=0}^{n+1} c_k Q_k(x), \quad c_k=\frac{\left(x Q_n, Q_k\right)}{\left(Q_k, Q_k\right)}=\frac{\left(Q_n, x Q_k\right)}{\left(Q_k, Q_k\right)},
    \]
    using the last (equivalent) formula for $c_k$.

    $k=n+1 \to  c_{n+1}=1$, since both $x Q_n(x)$ and $Q_{n+1}(x)$ are monic polynomials of degree $n+1$.

    $k=n \to  c_n=a_n$ in (\ref{eqn:3.4}).

    $k=n-1 \to $ Because of monicity, we have the equality $x Q_{n-1}=Q_n+p_{n-1}$ where $p_{n-1} \in \mathcal{P}_{n-1}$, so that $\left(Q_n, x Q_{n-1}\right)=\left(Q_n, Q_n+p_{n-1}\right)=\left(Q_n, Q_n\right)$, hence $c_{n-1}=b_n$ in (\ref{eqn:3.4}).

    $k<n-1 \to $ Then $x Q_k \in \mathcal{P}_{n-1}$, and we obtain $\left(Q_n, x Q_k\right)=0$, thus $c_k=0$.

    It follows that $x Q_n(x)=Q_{n+1}+a_n Q_n+b_n Q_{n-1}$, and that is equivalent to (\ref{eqn:3.3})-(\ref{eqn:3.4})
\end{proof}
\begin{remark}
    If $\left(Q_k\right)$ have leading coefficients $\left(\alpha_k\right)$, then the recurrence takes the form
    \[
    Q_{n+1}(x)=\frac{\alpha_{n+1}}{\alpha_n}\left(x-a_n\right) Q_n(x)-\frac{\alpha_{n+1} \alpha_{n-1}}{\alpha_n^2} b_n Q_{n-1}(x), \quad n=0,1, \ldots
    \]
    and, with an appropriate choice of $\left(\alpha_k\right)$, may become very simple.
\end{remark}

\subsection{Examples}
\ \vspace*{-1.5em}

\begin{example}
    Classical examples of orthogonal (non-monic) polynomials include
    \begin{center}
        \begin{tabular}{lcccc}
            \toprule 
            \bfseries Name &\bfseries {Notation} &\bfseries {Interval} & \bfseries{Weight} &\bfseries {Recurrence} \\
            \midrule {Legendre} & $P_n$ & ${[-1,1]}$ &  $1$ & $(n+1) P_{n+1}(x)=(2 n+1) x P_n(x)-n P_{n-1}(x)$ \\[.6em]
            {Chebyshev} & $T_n$ & ${[-1,1]}$ & $\left(1-x^2\right)^{-1 / 2}$ & $T_{n+1}(x)=2 x T_n(x)-T_{n-1}(x)$ \\[.6em]
            {Laguerre} & $L_n$ & ${[0, \infty)}$ & ${e}^{-x}$ & $(n+1) L_{n+1}(x)=(2 n+1-x) L_n(x)-n L_{n-1}(x)$ \\[.6em]
            {Hermite} & $H_n$ & $(-\infty, \infty)$ & ${e}^{-x^2}$ & $H_{n+1}(x)=2 x H_n(x)-2 n H_{n-1}(x)$ \\
            \bottomrule
        \end{tabular}
    \end{center}
\end{example}

\begin{example}[The Chebyshev polynomials]
    The Chebyshev polynomials $T_n$, on line 2 in the table above, were introduced in Lecture 2 as
    \[
    T_n(x)=\cos n \arccos x, \quad x \in[-1,1],
    \]
    where we also proved the three-term recurrence relation. Let us show that they are indeed orthogonal with the weight $w(x)=\left(1-x^2\right)^{-1 / 2}$. With the substistuion $x=\cos \theta, \theta \in[0, \pi]$ we have $T_n(x)=\cos n \theta$ and
    \[
    \begin{aligned}
    \left(T_n, T_m\right)_w & :=\int_{-1}^1 T_n(x) T_m(x) \frac{\dd x}{\sqrt{1-x^2}}\\ 
    &=\int_0^\pi \cos n \theta \cos m \theta \dd \theta=\frac{1}{2} \int_{-\pi}^\pi \cos n \theta \cos m \theta \dd \theta \\
    & =\frac{1}{4} \int_{-\pi}^\pi[\cos (n+m) \theta+\cos (n-m) \theta] \dd \theta=0, \quad n \neq m .
    \end{aligned}
    \]
\end{example}

\subsection{Least squares polynomial fitting}
Let $f$ be a continuous function defined on some interval $[a, b]$, and suppose we wish to approximate $f$ by a polynomial of degree $n$. If we equip $C[a, b]$ with the distance $\|f-g\|:=(f-g, f-g)^{1 / 2}$ induced by a scalar product $(f, g)=f_a^b f(x) g(x) w(x) d x$, then it is natural to seek a polynomial
\[
p^*:=\arg \min _{p \in \mathcal{P}_n}\|f-p\|,
\]
for which the distance $\left\|f-p^*\right\|$ is as small as possible. Such a polynomial is called a (weighted) \textbf{least squares approximant}.

\begin{theorem}
    Let $\left(Q_k\right)_{k=0}^n$ be polynomials orthogonal with respect to a given inner product. Then the least squares approximant to any $f \in C[a, b]$ from $\mathcal{P}_n$ is given by the formula
    \begin{equation}\label{eqn:3.5}
        p^*=p^*(f)=\sum_{k=0}^n c_k^* Q_k, \quad c_k^*=\frac{\left(f, Q_k\right)}{\left\|Q_k\right\|^2},
    \end{equation}
    and the value of the least squares approximation is
    \begin{equation}\label{eqn:3.6}
        \left\|f-p^*\right\|^2=\|f\|^2-\sum_{k=1}^n \frac{\left(f, Q_k\right)^2}{\left\|Q_k\right\|^2} .
    \end{equation}
\end{theorem}
\begin{remark}
    By orthogonality of $Q_k$, the norm of the extremal $p^*$ is equal to
    \[
    \left\|p^*\right\|^2=\left\|\sum_{k=0}^n c_k^* Q_k\right\|^2=\sum_{k=0}^n\left|c_k^*\right|^2\left\|Q_k\right\|^2=\sum_{k=1}^n \frac{\left(f, Q_k\right)^2}{\left\|Q_k\right\|^2},
    \]
    hence formula (3.6) takes the form
    \[
    \left\|f-p^*\right\|^2=\|f\|^2-\left\|p^*\right\|^2,
    \]
    which is a reminiscent of the Pythagoras theorem.
\end{remark}
\begin{proof}
    Let $p=\sum_{k=0}^n c_k Q_k$. Then
    \begin{align}\label{eqn:3.7}
        F(c)&=(f-p, f-p)=\left(f-\sum_{k=0}^n c_k Q_k, f-\sum_{k=0}^n c_k Q_k\right)\nonumber \\ 
        &=\|f\|^2-2 \sum_{k=0}^n c_k\left(f, Q_k\right)+\sum_{k=0}^n c_k^2\left\|Q_k\right\|^2
    \end{align}
    The right-hand side is a quadratic polynomial in each $c_k$, therefore it attains its minimum when
    \[
    \eval{\frac{\partial F}{\partial c_k}}_{c_k=c_k^*}=-2\left(f, Q_k\right)+2 c_k^*\left\|Q_k\right\|^2=0, \quad k=\overline{0, n},
    \]
    hence conclusion (\ref{eqn:3.5}). Putting optimal $c_k^*$ into (\ref{eqn:3.7}) we obtain (\ref{eqn:3.6}).
\end{proof}

\begin{method}
    Suppose we want to approximate $f \in C[a, b]$ with a prescribed accuracy $\epsilon$, i.e., to find $n=n(\epsilon)$ such that
    \[
    \left\|f-p^*\right\| \leq \epsilon, \quad p^* \in \mathcal{P}_n .
    \]
    From (\ref{eqn:3.6}), it follows that the required value $n$ can be calculated by summing the terms of $\frac{\left(f, Q_k\right)^2}{\left\|Q_k\right\|^2}$ until we reach the bound
    \[
    \sum_{k=1}^n \frac{\left(f, Q_k\right)^2}{\left\|Q_k\right\|^2} \geq\|f\|^2-\epsilon^2 .
    \]
\end{method}

The next theorem assures that this bound can be reached.

\begin{theorem}[The Parseval identity]
    If $[a, b]$ is finite, then $ \displaystyle \sum_{k=0}^{\infty} \frac{\left(f, Q_k\right)^2}{\left\|Q_k\right\|^2}=\|f\|^2$.
\end{theorem}
\begin{proof}[Proof (incomplete)]
    Let
    \[
    \sigma_n^2:=\left\|p^*\right\|^2=\sum_{k=0}^n \frac{\left(f, Q_k\right)^2}{\left\|Q_k\right\|^2},
    \]
    hence
    \[
    \inf_{p \in \mathcal{P}_n}\|f-p\|^2=\left\|f-p^*\right\|^2=\|f\|^2-\sigma_n^2 .
    \]
    According to the Weierstrass theorem, any function in $C[a, b]$ can be approximated arbitrarily close by a polynomial, hence $\lim _{n \rightarrow \infty} \inf _{p \in \mathcal{P}_n}\|f-p\|^2=0$ and we deduce that $\sigma_n^2 \rightarrow\|f\|^2$ as $n \rightarrow \infty$.
\end{proof}

\section{Approximation of linear functionals}
Given a vector space $\mathbb{X}$ (e.g., $\mathbb{R}^n$, or $C^m[a, b]$), a \textbf{linear functional} is any linear mapping
\[
\lambda: \mathbb{X} \rightarrow \mathbb{R} \quad(\text {so } \lambda(\alpha f+\beta g)=\alpha \lambda(f)+\beta \lambda(g), \ \forall f, g \in \mathbb{X} \text { and } \forall \alpha, \beta \in \mathbb{R}) .
\]
We will treat the space $\mathbb{X}=C^{n+1}[a, b]$, and the functionals
\begin{equation}\label{eqn:4.1}
    (1).\ \ \lambda(f)=f^{(k)}(\bar{x}), \quad(\bar{x}, k \text{ being fixed}),\qquad
(2).\ \ \lambda(f)=\int_a^b f(x) w(x) \dd{x}.
\end{equation}
Our goal is to find an approximation of the form
\begin{equation}\label{eqn:4.2}
    \lambda(f) \approx \sum_{i=0}^N a_i f(x_i), \quad f \in C^{n+1}[a, b] .
\end{equation}
For the functionals (1)-(2) in (\ref{eqn:4.1}), this is called \textbf{numerical differentiation} and \textbf{numerical integration}, respectively.

\begin{method}[Interpolating formulae]
    A suggestive approximation method is to interpolate $f$ by $p \in \mathcal{P}_n$ and take $\lambda(f) \approx$ $\lambda(p)$. This is called the \textbf{interpolating formula} (of degree $n$), in this case $N=n$.
\end{method}
We have already seen an interpolating formula, namely that of Lagrange for the functional $\lambda(f)=f(\bar{x})$:
\[
f(\bar{x}) \approx p(\bar{x})=\sum_{i=0}^n \ell_i(\bar{x}) f(x_i) .
\]
By linearity of $\lambda$, we have $\lambda(p)=\sum_{i=0}^n \lambda(\ell_i) p(x_i)$, thus the interpolating formula has the form
\begin{equation}\label{eqn:4.3}
    \lambda(f) \approx \sum_{i=0}^n \lambda(\ell_i) f(x_i) .
\end{equation}
Here $(\ell_i)_{i=0}^n$ are the fundamental Lagrange polynomials of degree $n, \ell_i(x_j)=\delta_{i j}$.
\begin{method}[`Exact' formulae]
    Another method is to require the formula (\ref{eqn:4.2}) to be \textbf{exact on} $\mathcal{P}_n$, i.e. to require for any $p \in \mathcal{P}_n$ that
    \[
    \lambda(p)=\sum_{i=0}^N a_i p\left(x_i\right) .
    \]
    In this case the number $N$ of terms (almost) need not to be restricted:
    \begin{itemize}
        \item if $N>n$, then it is not a polynomial of degree $n$ that substitutes the function;
        \item if $N<n$, then the formula is said to be of \textbf{high accuracy};
        \item if $N=n$, then the above methods are the same (see the following lemma).
    \end{itemize}
\end{method}
\begin{lemma}\label{lma:4.3}
    The formula $\lambda(f) \approx \sum_{i=0}^n a_i f(x_i)$ is interpolating $\Longleftrightarrow$ it is exact on $\mathcal{P}_n$.
\end{lemma}
\begin{proof}
    The interpolating formula (\ref{eqn:4.3}) is exact on $\mathcal{P}_n$ by definition. Conversely, if the formula in the lemma is exact on $\mathcal{P}_n$, take $f(x)=\ell_j(x)$ to obtain $\lambda(\ell_j)=\sum_{i=0}^n a_i \ell_j(x_i)=a_j$, i.e., (\ref{eqn:4.3}).
\end{proof}

\subsection{Numerical integration}
A formula of numerical integration (with some $w(x) \geq 0$ )
\[
\lambda(f):=I(f):=\int_a^b f(x) w(x) \dd{x} \approx \sum_{i=0}^n a_i f(x_i), \quad f \in C[a, b],
\]
is called a \textbf{quadrature formula} with \textbf{nodes} $(x_i)$ and \textbf{weights} $(a_i)$. By Lemma \ref{lma:4.3}, for any $n+1$ fixed $(x_i)$, the interpolating formula $a_i=I(\ell_i)$ is exact on $\mathcal{P}_n$. Can one find a \textbf{quadrature of higher accuracy} which is exact on $\mathcal{P}_m$ with some $m>n$ ? Since we are free in choosing $n+1$ nodes $(x_i)$, we may hope to increase the degree of accuracy from degree $n$ up to $2 n+1$. More is impossible.

\begin{lemma}
    No quadrature formula with $n+1$ nodes is exact for all $p \in \mathcal{P}_m$ if $m \geq 2 n+2$.
\end{lemma}
\begin{proof}
    We prove by presenting a counterexample. Take $p(x)=\prod_{i=0}^n(x-x_i)^2 \geq 0$. Then $p \in \mathcal{P}_{2 n+2}$, and $I(p)>0$, but $\sum_{i=0}^n a_i p(x_i)=0$ for any $a_i$. Hence the integral and the quadrature do not match.
\end{proof}
Our next goal is to show that $m=2 n+1$ can be attained. For this we need

\begin{lemma}
    Let $Q_{n+1}$ be orthogonal to all $p_n \in \mathcal{P}_n$ on $[a, b]$. Then all the zeros of $Q_{n+1}$ are real, distinct and lie in the interval $(a, b)$.
\end{lemma}
\begin{proof}
    Denote by $k$ the number of the sign changes of $Q_{n+1}$ in $(a, b)$ and assume that $k \leq n$. If $k=0$ set $p_k \equiv 1$, and if $1 \leq k \leq n$ set $p_k(x):=\prod_{i=1}^k(x-t_i)$ where $t_i \mathrm{~s}$ are the points where a sign change of $Q_{n+1}$ occurs. Then $p_k \in \mathcal{P}_k, k \leq n$, hence $(Q_{n+1}, p_k)=0$. On the other hand, by construction, $Q_{n+1}(x) p_k(x)$ does not change sign throughout $[a, b]$ and vanishes at a finite number of points, hence
    \[
        |(Q_{n+1}, p_k)|:=|\int_a^b Q_{n+1}(x) p_k(x) w(x) \dd{x}|=\int_a^b|Q_{n+1}(x) p_k(x)| w(x) \dd{x}>0,
    \]
    a contradiction. Thus, $k \geq n+1$ (hence $k=n+1$) and the statement follows.
\end{proof}

\begin{theorem}
    Let a quadrature with $n+1$ nodes $(x_i)_{i=0}^n$ be exact on $\mathcal{P}_n$ (i.e. interpolating). Then it is exact on $\mathcal{P}_{2 n+1}$ if and only if its nodes $(x_i)_{i=0}^n$ are the zeros of the $(n+1)$-st orthogonal polynomial $Q_{n+1}$.
\end{theorem}
\begin{proof}
    ($ \Longrightarrow $) If a quadrature with $n+1$ nodes $(x_i)$ is exact for all $p \in \mathcal{P}_{2 n+1}$, then letting $Q_{n+1}(x):=$ $\prod_{i=0}^n(x-x_i) \in \mathcal{P}_{n+1}$ and taking any $q_n \in \mathcal{P}_n$ we find
    \[
    \int_a^b Q_{n+1}(x) q_n(x) w(x) \dd{x}=\sum_{i=0}^n a_i[Q_{n+1}(x_i) q_n(x_i)]=0
    \]
    i.e., $Q_{n+1}$ is orthogonal to all $q_n \in \mathcal{P}_n$.

    ($ \Longleftarrow $) onversely, let $(x_i)_{i=0}^n$ be the zeros of orthogonal $Q_{n+1}$. Given any $p_{2 n+1} \in \mathcal{P}_{2 n+1}$, we can represent it uniquely as
    \[
    p_{2 n+1}(x)=Q_{n+1}(x) r_n(x)+s_n(x), \quad \text { with some } r_n, s_n \in \mathcal{P}_n
    \]
    (in fact, $s_n$ interpolates $p_{2 n+1}$ at the points $(x_i)$). Since $Q_{n+1}$ is orthogonal to $r_n$, we have
    \[
    I(p_{2 n+1})=\int_a^b p_{2 n+1}(x) w(x) \dd{x}=\int_a^b s_n(x) w(x) \dd{x}=I(s_n) .
    \]
    On the other hand, because $Q_{n+1}(x_i)=0$,
    \[
    \sum_{i=0}^n a_i p_{2 n+1}(x_i)=\sum_{i=1}^n a_i s_n(x_i) .
    \]
    But $s_n \in \mathcal{P}_n$, while the quadrature is exact on $\mathcal{P}_n$, hence $I(s_n)=\sum a_i s_n(x_i)$, i.e., the righthand sides of two previous equalities coincide, therefore the left-hand sides coincide too, i.e., $I(p_{2 n+1})=\sum a_i p_{2 n+1}(x_i)$.
\end{proof}

\begin{definition}
    A quadrature with $n+1$ nodes that is exact on $\mathcal{P}_{2 n+1}$ is called Gaussian quadrature.
\end{definition}
\begin{example}
    For $[a, b]=[-1,1]$ and $w(x) \equiv 1$, the underlying orthogonal polynomials are the Legendre polynomials. The corresponding weights and nodes of the Gaussian quadratures are as follows.
\[
\begin{array}{*3{>{\displaystyle}l}}
P_1(x)=x & x_0=0 & a_0=2 ; \\[1em]
P_2(x)=\frac{3}{2}\left(x^2-\frac{1}{3}\right) & x_0=-\frac{1}{\sqrt{3}}, x_1=\frac{1}{\sqrt{3}} & a_0=1, a_1=1 \\[1em]
P_3(x)=\frac{5}{2}\left(x^3-\frac{3}{5} x\right) & x_0=-\sqrt{\frac{3}{5}}, x_1=0, x_2=\sqrt{\frac{3}{5}} & a_0=\frac{5}{9}, a_1=\frac{8}{9}, a_2=\frac{5}{9}
\end{array}
\]
\end{example}
\begin{example}
    For $[a, b]=[-1,1]$ and $w(x)=\left(1-x^2\right)^{-1 / 2}$, the orthogonal polynomials are the Chebyshev polynomials and the qudrature rule is particularly attractive:
\[
T_{n+1}(x)=\cos (n+1) \arccos x, \quad x_i=\cos \frac{2 i+1}{2 n+2} \pi, \quad a_i=\frac{\pi}{n+1}, \quad i=0, \ldots, n .
\]
\end{example}

\begin{example}
    Simplest quadrature formulae ($w\equiv 1$)
    \begin{center}
        \begin{tabular}{rcl}
            the rectangle rule: & $\displaystyle  I(f) \approx (b-a) f(a) $ & \parbox{3.5cm}{1-point,\\ non-Gaussian,\\ exact on constants}\\[2em] 
            the midpoint rule: & $\displaystyle I(f)\approx (b-a)f\qty(\frac{a+b}{2}) $ & \parbox{3.5cm}{1-point, Gaussian,\\ exact on linear fns}\\[2em] 
            the trapezodial rule: & $\displaystyle I(f) \approx (b-a)\qty[\frac{1}{2}f(a)+\frac{1}{2}f(b)] $ & \parbox{3.5cm}{2-point,\\ non-Gaussian,\\ exact on linear fns}\\[2em] 
            the Simpson rule: & $\displaystyle I(f) \approx(b-a)\left[\frac{1}{6} f(a)+\frac{2}{3} f\left(\frac{a+b}{2}\right)+\frac{1}{6} f(b)\right]$ & \parbox{3.5cm}{3-point,\\ non-Gaussian,\\ but of higher accuracy \\ (exact on cubics)}
        \end{tabular}
    \end{center}
\end{example}

\subsection{Numerical differentiation}
Consider the interpolating formulae for numerical differentiation
\[
\lambda(f)=f^{(k)}(\bar{x}) \approx \sum_{i=0}^n a_i f\left(x_i\right), \quad a_i=\lambda\left(\ell_i\right)=\ell_i^{(k)}(\bar{x}),
\]
which are exact on the polynomials of degree $n$. The simplest ones are with $n=k$, i.e., $f^{(k)}(\bar{x}) \approx$ $p^{(k)}(\bar{x})$ where $p$ is the interpolating polynomial of degree $k$. But $p^{(k)}(\bar{x})$ is $k$ ! times the leading coefficient of $p$, i.e., $k ! f\left[x_0, \ldots, x_k\right]$, and we obtain the simplest rules
\[
f^{(k)}(\bar{x}) \approx k ! f\left[x_0, \ldots, x_k\right]
\]
\begin{example}
    Common differences 
    \begin{center}
        \begin{tabular}{rcl}
            the foward difference: & $\displaystyle  f^{\prime}(x) \approx f[x, x+h]=\frac{f(x+h)-f(x)}{h} $ & \parbox{3cm}{2-points,\\ exact on linear fns}\\[2em] 
            the central difference: & $\displaystyle f^{\prime}(x) \approx f[x-h, x+h]=\frac{f(x+h)-f(x-h)}{2 h} $ & \parbox{3cm}{2-point,\\ of higher accuracy\\ exact on quadratics}\\[2em] 
            the 2nd central difference: & $\displaystyle f^{\prime \prime}(x) \approx 2 f[x-h, x, x+h]=\frac{f(x-h)-2 f(x)+f(x-h)}{h^2}$ & \parbox{3cm}{3-point,\\ of higher accuracy\\ exact on cubics}
        \end{tabular}
    \end{center}
\end{example}

\begin{example}
    $n=2, k=1,[a, b]=[0,2]$. (Of course, one can transform any formula to any interval.)
\begin{equation}\label{eqn:4.4}
    f^{\prime}(0) \approx p_2^{\prime}(0)=-\frac{3}{2} f(0)+2 f(1)-\frac{1}{2} f(2) .
\end{equation}
Here (and in any other formula), given the nodes $\left(x_i\right)$, in our case $(0,1,2)$, we can find the corresponding coefficients $\left(a_i\right)$ in two ways.
\begin{enumerate}
    \item We determine fundamental lagrange polynomials $\ell_i$ (such that $\ell_i\left(x_j\right)=\delta_{i j}$),
    \[
    \ell_0(x)=\frac{1}{2}(x-1)(x-2), \quad \ell_1(x)=-x(x-2), \quad \ell_2(x)=\frac{1}{2} x(x-1),
    \]
    and set $a_i=\lambda\left(\ell_i\right)$ :
    \[
    a_0=\ell_0^{\prime}(0)=-\frac{3}{2}, \quad a_1=\ell_1^{\prime}(0)=2, \quad a_2=\ell_2^{\prime}(0)=-\frac{1}{2}
    \]
    \item Sometimes it is easier to solve the system of linear equations which arises if we require the formula to be exact on monomials $\left(x^m\right)$.
    \[
        \left\{ \begin{aligned}
             a_0+a_1+a_2 &= 0\quad (f(x)=1),\\ 
             a_1+2a_2&=1 \quad (f(x)=x),\\ 
             a_1+4a_2 &= 0\quad (f(x) = x^2).
        \end{aligned}  \right. \implies a_2=-\frac{1}{2},a_1=2,a_0=-\frac{3}{2}. 
    \]
\end{enumerate}
\end{example}

\section{Error of approximation}
Given a linear functional $\lambda$ and an approximation formula $\lambda(f) \approx \sum_{i=0}^N a_i f\left(x_i\right)$, we are interested in the error (which is a linear functional as well)
\[
e_\lambda(f):=\lambda(f)-\sum_{i=0}^N a_i f\left(x_i\right) .
\]
If $\lambda$ acts on $f \in C^{n+1}[a, b]$, then it is natural to seek an estimate in terms of $\left\|f^{(n+1)}\right\|_{\infty}$, i.e.,
\begin{equation}\label{eqn:5.1}
    \left|e_\lambda(f)\right| \leq c_\lambda\left\|f^{(n+1)}\right\|_{\infty}, \quad \forall f \in C^{n+1}[a, b] .
\end{equation}
Since $f^{(n+1)} \equiv 0$ on $\mathcal{P}_n$, such an estimate can exist only if
\[
e_\lambda(f)=0 \text { on } \mathcal{P}_n,
\]
i.e., the approximation formula \textit{must be exact on} $\mathcal{P}_n$ (e.g., it can be interpolating of degree $n$). 

If (\ref{eqn:5.1}) holds with some $c_\lambda$ and moreover, for any $\epsilon>0$, there is an $f_\epsilon \in C^{n+1}[a, b]$ such that
\[
\left|e_\lambda\left(f_\epsilon\right)\right|>\left(c_\lambda-\epsilon\right)\left\|f_\epsilon^{(n+1)}\right\|_{\infty},
\]
then the constant $c_\lambda$ in inequality (\ref{eqn:5.1}) is called \textbf{least} or \textbf{sharp}. The next section gives a general approach to obtaining sharp estimates for the error functionals $e_\lambda$ that vanish on $\mathcal{P}_n$.

\subsection{The Peano kernel theorem}

Our point of departure is the \textbf{Taylor formula with an integral remainder term},
\begin{equation}\label{eqn:5.2}
    f(x)=\sum_{i=0}^n \frac{1}{i !}(x-a)^i f^{(i)}(a)+\frac{1}{n !} \int_a^x(x-t)^n f^{(n+1)}(t)\dd{t},
\end{equation}
where the sum is the \textbf{Taylor polynomial} $q_n$ to $f$ (at the point $a$). One can can verify (5.2) by integration by parts. It is standard to write the remainder in the slightly different form, which makes the range of integration independent of $x$ :
\[
R(x)=\frac{1}{n !} \int_a^b(x-t)_{+}^n f^{(n+1)}(t)\dd{t}, \quad \text { with } \quad(x-t)_{+}^n:=(\max \{x-t, 0\})^n
\]
Let $\mu$ be a linear functional on $C^{n+1}$. Then
\[
\mu(f)=\mu\left(q_n\right)+\mu(R)=\mu\left(q_n\right)+\frac{1}{n !} \mu\left(\int_a^b(x-t)_{+}^n f^{(n+1)}(t)\dd{t}\right) .
\]
Let us put formally $\mu$ under the integration sign (that is exchange action of $\mu$ and of $\int_a^b$), so that, for any fixed value of $t$, it will act on the function
\[
g_t(\bullet):=(\bullet-t)_{+}^n,
\]
a function, say, of $x$ which (for a given $t$ ) is defined as $g_t(x):=(x-t)_{+}^n, x \in[a, b]$. To each value $t \in \mathbb{R}$ there corresponds a value $\mu\left(g_t\right) \in \mathbb{R}$, thus we have a function
\[
K_\mu(t):=\mu\left(g_t\right):=\mu\left((\bullet-t)_{+}^n\right), \quad t \in \mathbb{R} .
\]
It is called the \textbf{Peano kernel} of the functional $\mu$. So, formally, we may write
\begin{equation}\label{eqn:5.3}
    \mu(f)=\mu\left(q_n\right)+\frac{1}{n !} \int_a^b K_\mu(t) f^{(n+1)}(t) \dd{t} .
\end{equation}

\begin{definition}
    Denote by $\Lambda_0$ the set of linear functionals which are linear combinations of the functionals of two types
\[
    (1)\ \mu(f)=f^{(k)}(\bar{x}) \quad(0 \leq k \leq n), \quad \bar{x} \in[a, b]\qquad
    (2)\ \mu(f)=\int_a^{\bar{x}} f(x) w(x) \dd{x}, \quad \bar{x} \in[a, b]
\]
\end{definition}
\begin{lemma}
    If $\mu \in \Lambda_0$, then equality (\ref{eqn:5.3}) is valid (i.e., exchange of $\mu$ and $\int_a^b$ is justified).
\end{lemma}
\begin{theorem}[The Peano kernel theorem]
    Let $\lambda \in \Lambda_0$ and let $\lambda(f) \approx \sum_{i=0}^N a_i f\left(x_i\right)$ be an approximation formula which is exact on $\mathcal{P}_n$. Then the error functional $e_\lambda$ (and any other functional from $\Lambda_0$ that vanishes on $\mathcal{P}_n$ ) has the integral representation
\begin{equation}\label{eqn:5.4}
    e_\lambda(f)=\frac{1}{n !} \int_a^b K_{e_\lambda}(t) f^{(n+1)}(t) \dd{t} .
\end{equation}
\end{theorem}
\begin{proof}
    The formula follows from (\ref{eqn:5.3}), where we put $e_\lambda$ instead of $\mu$, and use assumptions $e_\lambda \in \Lambda_0$ and $e_\lambda\left(q_n\right)=0$
\end{proof}

\subsection{Sharp error bounds}
\ \vspace*{-1.5em}
\begin{theorem}
    Under assumptions of the previous theorem, for any $f \in C^{n+1}$, we have the sharp inequality
\begin{equation}\label{eqn:5.5}
    \left|e_\lambda(f)\right| \leq c_\lambda\left\|f^{(n+1)}\right\|_{\infty}, \quad c_\lambda=\frac{1}{n !}\left\|K_{e_\lambda}\right\|_1,
\end{equation}
where $\left\|K_{e_\lambda}\right\|_1:=\int_a^b\left|K_{e_\lambda}(t)\right| \dd{t}$, and $\left\|f^{(n+1)}\right\|_{\infty}:=\max _{t \in[a, b]}\left|f^{(n+1)}(t)\right|$.
\end{theorem}
\begin{proof}
    Applying the inequality
\begin{equation}\label{eqn:5.6}
    \left|\int_a^b g(t) h(t) \dd{t}\right| \leq \int_a^b|g(t)| \dd{t} \cdot \max _{t \in[a, b]}|h(t)|=:\|g\|_1\|h\|_{\infty}
\end{equation}
to the right-hand side of (\ref{eqn:5.4}), we obtain (\ref{eqn:5.5}).
Let us show that the constant $c_\lambda$ in (\ref{eqn:5.5}) is \textit{sharp} (the least possible). In (5.6), equality occurs if we take $h(t)=\operatorname{sgn} g(t)$. Therefore, we get equality in (\ref{eqn:5.5}) taking $f_0$ with $f_0^{(n+1)}(t)=\operatorname{sgn} K_{e_\lambda}(t)$.
\end{proof}
\begin{remark}
    A minor problem in the above proof of sharpness of $c_\lambda$ is that the sign-function $f_0^{(n+1)}$ that provides equality in (\ref{eqn:5.5}) is not continuous, whereas we consider $f \in C^{n+1}$.

Informally, we just \textit{allow} the functions with piecewise continuous $(n+1)$-st derivative, like $|x|^{n+1}$, to be considered together with $f \in C^{n+1}$.

Formally, we can approximate a discontinuous sign-function $f_0^{(n+1)}$ by a continuous function $f_\epsilon^{(n+1)}$ changing its values on arbitrarily small intervals, say of of length $\epsilon^{\prime}$, around the jumps. This will change the integral value in (\ref{eqn:5.4}) by $\epsilon^{\prime \prime}$, hence for such $f_\epsilon$ we get the estimate $e_\lambda\left(f_\epsilon\right)>$ $\left(c_\lambda-\epsilon\right)\left\|f_\epsilon^{(n+1)}\right\|_{\infty}$. Thus, $c_\lambda$ in (\ref{eqn:5.5}) is the least possible constant on $C^{n+1}$ as well.
\end{remark}

Finally, here are two general results on the sharp constants in numerical integration and numerical differentiation.

\begin{theorem}[Error of Gaussian quadrature]
    Given $n \in \mathbb{N}$ and $w(x) \geq 0$, let $\left(a_i\right)_{i=0}^n$ and $\left(x_i\right)_{i=0}^n$ be the weights and nodes of the Gaussian quadrature, respectively, i.e., $\left(x_i\right)$ are zeros of orthogonal polynomial $Q_{n+1}$. Then we have the sharp inequality
\[
\left|\int_a^b f(x) w(x) \dd{x}-\sum_{i=0}^n a_i f\left(x_i\right)\right| \leq \frac{1}{(2 n+2) !}\left\|Q_{n+1}\right\|_2^2\left\|f^{(2 n+2)}\right\|_{\infty} .
\]
\end{theorem}
\begin{theorem}[Shadrin]
    For any $n \in \mathbb{N}$ and any $\Delta=\left(x_i\right)_{i=0}^n$, let $\ell_{\Delta} \in \mathcal{P}_n$ be the polynomial that interpolates $f$ on $\Delta$. Then, for any $1 \leq k \leq n$, we have the sharp inequality
\[
\left\|f^{(k)}-\ell_{\Delta}^{(k)}\right\|_{\infty} \leq \frac{1}{(n+1) !}\left\|\omega_{\Delta}^{(k)}\right\|_{\infty}\left\|f^{(n+1)}\right\|_{\infty} .
\]
\end{theorem}

\subsection{Examples}
\ \vspace*{-1.5em}
\begin{example}
    Take formula (\ref{eqn:4.4}):
\[
f^{\prime}(0) \approx p_2^{\prime}(0)=-\frac{3}{2} f(0)+2 f(1)-\frac{1}{2} f(2) .
\]
It is exact on $\mathcal{P}_2$, hence the error on $C^3[0,2]$ can be expressed as
\[
e(f):=f^{\prime}(0)-\left[-\frac{3}{2} f(0)+2 f(1)-\frac{1}{2} f(2)\right]=\frac{1}{2 !} \int_0^2 K(t) f^{\prime \prime \prime}(t) \dd{t} .
\]
Here, with $g_t(x):=(x-t)_{+}^2$, the Peano kernel $K$ has the form
\[
\begin{aligned}
K(t)=e\left(g_t\right) & =2(0-t)_{+}^1+\frac{3}{2}(0-t)_{+}^2-2(1-t)_{+}^2+\frac{1}{2}(2-t)_{+}^2 \\
& =\left\{\hspace*{-.3em}\begin{array}{rl}
-2(1-t)^2+\frac{1}{2}(2-t)^2 \\[.5em]
\frac{1}{2}(2-t)^2
\end{array}=\left\{\hspace*{-.3em}\begin{array}{cc}
2 t-\frac{3}{2} t^2, & t \in[0,1], \\[.5em]
\frac{1}{2}(2-t)^2, & t \in[1,2] .
\end{array}\right.\right.
\end{aligned}
\]
So, $K(t) \geq 0$, and integration gives the value
\[
\|K\|_1:=\int_0^2|K(t)| \dd{t}=\left(\int_0^1+\int_1^2\right) K(t) \dd{t}=\frac{1}{2}+\frac{1}{6}=\frac{2}{3} .
\]
Therefore, we have the estimate
\[
|e(f)| \leq \frac{1}{2 !} \frac{2}{3}\left\|f^{\prime \prime \prime}\right\|_{\infty}=\frac{1}{3}\left\|f^{\prime \prime \prime}\right\|_{\infty} .
\]
The constant $c=\frac{1}{3}$ is sharp: since $K(t) \geq 0$, we will get equality for $f^{\prime \prime \prime} \equiv$ const, e.g. for $f(x)=x^3$.
\end{example}

\begin{example}
    The Simpson's rule,
\[
\int_{-1}^1 f(t) \dd{t} \approx \int_{-1}^1 p_2(t) \dd{t}=\frac{1}{3} f(-1)+\frac{4}{3} f(0)+\frac{1}{3} f(1),
\]
is exact on $\mathcal{P}_2$, hence the error on $C^3[-1,1]$ is
\[
e(f)=\int_{-1}^1 f(t) \dd{t}-\frac{1}{3} f(-1)-\frac{4}{3} f(0)-\frac{1}{3} f(1)=\frac{1}{2 !} \int_{-1}^1 K(t) f^{\prime \prime \prime}(t) \dd{t},
\]
where with $g_t(x):=(x-t)_{+}^2$
\[
\begin{aligned}
K(t)=e\left(g_t\right) & =\int_{-1}^1(x-t)_{+}^2 \dd{x}-\frac{1}{3}(-1-t)_{+}^2-\frac{4}{3}(0-t)_{+}^2-\frac{1}{3}(1-t)_{+}^2 \\
& =\left\{\hspace*{-.3em}\begin{array}{ll}
\frac{1}{3}(1-t)^3-\frac{4}{3} t^2&\hspace*{-.8em}-\frac{1}{3}(1-t)^2 \\[.5em]
\frac{1}{3}(1-t)^3 &\hspace*{-.8em} -\frac{1}{3}(1-t)^2
\end{array}=\left\{\hspace*{-.3em}\begin{array}{cc}
-\frac{1}{3} t(1+t)^2, & t \in[-1,0] \\[.5em]
-\frac{1}{3} t(1-t)^2, & t \in[0,1]
\end{array}\right.\right.
\end{aligned}
\]
Now, $K(t)$ changes its sign at $t=0$, so its $L_1$-norm has the value
\[
\|K\|_1:=\int_{-1}^1|K(t)| \dd{t}=\left(\int_{-1}^0-\int_0^1\right) K(t) \dd{t}=2 \cdot \frac{1}{3}\left(\frac{1}{2}-\frac{2}{3}+\frac{1}{4}\right)=\frac{1}{18},
\]
so that
\[
e(f) \leq \frac{1}{2 !} \frac{1}{18}\left\|f^{\prime \prime \prime}\right\|_{\infty}=\frac{1}{36}\left\|f^{\prime \prime \prime}\right\|_{\infty} .
\]
The constant $c=\frac{1}{36}$ is sharp again: since $K(t)$ changes its sign at one point $t=0$, we get equality for $f^{\prime \prime \prime}(t)=C \operatorname{sgn} t$, e.g. for $f(x)=|x|^3$. For this $f$ its third derivative has a jump, but for any $\epsilon>0$ it can be approximated by $f_\epsilon \in C^3$ for which the constant would be $c>\frac{1}{36}-\epsilon$, i.e.
\[
e\left(f_\epsilon\right)>\left(\frac{1}{36}-\epsilon\right)\left\|f_\epsilon^{\prime \prime \prime}\right\|_{\infty} .
\]
\end{example}

\section{Ordinary differential equations - basics}

\begin{problem}
    We wish to approximate the exact solution of the ordinary differential equation (ODE)
\begin{equation}\label{eqn:6.1}
    \mathbf{y}^{\prime}=\mathbf{f}(t, \mathbf{y}), \quad 0 \leq t \leq T,
\end{equation}
where $y \in \mathbb{R}^d$ and the function $\bff: \mathbb{R} \times \mathbb{R}^d \rightarrow \mathbb{R}^d$ is sufficiently `smooth'.
\end{problem}

In principle, it is enough for $\mathbf{f}$ satisfy the Lipschitz condition with respect to the second argument to ensure that the solution exists and is unique, namely there exists $\lambda>0$ such that
\begin{equation}\label{eqn:6.2}
    \|\mathbf{f}(t, \mathbf{x})-\mathbf{f}(t, \mathbf{y})\| \leq \lambda\|\mathbf{x}-\mathbf{y}\|, \quad t \in[0, T], \quad \mathbf{x}, \mathbf{y} \in \mathbb{R}^d .
\end{equation}
Yet, for simplicity, we henceforth assume that $f$ is analytic: in other words, we are always able to expand locally into Taylor series.

The equation (\ref{eqn:6.1}) is accompanied by the initial condition $\bfy(0)=\bfy_0$.

For a small \textbf{time step} $h>0$, we let $t_n=n h$ and our purpose is to approximate $\mathbf{y}_{n+1} \approx \mathbf{y}\left(t_{n+1}\right)$, from $\bfy_0, \bfy_1, \ldots, \bfy_n$ and equation (\ref{eqn:6.1}).

\begin{definition}[One-step method]
    A one-step method for solving (\ref{eqn:6.1}) is a map $\mathbf{y}_{n+1}=\boldsymbol{\varphi}\left(t_n, \bfy_n\right)$, i.e. an algorithm which allows $\mathbf{y}_{n+1}$ to depend only on $t_n, \mathbf{y}_n, h$ and $\bff$.
\end{definition}

\begin{method}[Euler's method]
    We approximate $\mathbf{y}^{\prime}$ by the first difference $\mathbf{y}^{\prime}(t) \approx \frac{1}{h}(\mathbf{y}(t+h)-\mathbf{y}(t))$, thus obtaining the \textbf{Euler method}
\begin{equation}\label{eqn:6.3}
    \mathbf{y}_{n+1}=\mathbf{y}_n+h \mathbf{f}\left(t_n, \mathbf{y}_n\right), \quad n=0,1, \ldots
\end{equation}
On $\left[t_n, t_{n+1}\right]$, it approximates $\mathbf{y}(t)$ with a straight line with slope $\mathbf{f}\left(t_n, \mathbf{y}_{n}\right)$.
\end{method}

\begin{definition}
    Let $T>0$ be given, and suppose that, for every $h>0$, a method produces the sequence $\mathbf{y}_n=\mathbf{y}_{n, h}$, where $0 \leq n \leq\lfloor T / h\rfloor$. We say that the method \textbf{converges}, if $\max _{n}\left\|\mathbf{y}_n-\mathbf{y}\left(t_n\right)\right\| \rightarrow 0$ as $h \rightarrow 0$. 
\end{definition}

It follows that if $n h \rightarrow t$ as $h \rightarrow 0$ and $n \rightarrow \infty$, then we have convergence $\mathbf{y}_{n, h} \rightarrow \mathbf{y}(t)$ to the exact solution of (\ref{eqn:6.1}), uniformly for $t \in[0, T]$.

\begin{theorem}
    Suppose that $f$ satisfies the Lipschitz condition (\ref{eqn:6.2}). Then the Euler method (\ref{eqn:6.3}) converges, and the error $\mathbf{e}_n=\mathbf{y}\left(t_n\right)-\bfy_n$ admits the estimate
\begin{equation}\label{eqn:6.4}
    \left\|\bfe_n\right\| \leq c_0 h
\end{equation}
\end{theorem}

\begin{proof}
    The Taylor series for the exact solution $\mathbf{y}$ provides $\mathbf{y}\left(t_{n+1}\right)=\mathbf{y}\left(t_n\right)+h \mathbf{y}^{\prime}\left(t_n\right)+\frac{1}{2} h^2 \mathbf{y}^{\prime \prime}\left(\tau_n\right)$, where $\tau_n$ is a point in the interval $\left[t_n, t_{n+1}\right]$. Therefore, using the identity $\mathbf{y}^{\prime}(t)=\mathbf{f}(t, \mathbf{y}(t))$, we may write
\begin{equation}\label{eqn:6.5}
    \mathbf{y}\left(t_{n+1}\right)=\mathbf{y}\left(t_n\right)+h\, \mathbf{f}(t_n, \mathbf{y}(t_n))+\frac{1}{2} h^2 \mathbf{y}^{\prime \prime}\left(\tau_n\right) .
\end{equation}
By subtracting formula (\ref{eqn:6.3}) from this equation, and assuming that $\frac{1}{2}\left\|\mathbf{y}^{\prime \prime}\right\|_{\infty}=c$, we find that the errors $\bfe_n=\mathbf{y}\left(t_n\right)-\mathbf{y}_n$ satisfy the following relations
\begin{align*}
    \left\|\bfe_{n+1}\right\| & \leq\left\|\bfe_n\right\|+h\left\|\mathbf{f}\left(t_n, \mathbf{y}\left(t_n\right)\right)-\mathbf{f}\left(t_n, \mathbf{y}_n\right)\right\|+c h^2 \\
    & \stackrel{(*)}{\leq}\left\|\mathbf{e}_n\right\|+\lambda h\left\|\mathbf{y}\left(t_n\right)-\mathbf{y}_n\right\|+c h^2=(1+\lambda h)\left\|\mathbf{e}_n\right\|+c h^2
\end{align*}
where in $(*)$ we used the Lipschitz condition on $\bff$. Consequently, by induction,
\[
\left\|\bfe_{n+1}\right\| \leq(1+\lambda h)^m\left\|\bfe_{n+1-m}\right\|+c h^2 \sum_{i=0}^{m-1}(1+\lambda h)^i, \quad m=1,2, \ldots, n+1 .
\]
In particular, letting $m=n+1$ and bearing in mind that $\bfe_0= \mathbf{0} $, we have
\[
\left\|\bfe_{n+1}\right\| \leq c h^2 \sum_{i=0}^n(1+\lambda h)^i=c h^2 \frac{(1+\lambda h)^{n+1}-1}{(1+\lambda h)-1} \leq \frac{c h}{\lambda}(1+\lambda h)^{n+1} .
\]
Since $1+\lambda h \leq {e}^{\lambda h}$ and $(n+1) h \leq T$, we obtain that $(1+\lambda h)^{n+1} \leq {e}^{\lambda T}$, therefore
\[
\left\|\bfe_n\right\| \leq \frac{c e^{\lambda T}}{\lambda} h \rightarrow 0 \quad(h \rightarrow 0), \quad \text { uniformly for } 0 \leq n h \leq T,
\]
and the theorem is true.
\end{proof}

\begin{definition}
    The \textbf{local truncation error} of a general numerical method 
    \[
        \mathbf{y}_{n+1}=\boldsymbol{\varphi}_h\left(t_n, \mathbf{y}_0, \ldots, \mathbf{y}_n\right)
    \]
    for solving (\ref{eqn:6.1}) is the error of the method on the true solution, i.e., the value $\boldsymbol{\eta}_{n+1}$ such that
\[
\mathbf{y}\left(t_{n+1}\right)=\boldsymbol{\varphi}_h\left(t_n, \mathbf{y}\left(t_0\right), \mathbf{y}\left(t_1\right), \ldots, \mathbf{y}\left(t_n\right)\right)+\mathbf{\boldsymbol{\eta}}_{n+1} .
\]
The \textbf{order} of the method is the largest integer $p \geq 0$ such that
\[
\boldsymbol{\eta}_{n+1}=\mathcal{O}\left(h^{p+1}\right)
\]
for all $h>0, n \geq 0$ and all sufficiently smooth functions $f$ in (\ref{eqn:6.1}). Note that, unless $p \geq 1$, the method is an unsuitable approximation to (\ref{eqn:6.1}): in particular, $p \geq 1$ is necessary for convergence.
\end{definition}

\begin{remark}[The order of Euler's method]
    From (\ref{eqn:6.5}), it follows that
\[
\boldsymbol{\eta}_{n+1}=\frac{1}{2} h^2 y^{\prime \prime}\left(\tau_n\right)=\mathcal{O}\left(h^2\right)
\]
and we deduce that Euler's method is of order 1 (which is justified by (\ref{eqn:6.4}))
\end{remark}

\begin{definition}[Theta methods]\label{def:theta_methods}
    For $\theta \in[0,1]$, we consider methods of the form
\begin{equation}\label{eqn:6.6}
    \mathbf{y}_{n+1}=\mathbf{y}_n+h\left[\theta \mathbf{f}\left(t_n, \mathbf{y}_n\right)+(1-\theta) \mathbf{f}\left(t_{n+1}, \mathbf{y}_{n+1}\right)\right], \quad n=0,1, \ldots
\end{equation}
\begin{enumerate}[(1)]
    \item If $\theta=1$, we recover Euler's method and the choices $\theta=0$ and $\theta=\frac{1}{2}$ are known as
    \begin{center}
    \begin{tabular}{rl}
        Backward Euler: & $\quad \mathbf{y}_{n+1}=\mathbf{y}_n+h \mathbf{f}\left(t_{n+1}, \mathbf{y}_{n+1}\right)$,\\[.5em]
        Trapezoidal rule: & $\quad \mathbf{y}_{n+1}=\mathbf{y}_n+\frac{1}{2} h\left[\mathbf{f}\left(t_n, \mathbf{y}_n\right)+\mathbf{f}\left(t_{n+1}, \mathbf{y}_{n+1}\right)\right]$.
    \end{tabular}
    \end{center}
    \item If $\theta \in[0,1)$, then the theta method (\ref{eqn:6.6}) is \textbf{implicit}: each time step requires the solution of a system of $d$ (in general, nonlinear) algebraic equations for the unknown vector $\mathbf{y}_{n+1}$.
\end{enumerate}
\end{definition}

Solution of nonlinear algebraic equations $\mathbf{F}(\mathbf{y})= \mathbf{0}$ can be done by iteration. For example,
\begin{center}
    \begin{tabular}{rl}
        Direct iteration:& $\quad \mathbf{y}^{[k+1]}=\mathbf{y}^{[k]}-\mathbf{F}(\mathbf{y}^{[k]})$,\\[0.5em]
Newton-Raphson (N-R):& $\quad \mathbf{y}^{[k+1]}=\mathbf{y}^{[k]}-[J_\bfF(\mathbf{y}^{[k]})]^{-1} \mathbf{F}(\mathbf{y}^{[k]})$,\\[0.5em]
Modified N-R:& $\quad \mathbf{y}^{[k+1]}=\mathbf{y}^{[k]}-[J_{\mathbf{F}}(\mathbf{y}^{[0]})]^{-1} \mathbf{F}(\mathbf{y}^{[k]})$
    \end{tabular}
\end{center}
where $J$ is the Jacobian, i.e. derivative of $\mathbf{F}$.

\begin{remark}[The order of the theta method]\label{rmk:6.9}
    It follows from (\ref{eqn:6.6}) and Taylor's theorem that the local truncation error of the theta method is
\[
\begin{aligned}
& \mathbf{y}\left(t_{n+1}\right)-\mathbf{y}\left(t_n\right)-h\left[\theta \mathbf{y}^{\prime}\left(t_n\right)+(1-\theta) \mathbf{y}^{\prime}\left(t_{n+1}\right)\right] \\
&= {\left[h \mathbf{y}^{\prime}\left(t_n\right)+\frac{1}{2} h^2 \mathbf{y}^{\prime \prime}\left(t_n\right)+\frac{1}{6} h^3 \mathbf{y}^{\prime \prime \prime}\left(t_n\right)\right] } \\
& \quad-\theta h \mathbf{y}^{\prime}\left(t_n\right)-(1-\theta) h\left[\mathbf{y}^{\prime}\left(t_n\right)+h \mathbf{y}^{\prime \prime}\left(t_n\right)+\frac{1}{2} h^2 \mathbf{y}^{\prime \prime \prime}\left(t_n\right)\right]+\mathcal{O}\left(h^4\right) \\
&=\left(\theta-\frac{1}{2}\right) h^2 \mathbf{y}^{\prime \prime}\left(t_n\right)+\left(\frac{1}{2} \theta-\frac{1}{3}\right) h^3 \mathbf{y}^{\prime \prime \prime}\left(t_n\right)+\mathcal{O}\left(h^4\right) .
\end{aligned}
\]
Therefore the theta method is of order 1, except that the trapezoidal rule is of order 2.
\end{remark}

\section{Multistep methods I}
\subsection{Order of the multistep methods}
\ \vspace*{-1.5em}
\begin{definition}[Multistep methods]
    It is often useful to use several past solution values in computing a new value. Thus, assuming that $\mathbf{y}_n, \mathbf{y}_{n+1}, \ldots, \mathbf{y}_{n+s-1}$ are available, where $s \geq 1$, we say that
\begin{equation}\label{eqn:7.1}
    \sum_{m=0}^s a_m \mathbf{y}_{n+m}=h \sum_{m=0}^s b_m \mathbf{f}_{n+m}, \quad n=0,1, \ldots,
\end{equation}
where $a_s=1$, and $\mathbf{f}_{n+m}=\mathbf{f}\left(t_{n+m}, \mathbf{y}_{n+m}\right)$, is an \textbf{$s$-step method}. If $b_s=0$, the method is \textbf{explicit}, otherwise it is \textbf{implicit}. If $s \geq 2$, we need to obtain extra \textbf{starting values} $\mathbf{y}_1, \ldots, \mathbf{y}_{s-1}$ by different time-stepping method.
\end{definition}

\begin{theorem}
    The multistep method (\ref{eqn:7.1}) is of order $p \geq 1$ if and only if
\begin{equation}\label{eqn:7.2}
    \sum_{m=0}^s a_m=0, \quad \sum_{m=0}^s m^k a_m=k \sum_{m=0}^s m^{k-1} b_m, \quad k=\overline{1,p}. 
\end{equation}
\end{theorem}

\begin{proof}
    Substituting the exact solution and expanding into Taylor series about $t_n$, we obtain
\[
\begin{aligned}
&\sum_{m=0}^s a_m \mathbf{y}\left(t_{n+m}\right)-h \sum_{m=0}^s b_m \mathbf{y}^{\prime}\left(t_{n+m}\right)\\ 
&=\sum_{m=0}^s a_m \sum_{k=0}^{\infty} \frac{(m h)^k}{k !} \mathbf{y}^{(k)}\left(t_n\right)-h \sum_{m=0}^s b_m \sum_{k=1}^{\infty} \frac{(m h)^{k-1}}{(k-1) !} \mathbf{y}^{(k)}\left(t_n\right) \\
&=\left(\sum_{m=0}^s a_m\right) \mathbf{y}\left(t_n\right)+\sum_{k=1}^{\infty} \frac{h^k}{k !}\left(\sum_{m=0}^s m^k a_m-k \sum_{m=0}^s m^{k-1} b_m\right) \mathbf{y}^{(k)}\left(t_n\right) .
\end{aligned}
\]
Thus, to obtain the local truncation error of order $\mathcal{O}\left(h^{p+1}\right)$ regardless of the choice of $\mathbf{y}$, it is necessary and sufficient that the coefficients at $h^k$ vanish for $k \leq p$, i.e., that (\ref{eqn:7.2}) are satisfied.
\end{proof}

\begin{remark}
    Since the Taylor expansion of polynomials of degree $p$ contains only $\mathcal{O}\left(h^k\right)$ terms with $k \leq p$, the multistep method is of order $p$ iff
\[
\sum_{m=0}^s a_m Q\left(t_{n+m}\right)=h \sum_{m=0}^s b_m Q^{\prime}\left(t_{n+m}\right), \quad \forall Q \in \mathcal{P}_p .
\]
In particular, taking $Q(x)=x^k$ for $k=\overline{0,p}$ and $t_{n+m}=m, h=1$, we obtain (\ref{eqn:7.2}).
\end{remark}
Given a multistep method (\ref{eqn:7.1}), we define two polynomials of degree $s$ :
\[
\rho(w)=\sum_{m=0}^s a_m w^m, \quad \sigma(w)=\sum_{m=0}^s b_m w^m .
\]

\begin{theorem}
    The multistep method (\ref{eqn:7.1}) is of order $p \geq 1$ if and only if
\begin{equation}\label{eqn:7.3}
    \rho\left(e^z\right)-z \sigma\left(e^z\right)=\mathcal{O}\left(z^{p+1}\right), \quad z \rightarrow 0.
\end{equation}
\end{theorem}

\begin{proof}
    Expanding again into Taylor series,
\[
\begin{aligned}
\rho\left(e^z\right)-z \sigma\left(e^z\right) & =\sum_{m=0}^s a_m e^{m z}-z \sum_{m=0}^s b_m e^{m z}\\ 
&=\sum_{m=0}^s a_m \sum_{k=0}^{\infty} \frac{1}{k !} m^k z^k-z \sum_{m=0}^s b_m \sum_{k=0}^{\infty} \frac{1}{k !} m^k z^k \\
& =\sum_{k=0}^{\infty} \frac{z^k}{k !} \sum_{m=0}^s m^k a_m-\sum_{k=1}^{\infty} \frac{z^k}{(k-1) !} \sum_{m=0}^s m^{k-1} b_m \\
& =\left(\sum_{m=0}^s a_m\right)+\sum_{k=1}^{\infty} \frac{z^k}{k !}\left(\sum_{m=0}^s m^k a_m-k \sum_{m=0}^s m^{k-1} b_m\right) .
\end{aligned}
\]
The theorem follows from (\ref{eqn:7.2}).
\end{proof}
The reason that (\ref{eqn:7.3}) is equivalent to (\ref{eqn:7.2}) (and that their proofs are almost identical) is due to the (informal) relation between the Taylor series and the exponent $e^{h D}$, where $D$ is the operator of differentiation. Namely
\[
\mathbf{f}(x+h)=\left(I+h D+\frac{1}{2} h^2 D^2+\cdots\right) \mathbf{f}(x)=e^{h D} \mathbf{f}(x)
\]
\begin{example}[Adams-Bashforth method]\label{eg:7.5}
    The 2-step \textbf{Adams-Bashforth method} is
\begin{equation}\label{eqn:7.4}
    \mathbf{y}_{n+2}-\mathbf{y}_{n+1}=h\left[\frac{3}{2} \mathbf{f}_{n+1}-\frac{1}{2} \mathbf{f}_n\right] .
\end{equation}
Therefore $\rho(w)=w^2-w, \sigma(w)=\frac{3}{2} w-\frac{1}{2}$ and
\begin{align*}
    \rho\left(e^z\right)-z \sigma\left(e^z\right)&=\left[1+2 z+2 z^2+\frac{4}{3} z^3\right]-\left[1+z+\frac{1}{2} z^2+\frac{1}{6} z^3\right]-\frac{3}{2} z\left[1+z+\frac{1}{2} z^2\right]+\frac{1}{2} z+\mathcal{O}\left(z^4\right)\\ 
    &=\frac{5}{12} z^3+\mathcal{O}\left(z^4\right).
\end{align*}
Hence the method is of order 2.
\end{example}

\subsection{Convergence}
\ \vspace*{-1.5em}
\begin{definition}[Convergence of a multistep method]
    Let a multistep method (\ref{eqn:7.1}) be applied to the usual ODE (\ref{eqn:6.1}), and let $\widehat{e}(h)$ and $e(h)$ be the numbers
\[
\widehat{e}(h):=\max \left\|\mathbf{y}\left(t_i\right)-\mathbf{y}_i\right\|: i=0 \ldots s-1, \quad e(h):=\max \left\|\mathbf{y}\left(t_i\right)-\mathbf{y}_i\right\|: i=0 \ldots N .
\]
The method is defined to be \textbf{convergent} if, for every ODE whose right-hand side satisfies the Lipshitz condition, the limits $h \rightarrow 0$ and $\widehat{e}(h) \rightarrow 0$ imply $e(h) \rightarrow 0$.
\end{definition}

\begin{example}[Absence of convergence]
    Consider the 2-step method
\begin{equation}\label{eqn:7.5}
    \mathbf{y}_{n+2}+4 \mathbf{y}_{n+1}-5 \mathbf{y}_n=h\left(4 \mathbf{f}_{n+1}+2 \mathbf{f}_n\right)
\end{equation}
Now $\rho(w)=w^2+4 w-5, \sigma(w)=4 w+2$ and it is easy to verify that the method is of order 3 . Let us apply it, however, to the trivial ODE $y^{\prime}=0, y(0)=1$. Hence a single step reads $y_{n+2}+4 y_{n+1}-5 y_n=0$ and the general solution of this recursion is $y_n=c_1 1^n+c_2(-5)^n$, where $c_1, c_2$ are arbitrary constants, which are determined by $y_0=1$ and our value of $y_1$. If $y_1 \neq 1$, i.e. if there is a small error in our starting values, then $c_2 \neq 0$, thus $\left|y_n\right| \rightarrow \infty$ and we cannot recover the exact solution $y(t) \equiv 1$.

As a more convincing example, we may consider ODE $y^{\prime}=-y, y(0)=1$, with the exact solution $y(t)=e^{-t}$. Even if we take the exact $y_1=e^{-h}$, the sequence $\left(y_n\right)$ will grow like $h^4(-5)^n$. So, it is not only the order that matters.
\end{example}

\begin{definition}[Root condition]
    We say that a polynomial obeys the \textbf{root condition} if all its zeros reside in $|w| \leq 1$ and all zeros of unit modulus are simple, i.e. have multiplicity 1. (If $\rho$ obeys the root condition, the method (\ref{eqn:7.1}) is sometimes said to be \textbf{zero-stable}.)
\end{definition}

\begin{theorem}[The Dahlquist equivalence theorem]
    The multistep method (\ref{eqn:7.1}) is convergent if and only if it is of order $p \geq 1$ and the polynomial $\rho$ obeys the root condition.
\end{theorem}

For the method (\ref{eqn:7.4}) we have $\rho(w)=w(w-1)$, and the root condition is obeyed. However, for (\ref{eqn:7.5}) we obtain $\rho(w)=(w+5)(w-1)$, the root condition fails and we deduce that there is no convergence.

\subsection{Construction I}
A useful procedure to generate multistep methods which are convergent and of high order is as follows.
\begin{technique}\label{tech:7.10}
    According to (\ref{eqn:7.2}), order $p \geq 1$ implies $\rho(1)=0$. Choose an arbitrary $s$-degree polynomial $\rho$ that obeys the root condition and such that $\rho(1)=0$. To maximize order, we let $\sigma$ be the polynomial of degree $s$ for implicit methods (alternatively, of degree $(s-1)$ for explicit methods) arising from the truncation of the Taylor expansion of $\frac{\rho(w)}{\log w}$ about the point $w=1$. Thus, for example, for an implicit method,
    \[
    \sigma(w)=\frac{\rho(w)}{\log w}+\mathcal{O}\left(|w-1|^{s+1}\right) \Leftrightarrow \rho\left(e^z\right)-z \sigma\left(e^z\right)=\mathcal{O}\left(z^{s+2}\right)
    \]
    and (\ref{eqn:7.3}) implies order at least $s+1$ (for explicit methods order $s$).
\end{technique}
\begin{example}\label{eg:7.11}
    The choice $\rho(w)=w^{s-1}(w-1)$ corresponds to \textbf{Adams methods}:

    \textbf{Adams-Bashforth methods} if $b_s=0$, hence explicit, with the order $s$ (e.g. (\ref{eqn:7.4}) for $s=2$),

    \textbf{Adams-Moulton methods} if $b_s \neq 0$, hence implicit, but with the order $(s+1)$.

For example, letting $s=2$ and $\xi=w-1$, we obtain the 3rd-order Adams-Moulton method by expanding
\[
\begin{aligned}
\frac{w(w-1)}{\log w} & =\frac{\xi+\xi^2}{\log (1+\xi)}=\frac{\xi+\xi^2}{\xi-\frac{1}{2} \xi^2+\frac{1}{3} \xi^3-\cdots}=\frac{1+\xi}{1-\frac{1}{2} \xi+\frac{1}{3} \xi^2-\cdots} \\
& =(1+\xi)\left[1+\left(\frac{1}{2} \xi-\frac{1}{3} \xi^2\right)+\left(\frac{1}{2} \xi-\frac{1}{3} \xi^2\right)^2+\mathcal{O}\left(\xi^3\right)\right]=1+\frac{3}{2} \xi+\frac{5}{12} \xi^2+\mathcal{O}\left(\xi^3\right) \\
& =1+\frac{3}{2}(w-1)+\frac{5}{12}(w-1)^2+\mathcal{O}|w-1|^3=-\frac{1}{12}+\frac{2}{3} w+\frac{5}{12} w^2+\mathcal{O}\left(|w-1|^3\right) .
\end{aligned}
\]
Therefore the 2-step, 3rd-order Adams-Moulton method is
\[
\mathbf{y}_{n+2}-\mathbf{y}_{n+1}=h\left[\frac{5}{12} \mathbf{f}_{n+2}+\frac{2}{3} \mathbf{f}_{n+1}-\frac{1}{12} \bff_n\right] .
\]
(If we cut expansion at $1+\frac{3}{2} \xi=1+\frac{3}{2}(w-1)=\frac{3}{2} w-\frac{1}{2}$, we get explicit 2-step 2nd-order method (\ref{eqn:7.4}).)
\end{example}

\section{Multistep methods II}
\subsection{Construction II}

Continue on the last section, 
\begin{method}[BDF]
    As another exercise in applying Technique \ref{tech:7.10} (and for future applications) let us construct $s$-step $s$-order methods such that $\sigma(w) = b_s w^s$ for some non-zero $b_s,$ say $b_s=1$. In other words, 
    \begin{equation}\label{eqn:8.1}
        \sum_{m=0}^{s}a_m \mathbf{y}_{n+m }= h \mathbf{f}_{n+s},\quad n = 0,1,\dots
    \end{equation}
\end{method}
Such methods are called backward differentiation formulae (BDF), and their coefficients $\left(a_m\right)$ can be obtained from the following expression.

\begin{lemma}
    The polynomial $\rho(w)=\sum_{m=0}^s a_m w^m$ of the s-step s-order BDF method (\ref{eqn:8.1}) has the form
\begin{equation}\label{eqn:8.2}
    \rho(w)=\sum_{k=1}^s \frac{1}{k} w^{s-k}(w-1)^k .
\end{equation}
\end{lemma}
\begin{proof}
    We need to fulfill the $s$-order condition $\rho\left(e^z\right)-z \sigma\left(e^z\right)=\mathcal{O}\left(z^{s+1}\right)$ which, with $w=e^z$ and $\sigma(w)=w^s$, becomes
    \[
    \rho(w)=w^s \log w+\mathcal{O}\left(|w-1|^{s+1}\right) .
    \]
    But we have $\log w=-\log \left(\frac{1}{w}\right)=-\log \left(1-\frac{w-1}{w}\right)$, and $\log (1-x)=-\sum_{k=1}^{\infty} \frac{1}{k} x^k$, therefore
    \[
    w^s \log w=w^s \sum_{k=1}^s \frac{1}{k}\left(\frac{w-1}{w}\right)^k+\mathcal{O}\left(|w-1|^{s+1}\right),
    \]
    and the first term is a polynomial of degree $s$, hence the result.
\end{proof}

\begin{example}
    For $s=2$, we obtain $\frac{3}{2} \mathbf{y}_{n+2}-2 \mathbf{y}_{n+1}+\frac{1}{2} \mathbf{y}_n=h \mathbf{f}_{n+2}$, or with the standard normalization
    \[
    \mathbf{y}_{n+2}-\frac{4}{3} \mathbf{y}_{n+1}+\frac{1}{3} \mathbf{y}_n=\frac{2}{3} h \mathbf{f}_{n+2} .
    \]
\end{example}
\textit{Warning}. We cannot take it for granted that BDF methods are convergent (i.e. that $\rho(w)$ in (\ref{eqn:8.2}) satisfies the root condition). In fact, this is the case if and only if $s \leq 6$, thus they cannot be used outside this range.

\begin{method}
    Let us look at construction of both Adams methods and BDF formulae from a different (although equivalent) point of view, namely using the polynomial criterion for the $p$-order methods:
\[
\sum_{m=0}^s a_m Q(m)=\sum_{m=0}^s b_m Q^{\prime}(m) \quad \forall Q \in \mathcal{P}_p .
\]
(1) For Adams methods, the right-hand side is $Q(s)-Q(s-1)=\int_{s-1}^s Q^{\prime}(t) \dd{t}$, therefore letting $q=Q^{\prime}$ we are looking for the quadrature formula (of numerical integration)
\[
\int_{s-1}^s q(t) \dd{t}=\sum_{m=0}^s b_m q(m)
\]
which should be valid for all polynomials $q$ of degree $s$ (if $b_s \neq 0$) or of degree $s-1$ (if $b_s=0$). By the Lagrange interpolating formula, we can write $q$ as $q(t)=\sum_{m=0}^s \ell_m(t) q(m)$, where $\ell_m$ are the fundamental Lagrange polynomials satisfying $\ell_m(k)=\delta_{m k}$, and it follows that
\[
b_m=\int_{s-1}^s \ell_m(t) \dd{t}, \quad \ell_m(t)=\frac{\omega(t)}{(t-m) \omega^{\prime}(m)}, \quad \omega(t)=\prod_{m=0}^s(t-m) .
\]
(2) For BDF, we are looking for the formulae of numerical (backward) differentiation instead,
\[
\sum_{m=0}^s a_m Q(m)=Q^{\prime}(s),
\]
which should be valid for all polynomials of degree $s$. Again, from the Lagrange interpolating formula $Q(t)=\sum_{m=0}^s \ell_m(t) Q(m)$, it follows that $a_m=\ell_m^{\prime}(s)$, and using explicit form of $\ell_m$ given above we derive
\[
a_s=\sum_{m=1}^s \frac{1}{m}, \quad a_m=\frac{(-1)^{s-m}}{s-m}\binom{s}{m}
\]
\end{method}

\subsection{Higher order and convergence (non-examinable)}
\ \vspace*{-1.5em}
\begin{lemma}
    There is no $s$-step methods of order $2 s+1$. The implicit and explicit methods of order $2 s$ and $2 s-1$, respectively, do exist and are unique.
\end{lemma}

\begin{example}
    For $s=1$ and $s=2$ the highest order (implicit) methods have the form
\begin{IEEEeqnarray*}{LLr}
    (1)\ \text{one-step order 2:} &\quad \bfy_{n+1}-\bfy_n=h\left(\frac{1}{2} \bff_{n+1}+\frac{1}{2} \bff_n\right) & \text{[trapezoidal rule]},\\
    (2)\ \text{two-step order 4:} &\quad \bfy_{n+2}-\bfy_n=h\left(\frac{1}{3} \bff_{n+2}+\frac{4}{3} \bff_{n+1}+\frac{1}{3} \bff_n\right) & \text{[Simpson rule]}.
\end{IEEEeqnarray*}
Obviously, the root condition for both methods is fulfilled. However, for $s \geq 3$, the $s$-step methods of order $2 s$ do not satisfy the root condition, and that makes them not very useful for numerical implementation. In fact, we should not go very far in our search for the higher order multistep methods because of the following restriction.
\end{example}

\begin{theorem}[The first Dalquist barrier]
    If an $s$-step method converges, then it has the order
\[
p \leq \begin{cases}
s & \text { for explicit methods, } \\
s+1 & \text { if } s \text { is odd, } \\
s+2 & \text { if } s \text { is even. }
\end{cases}
\]
\end{theorem}
So, in Example \ref{eg:7.11}, the Adams-Bashforth methods are optimal in the sense that they attain the bound $p=s$ for explicit methods, and the Adams-Moulton methods for odd $s$ are also optimal, with $p=s+1$.

An optimal implicit method with even number of steps $s$ with order $p=s+2$ is given for example by $\rho(w)=w^s-1$. This method corresponds to the quadrature (where $q=Q^{\prime}$, with $Q \in \mathcal{P}_{s+2}$ )
\[
\int_0^s q(t) \dd{t}=\sum_{m=0}^s b_m q(m) \quad \forall q \in \mathcal{P}_{s+1} .
\]

\section{Runge-Kutta methods}

Recall the quadrature formulae of approximating the integral by 
\begin{equation}
    \int_{0}^{1} f(t) \dd{t} \approx \sum_{i=1}^{s}b_i f(c_i),
\end{equation}
where for given knots $c_i$, the weights $b_i$ are chosen to make this quadrature exact for all polynomials of degree $s-1$, that is, $ b_i = \int_{0}^{1} \ell_i (t) \dd{t} $ where $\ell_i$ are fundamental Lagrange polynomials such that $ \ell_i(c_j) = \delta_{ij} $. 
If $ \omega(t) = \prod_{i=1}^{s}(t-c_i) $ is orthogonal to all polynomials of degree $s-1$ on $[0,1]$ then we obtain the Gaussian quadrature: the formula that is exact for all polynomials of degree $2s-1$. 

Suppose that we wish to solve the ODE $ y' = f(t) $ with $y(0) = y_0$. The exact solution is 
\[
    y(t_{n+1}) = y(t_n) + \int_{t_n}^{t_{n+1}} f(t) \dd{t}
\]
and we can approximate it by the quadrature, scaling from $[0,1]$ to $[t_n,t_{n+1}]$. In that way, we obtain the time-stepping scheme 
\[
    y_{n+1}=y_n+h \sum_{i=1}^s b_i f(t_n+c_i h), \quad h=t_{n+1}-t_n .
\]
We aim to generalise this to genuine ODEs of the form $ \mathbf{y}' = \mathbf{f}(t,\mathbf{y}) $. Formally,
\[
    \mathbf{y}(t_{n+1}) = \mathbf{y}(t_n) + \int_{t_n}^{t_{n+1}} \mathbf{f}(t,\mathbf{y}(t)) \dd{t},
\]a
and this can be approximated as before by 
\begin{equation}\label{eqn:9.2}
    \mathbf{y}_{n+1} = \mathbf{y}_n + h \sum_{i=1}^{s} b_i \mathbf{f}(t_n + c_i h, \mathbf{y}(t_n+c_i h)),
\end{equation}
except that the vectors $ \mathbf{y}(t_n+c_i h) $ are unknown. But they can be approximated by another quadrature using approximate values of $\mathbf{y}(t_n+c_j h)$ obtained earlier:
\[
    \mathbf{y}\left(t_n+c_i h\right)=\mathbf{y}\left(t_n\right)+\int_{t_n}^{t_n+c_i h} \mathbf{f}(t, \mathbf{y}(t)) \dd{t} \approx \mathbf{y}\left(t_n\right)+h \sum_{j=1}^{i-1} a_{i j} \mathbf{f}\left(t_n+c_j h, \mathbf{y}\left(t_n+c_j h\right)\right),
\]

so that applying $f$ to both sides of this formula we obtain
\begin{equation}\label{eqn:9.3}
    \mathbf{f}\left(t_n+c_i h, \mathbf{y}\left(t_n+c_i h\right)\right) \approx \mathbf{f}\left(t_n+c_i h, \mathbf{y}\left(t_n\right)+h \sum_{j=1}^{i-1} a_{i j} \mathbf{f}\left(t_n+c_j h, \mathbf{y}\left(t_n+c_j h\right)\right)\right)
\end{equation}

\begin{method}[Explicit Runge-Kutta methods]
    From (\ref{eqn:9.2}) and (\ref{eqn:9.3}), letting $ \mathbf{k}_i \approx \mathbf{f}(t_n+c_i h, \mathbf{y}(t_n+c_i h)) $, we arrive at the general form of an $s$-stage \textbf{explicit Runge-Kutta method} (RK):
    \[
        \mathbf{k}_i = \mathbf{f}\left( t_n + c_i h, \mathbf{y}_n + h \sum_{j=1}^{i-1}a_{ij}\mathbf{k}_j \right),\ i = 1, \dots, s,\quad 
        \mathbf{y}_{n+1} = \mathbf{y}_n + h \sum_{i=1}^{s} b_i \mathbf{k}_i.
    \]
\end{method}

Here are the same formulas with more details:
\begin{IEEEeqnarray*}{rClrCl}
    \mathbf{k}_1 &=& \mathbf{f}(t_n,\mathbf{y}_n) & c_1 &=& 0\\ 
    \mathbf{k}_2 &=& \mathbf{f}(t_n + c_2h, \mathbf{y}_n+ h a_{21} \mathbf{k}_1) & c_2 &=& a_{21}\\ 
    \mathbf{k}_3 &=& \bff(t_n+c_3 h, \mathbf{y}_n+h(a_{31} \mathbf{k}_1+a_{32} \mathbf{k}_2)) &\quad c_3&=&a_{31}+a_{32}\\ 
    &\vdots& &  &\vdots & \\ 
    \mathbf{k}_s &=& \mathbf{f}\left( t_n + c_s h, \mathbf{y}_n + h \sum_{j=1}^{s-1}a_{sj}\mathbf{k}_j \right) & c_s &=& \sum_{j=1}^{s-1}a_{sj}\\
    \mathbf{y}_{n+1} &=& \mathbf{y}_n + h \sum_{i=1}^{s} b_i \mathbf{k}_i & \sum b_i &=& 1
\end{IEEEeqnarray*}
Here, equalities $c_i=\sum_{j=1}^{i-1} a_{ij}$ (as well as $\sum b_i=1$ ) simply say that quadratures are exact on constants: a reasonable (although not necessary for $a_{i j}$) requirement. A more sophisticated choice of the RK coefficients $a_{i j}$ is motivated at the first instance by order considerations.

\subsection{Order of Runge-Kutta methods}
\ \vspace*{-1.5em}
\begin{example}[Two-stage methods of order 2]
    Consider the following two-stage methods
    \begin{align*}
        \mathbf{k}_1 &= \mathbf{f}(t_n,\mathbf{y}_n)\\ 
        \mathbf{k}_2 &= \mathbf{f}(t_n+c_2h, \mathbf{y}_n+c_2h \mathbf{k}_1)\\ 
        \mathbf{y}_{n+1} &= \mathbf{y}_n + h (b_1\mathbf{k}_1 + b_2\mathbf{k}_2)
    \end{align*}
    Taylor expand $\mathbf{k}_2$ about $(t_n,\mathbf{y}_n)$ and using $\mathbf{k}_1=\mathbf{f}$, we obtain
    \[
        \mathbf{k}_2 = \mathbf{f}(t_n+c_2h, \mathbf{y}_n + c_2h \mathbf{f}) = \mathbf{f} + c_2h(\mathbf{f}_t+\mathbf{f}_y \mathbf{f}) + \mathcal{O}(h^2). 
    \]
    But $ \mathbf{y}'(t) = \mathbf{f}(t,\mathbf{y}) $, so $ \mathbf{y}'' = \mathbf{f}_t + \mathbf{f}_y \mathbf{y}' = \mathbf{f}_t + \mathbf{f}_y \mathbf{f} $. Therefore, substituing the exact solution $ \mathbf{y}_n = \mathbf{y}(t_n) $ into the scheme, we obtain $ \mathbf{k}_1 = \mathbf{y}', \mathbf{k}_2 = \mathbf{y}' + c_2h \mathbf{y}'' + \mathcal{O}(h^2) $. Consequently, the RK method produces 
    \[
        \mathbf{y}(t_{n+1}) = \mathbf{y} + (b_1+b_2) h \mathbf{y}' + b_2c_2 h^2 \mathbf{y}'' + \mathcal{O}(h^3)
    \]
    and we deduce that it is of order 2 if 
    \[
        b_1+b_2 = 1, \quad b_2 c_2 = \frac{1}{2}.
    \]
    It is easy to demonstrate that no such method may be of order $ \ge 3 $, e.g. by applying it to $ y' = y $. 
\end{example}

\begin{method}[General RK method]
    A general $s$-stage RK method is 
    \[
        \mathbf{k}_i = \mathbf{f}\qty(t_n + c_i h, \mathbf{y}_n + h \sum_{j=1}^{s}a_{ij}\mathbf{k}_j),\ i = 1, \dots, s,\quad \mathbf{y}_{n+1} = \mathbf{y}_n + h \sum_{i=1}^{s}b_i \mathbf{k}_i. 
    \]
\end{method}
\begin{example}\label{eg:9.5}
Consider the 2-stage implicit method
\[
\begin{aligned}
\bfk_1 & =\bff\left(t_n, \mathbf{y}_n+\frac{1}{4} h\left(\mathbf{k}_1-\mathbf{k}_2\right)\right), \\
\mathbf{k}_2 & =\bff\left(t_n+\frac{2}{3} h, \mathbf{y}_n+\frac{1}{12} h\left(3 \mathbf{k}_1+5 \mathbf{k}_2\right)\right), \\
\mathbf{y}_{n+1} & =\mathbf{y}_n+\frac{1}{4} h\left(\mathbf{k}_1+3 \mathbf{k}_2\right) .
\end{aligned}
\]
We analyse its order with respect to scalar \textbf{autonomous} equations of the form $y^{\prime}=f(y)$. For brevity, we use the convention that all functions are evaluated at $y=y\left(t_n\right)$. Thus,
\[
\begin{aligned}
& k_1=f+\frac{1}{4} h f_y\left(k_1-k_2\right)+\frac{1}{32} h^2 f_{y y}\left(k_1-k_2\right)^2+\mathcal{O}\left(h^3\right) \\
& k_2=f+\frac{1}{12} h f_y\left(3 k_1+5 k_2\right)+\frac{1}{288} h^2 f_{y y}\left(3 k_1+5 k_2\right)^2+\mathcal{O}\left(h^3\right) .
\end{aligned}
\]
Substitution $k_1=f+\mathcal{O}(h), k_2=f+\mathcal{O}(h)$ in the above equations ($h$-terms) yields
\[
\begin{aligned}
& k_1=f+\mathcal{O}\left(h^2\right), \\
& k_2=f+\frac{2}{3} h f_y f+\mathcal{O}\left(h^2\right) .
\end{aligned}
\]
Substituting again, we obtain
\[
\begin{aligned}
k_1 & =f-\frac{1}{6} h^2 f_y^2 f+\mathcal{O}\left(h^3\right), \\
k_2 & =f+\frac{2}{3} h f_y f+h^2\left(\frac{5}{18} f_y^2 f+\frac{2}{9} f_{y y} f^2\right)+\mathcal{O}\left(h^3\right), \\
y\left(t_{n+1}\right) & =y+h f+\frac{1}{2} h^2 f_y f+\frac{1}{6} h^3\left(f_y^2 f+f_{y y} f^2\right)+\mathcal{O}\left(h^4\right) .
\end{aligned}
\]
But $y^{\prime}=f$ implies $y^{\prime \prime}=f_y f$ and $y^{\prime \prime \prime}=f_y^2 f+f_{y y} f^2$ and we deduce from Taylor's theorem that the method is at least of order 3. (It is easy to verify that it isn't of order 4 , for example applying it to the equation $y^{\prime}=y$.)
\end{example}

\begin{theorem}[High-order RK methods]
    Let $\omega(t)=\prod_{i=1}^s\left(t-c_i\right)$ be orthogonal on the interval $[0,1]$ to all polynomials of degree $r-1 \leq s-1$, let $\ell_i$ be the fundamental Lagrange polynomials of degree $s-1$ for the knots $c_i$, and let
\begin{equation}\label{eqn:9.4}
    a_{i j}=\int_0^{c_i} \ell_j(t) \dd{t}, \quad b_i=\int_0^1 \ell_j(t) \dd{t}
\end{equation}
Then the (highly implicit) $s$-stage RK method with parameters (\ref{eqn:9.4}) is of order $s+r$. 
\end{theorem}
Thus, the highest order of an $s$-stage implicit RK method is $2 s$, it corresponds to collocation at zeros of Legendre polynomial (Gauss-Legendre RK method). Construction of explicit methods with high order is a kind of art.

\section{Stiff ODEs: linear stability and A-stability}

Consider the linear scalar ODE
\[
y^{\prime}=\lambda y, \quad y(0)=1, \quad \text { where } \lambda<0 .
\]
The exact solution is $y(t)=e^{\lambda t}$ which decays to zero as $t \rightarrow \infty$, so we would like from a numerical method to maintain the same property for the sequence $\left(y_n\right)$. However, if we solve this ODE with forward Euler's method, then $y_{n+1}=y_n+\lambda h y_n=(1+\lambda h)^{n+1}$, and in order to have $y_n \rightarrow 0$, we should require
\[
y_n \rightarrow 0 \Leftrightarrow|1+\lambda h|<1 \Leftrightarrow h<\frac{2}{|\lambda|} .
\]
For large $\lambda<0$, say $\lambda=-100$, this could be a severe restriction on $h$.

It is important to realise that this restriction, necessary to recovery of correct asymptotic behaviour, has nothing to do with local accuracy, since, for large $n$, the genuine solution $y\left(t_n\right)$ is exceedingly small. So, this restriction is solely to prevent an unbounded growth in the numerical solution.

\begin{definition}[Stiff ODEs]
    We say that the ODE $\mathbf{y}^{\prime}=\mathbf{f}(t, \mathbf{y})$ is \textbf{stiff} if (for some methods) we need to depress $h$ to maintain \textbf{stability} well beyond requirements of accuracy.
\end{definition}

\begin{definition}[Linear stability domain]
    Suppose that a numerical method, applied to $y^{\prime}=\lambda y$, $y(0)=1$, with constant $h$, produces the solution sequence $\left\{y_n\right\}$. We call the set
\[
\mathcal{D}=\left\{z:=\lambda h \in \mathbb{C}: \lim _{n \rightarrow \infty} y_n=0\right\}
\]
the \textbf{linear stability domain} of the method.

Noting that the set of $\lambda \in \mathbb{C}$ for which $y(t)=e^{\lambda t} \rightarrow 0$ as $t \rightarrow \infty$ is the left half-plane $\mathbb{C}^{-}=\{z \in$ $\mathbb{C}: \operatorname{Re} z<0\}$, we say that the method is \textbf{A-stable} if $\mathbb{C}^{-} \subseteq \mathcal{D}$.
\end{definition}

\begin{example}
    (1) For Euler's method, as we have already seen $y_n \rightarrow 0$ iff $|1+\lambda h|<1$, therefore its linear stability domain is $\mathcal{D}=\{z \in \mathbb{C}:|1+z|<1\}$.

(2) Solving $y^{\prime}=\lambda y$ with the trapezoidal rule: $y_{n+1}=y_n+h\left(\frac{1}{2} f_n+\frac{1}{2} f_{n+1}\right)$, we obtain
\[
    y_{n+1}=\left[\left(1+\frac{1}{2} \lambda h\right) \bigg/\left(1-\frac{1}{2} \lambda h\right)\right] y_n
\]
thus, by induction, $y_n=\left[\left(1+\frac{1}{2} \lambda h\right) /\left(1-\frac{1}{2} \lambda h\right)\right]^n y_0$. Therefore
\[
z \in \mathcal{D}  \iff \left|\frac{1+\frac{1}{2} z}{1-\frac{1}{2} z}\right|<1  \iff  \operatorname{Re} z<0,
\]
and we deduce that $\mathcal{D}=\mathbb{C}^{-}$. Hence, the method is A-stable.

(3) For backward Euler: $y_{n+1}=y_n+\lambda h y_{n+1}$, we get $y_{n+1}=(1-\lambda h)^{-1} y_n$, whence $\mathcal{D}=\{z \in \mathbb{C}$ : $|1-z|>1\}$, and the method is also A-stable.
\end{example}
Note that A-stability does not mean that any step size will do. We need to choose $h$ small enough to ensure the right accuracy, but we don't want to depress it much further to prevent instability.

\subsection{Stability of multistep methods}
By definition, the number $z=\lambda h$ is in the linear stability domain $\mathcal{D}$ of a multistep method
\begin{equation}
    \sum_{m=0}^s a_m y_{n+m}=h \sum_{m=0}^s b_m f_{n+m}
\end{equation}
if the sequence $\left(y_n\right)$ which is a solution to the recurrence relation arising from (10.1) with $f_n=\lambda y_n$ satisfies $y_n \rightarrow 0$. Plugging in back, the latter is true if and only if the roots of the (characteristic) equation
\begin{equation}
    p(x):=\rho(x)-z \sigma(x)=\sum_{m=0}^s a_m x^m-z \sum_{m=0}^s b_m x^m=0
\end{equation}
are less than one in absolute values. (Recall the solution of difference equations)

If $z=\lambda h$ is on the boundary of the linear stability domain $\mathcal{D}$, then the characteristic polynomial $p$ has a root $x$ of modulus 1, i.e. $x=e^{i t}$. Substituting such an $x$ into (10.2), we get that the boundary of $\mathcal{D}$ is the curve $z(t)=\frac{\rho\left(e^{i t}\right)}{\sigma\left(e^{i t}\right)}$.

If we need to determine the \textit{real} stability interval $I=\mathcal{D} \cap \mathbb{R}$, then it is highly likely that $z_1=\frac{\rho(1)}{\sigma(1)}$ and $z_2=\frac{\rho(-1)}{\sigma(-1)}$ are the end-points of $I$. Further, since $\rho(1)=0$, this interval for many methods is $I=\left(\frac{\rho(-1)}{\sigma(-1)}, 0\right)$. Still some further work is needed to prove that this is indeed the case.

\begin{example}
    Consider the 3-step 3-rd-order Adams-Bashforth method
\[
y_{n+3}-y_{n+2}=h\left(\frac{23}{12} f_{n+2}-\frac{4}{3} f_{n+1}+\frac{5}{12} f_n\right) .
\]
With $f=\lambda y$, the characteristic polynomial is
\[
p(x)=\left(x^3-x^2\right)-z\left(\frac{23}{12} x^2-\frac{4}{3} x+\frac{5}{12}\right) .
\]
We have then
\[
p(1)=-z, \quad p(-1)=-2-\frac{11}{3} z
\]
so we guess that $\left(-\frac{6}{11}, 0\right)$ is the stability interval $I=\mathcal{D} \cap \mathbb{R}$.

(1) If $z>0$, then
\[
p(1)=-z<0, \quad p(+\infty)=+\infty
\]
hence $p$ has a root $x_*>1$, thus no stability.

(2) If $z<-\frac{6}{11}$, then
\[
p(-1)=-2-\frac{11}{3} z>0, \quad p(-\infty)=-\infty
\]

hence $p$ has a root $x_*<-1$, and there is no stability either.

(3) For $z \in\left(-\frac{6}{11}, 0\right)$, we have
\[
p(-1)=-2-\frac{11}{3} z<0, \quad p(0)=-\frac{5}{12} z>0, \quad p\left(\frac{1}{2}\right)=-\frac{1}{8}-\frac{11}{48} z<0, \quad p(1)=-z>0,
\]
hence all three roots of $p$ are real and reside within $(-1,1)$, therefore the method is stable for $z=\lambda h \in\left(-\frac{6}{11}, 0\right)$.
\end{example}

\begin{theorem}[The second Dahlquist barrier]
    No multistep method of order $p \geq 3$ may be A-stable. (The $p=2$ barrier for A-stability is attained by the trapezoidal rule.)
\end{theorem}

However, some high-order multistep methods are still of good use. Stability properties of BDF, say, are satisfactory for most stiff equations. The point is that, in many stiff linear systems in applications, the eigenvalues are not just in $\mathbb{C}^{-}$but also well away from $i \mathbb{R}$. All BDF methods of order $p \leq 6$ (i.e., all convergent BDF methods) share the feature that their linear stability domain $\mathcal{D}$ includes a wedge about $(-\infty, 0)$ : such methods are said to be \textbf{$\text{A}_0$-stable}.

\subsection{Stability of Runge-Kutta methods}
\ \vspace*{-1.5em}
\begin{lemma}
    No explicit Runge-Kutta method may be A-stable (or even $\text{A}_0$-stable).
\end{lemma}

\begin{proof}
    See ES2Q6. 
\end{proof}

Unlike multistep or explicit Runge-Kutta methods, implicit high-order RK may be A-stable. For an $s$-stage method, its linear stability domain $\mathcal{D}$ is determined from the relation
\[
y_{n+1}=r(z) y_n, \quad r(z)=\frac{p(z)}{q(z)}, \quad p, q \in \mathcal{P}_s,
\]
where $r(z)$ is a rational function of degree $s$, and the inequality $|r(z)|<1$ for $z \in \mathbb{C}^{-}$is possible.

\begin{example}
    Consider the 2-stage 3rd-order method from Example \ref{eg:9.5}
\[
\begin{aligned}
\mathbf{k}_1 & =\mathbf{f}\left(t_n, \mathbf{y}_n+\frac{1}{4} h\left(\mathbf{k}_1-\mathbf{k}_2\right)\right) \\
\mathbf{k}_2 & =\mathbf{f}\left(t_n+\frac{2}{3} h, \mathbf{y}_n+\frac{1}{12} h\left(3 \mathbf{k}_1+5 \mathbf{k}_2\right)\right), \\
\mathbf{y}_{n+1} & =\mathbf{y}_n+\frac{1}{4} h\left(\mathbf{k}_1+3 \mathbf{k}_2\right) .
\end{aligned}
\]
Applying it to $y^{\prime}=\lambda y$, we have
\[
\begin{aligned}
& h k_1=h \lambda\left(y_n+\frac{1}{4} h k_1-\frac{1}{4} h k_2\right), \\
& h k_2=h \lambda\left(y_n+\frac{1}{4} h k_1+\frac{5}{12} h k_2\right) .
\end{aligned}
\]
This is a linear system, whose solution (with $z=\lambda h$ ) is
\[
\begin{bmatrix}
    h k_1 \\
h k_2
\end{bmatrix}=
\begin{bmatrix}
    1-\frac{1}{4} z & \frac{1}{4} z \\
-\frac{1}{4} z & 1-\frac{5}{12} z
\end{bmatrix}^{-1}
\begin{bmatrix}
    z y_n \\
z y_n
\end{bmatrix}=\frac{1}{\det(A)}
\begin{bmatrix}
    1-\frac{5}{12} z & -\frac{1}{4} z \\
\frac{1}{4} z & 1-\frac{1}{4} z
\end{bmatrix}
\begin{bmatrix}
    z y_n \\
z y_n
\end{bmatrix}.
\]
We need $\frac{1}{4}\left(h k_1+3 h k_2\right)=\frac{z\left(1-\frac{1}{6} z\right)}{\det(A)} y_n$, therefore
\[
y_{n+1}=y_n+\frac{1}{4} h\left(k_1+3 k_2\right)=\left(1+\frac{z\left(1-\frac{1}{6} z\right)}{1-\frac{2}{3} z+\frac{1}{6} z^2}\right) y_n=\frac{1+\frac{1}{3} z}{1-\frac{2}{3} z+\frac{1}{6} z^2} y_n .
\]
Let
\[
r(z)=\frac{1+\frac{1}{3} z}{1-\frac{2}{3} z+\frac{1}{6} z^2} .
\]
Then $y_{n+1}=r(z) y_n$, therefore, by induction, $y_n=r(z)^n y_0$ and we deduce that
\[
\mathcal{D}=\{z \in \mathbb{C}:|r(z)|<1\}
\]
We wish to prove that $|r(z)|<1$ for every $z \in \mathbb{C}^{-}$, since this is equivalent to A-stability. This will be done by a technique that can be applied to other RK methods. According to the \textbf{maximum modulus principle} from Complex Methods, if $g$ is analytic in a complex domain $\Omega$ then $|g|$ attains its maximum on $\partial \Omega$. We let $g=r$. This is a rational function, hence its only singularities are the poles $2 \pm i \sqrt{2}$ and $g$ is analytic in $\Omega=\mathbb{C}^{-}=\{z \in \mathbb{C}: \operatorname{Re} z<0\}$. Therefore, it attains its maximum modulus either at $z=\infty$ or on $\partial \Omega=i \mathbb{R}$. Clearly, $r(z) \rightarrow 0$ as $z \rightarrow \infty$, and
\[
\text {A-stability} \iff |r(z)|<1, \quad z \in \mathbb{C}^{-}  \iff  |r(i t)| \leq 1, \quad t \in \mathbb{R} .
\]
In turn,
\[
|r(i t)|^2 \leq 1  \iff  \left|1-\frac{2}{3} i t-\frac{1}{6} t^2\right|^2-\left|1+\frac{1}{3} i t\right|^2 \geq 0 .
\]
But the last expression is $\left(1-\frac{1}{6} t^2\right)^2+\frac{4}{9} t^2-\left(1+\frac{1}{9} t^2\right)=\frac{1}{36} t^4 \geq 0$, and it follows that the method is A-stable.
\end{example}

\section{Numerical implementation}
\begin{problem}
    The step size $h$ is not some preordained quantity: it is a parameter of the method (in reality, many parameters, since we may vary it from step to step). 
    
    The basic input of a well-written computer package for ODEs is not the step size but the \textbf{error tolerance}: the level of precision, as required by the user. The choice of $h>0$ is an important tool at our disposal to keep a local estimate of the error beneath the required tolerance in the solution interval. 
    
    In other words, we need not just a \textbf{time-stepping algorithm}, but also mechanisms for \textbf{error control} and for amending the step size.
\end{problem}

\begin{technique}[The Milne device]
    Suppose that we wish to monitor the error of the trapezoidal rule (TR),
\[
\mathbf{y}_{n+1}=\mathbf{y}_n+h\left[\frac{1}{2} \mathbf{f}\left(t_n, \mathbf{y}_n\right)+\frac{1}{2} \mathbf{f}\left(t_n, \mathbf{y}_{n+1}\right)\right]
\]
We already know that the order is 2, more precisely (see Remark \ref{rmk:6.9}) the truncation error satisfies
\begin{equation}\label{eqn:11.1}
    \mathbf{y}\left(t_{n+1}\right)-\mathbf{y}_{n+1}^{\mathrm{TR}}=c_{\mathrm{TR}} h^3 \mathbf{y}^{\prime \prime \prime}\left(t_n\right)+\mathcal{O}\left(h^4\right), \quad c_{\mathrm{TR}}=-\frac{1}{12}
\end{equation}
where the number $c_{\mathrm{TR}}=-\frac{1}{12}$ is called the \textbf{error constant} of TR. Unfortunately, this error estimate does not help much because the value $\mathbf{y}^{\prime \prime \prime}\left(t_n\right)$ is unknown. However, each multistep method (but not RK!) has its own error constant. For example, the 2-nd order 2-step Adams-Bashforth method $(\mathrm{AB})$
\[
\mathbf{y}_{n+1}-\mathbf{y}_n=h\left[\frac{3}{2} \mathbf{f}\left(t_n, \mathbf{y}_n\right)-\frac{1}{2} \mathbf{f}\left(t_{n-1}, \mathbf{y}_{n-1}\right)\right]
\]
has the error constant $c_{\mathrm{AB}}=\frac{5}{12}$ (see Example \ref{eg:7.5}), i.e.,
\begin{equation}\label{eqn:11.2}
    \mathbf{y}\left(t_{n+1}\right)-\mathbf{y}_{n+1}^{\mathrm{AB}}=c_{\mathrm{AB}} h^3 \mathbf{y}^{\prime \prime \prime}\left(t_n\right)+\mathcal{O}\left(h^4\right), \quad c_{\mathrm{AB}}=\frac{5}{12} .
\end{equation}
The idea behind the \textbf{Milne device} is to use two multistep methods of the same order, one explicit and the second implicit (e.g., two methods just mentioned), to estimate the local error of the implicit method. Subtracting (\ref{eqn:11.2}) from (\ref{eqn:11.1}), we obtain the estimate $h^3 \mathbf{y}^{\prime \prime \prime}\left(t_n\right) \approx-\frac{\mathbf{y}_{n+1}^{\mathrm{AB}}-\mathbf{y}_{n+1}^{\mathrm{TR}}}{c_{\mathrm{AB}}-c_{\mathrm{TR}}}$, therefore
\begin{equation}\label{eqn:11.3}
    \mathbf{y}\left(t_{n+1}\right)-\mathbf{y}_{n+1}^{\mathrm{TR}} \approx-\frac{c_{\mathrm{TR}}}{c_{\mathrm{AB}}-c_{\mathrm{TR}}}\left(\mathbf{y}_{n+1}^{\mathrm{AB}}-\mathbf{y}_{n+1}^{\mathrm{TR}}\right)=\frac{1}{6}\left(\mathbf{y}_{n+1}^{\mathrm{AB}}-\mathbf{y}_{n+1}^{\mathrm{TR}}\right)
\end{equation}
and we use the right hand side as an estimate of the local error.
\end{technique}

Note that TR is a far better method than AB: it is A-stable, hence its global behaviour is superior. We employ $\mathrm{AB}$ solely to estimate the local error (well, and for some other things). This adds very little to the overall cost of TR, since AB is an explicit method.

\begin{implementation}
    To implement the Milne device, we work with a pair of multistep methods of the same order, one explicit (predictor) and the other implicit (corrector), e.g.
    \begin{align}
        \text {Predictor: }\quad & \bfy_{n+2}^{\mathrm{P}}=\mathbf{y}_{n+1}+h\left[\frac{5}{12} \bff_{n-1}-\frac{4}{3} \bff_n+\frac{23}{12} \bff_{n+1}\right], \nonumber\\
    \text {Corrector: }\quad & \bfy_{n+2}^{\mathrm{C}}=\mathbf{y}_{n+1}+h\left[-\frac{1}{12} \bff_n+\frac{2}{3} \bff_{n+1}+\frac{5}{12} \bff_{n+2}\right],\label{eqn:11.4}
    \end{align}
    the third-order Adams-Bashforth and Adams-Moulton methods respectively. Depending on whether the error tolerance $\epsilon$ has been achieved (from the estimate similar to (\ref{eqn:11.3})) we amend the step size $h$ (this can be done with polynomial interpolation).
\end{implementation}

The predictor is employed not just to estimate the error of the corrector, but also to provide \textit{an initial guess in the solution of the implicit corrector equations}, which then can be solved, say, by simple (direct) iteration (see p. \pageref{def:theta_methods}). In the latter case, the formulas are as follows
\begin{align}
    \mathbf{y}_{n+2}^{(1)} & =\mathbf{y}_{n+1}+h\left[\frac{5}{12} \bff_{n-1}-\frac{4}{3} \bff_n+\frac{23}{12} \bff_{n+1}\right] \label{eqn:11.5} \\
    \mathbf{y}_{n+2}^{(\ell+1)} &=\mathbf{y}_{n+1}+h\left[-\frac{1}{12} \bff_n+\frac{2}{3} \bff_{n+1}+\frac{5}{12} \bff\left(t_{n+2}, \mathbf{y}_{n+2}^{(\ell)}\right)\right], \quad \ell=1,2, \ldots \label{eqn:11.6}
\end{align}
Usually, one stops the iteration when $\left\|\mathbf{y}_{n+2}^{(\ell+1)}-\mathbf{y}_{n+2}^{(\ell)}\right\|<\epsilon$ is achieved.

\begin{lemma}[A condition for convergence]
    Given an implicit multistep method, say (\ref{eqn:11.4}), let a simple iteration method (\ref{eqn:11.5})-(\ref{eqn:11.6}) be applied to determine $\mathbf{y}_{n+2}$. Further, let $\mathbf{f}$ satisfy a Lipschitz condition with a constant $L$, and let $h$ satisfy $h L\left|b_2\right|<1$. Then the sequence $(\mathbf{y}_{n+2}^{(\ell)})$ converges to a solution $\mathbf{y}_{n+2}$ of (\ref{eqn:11.4}).
\end{lemma}
\begin{proof}
    The simple iteration provides the relation
\[
\begin{aligned}
\left\|\mathbf{y}_{n+2}^{(\ell+2)}-\mathbf{y}_{n+2}^{(\ell+1)}\right\| & =h\left|b_2\right|\left\|\mathbf{f}\left(t_{n+2}, \mathbf{y}_{n+2}^{(\ell+1)}\right)-\mathbf{f}\left(t_{n+2}, \mathbf{y}_{n+2}^{(\ell)}\right)\right\| \\ 
&\leq h L\left|b_2\right|\left\|\mathbf{y}_{n+2}^{(\ell+1)}-\mathbf{y}_{n+2}^{(\ell)}\right\| \\
& \leq\left(h L\left|b_2\right|\right)^{\ell}\left\|\mathbf{y}_{n+2}^{(1)}-\mathbf{y}_{n+2}^{(0)}\right\|
\end{aligned}
\]
Thus, $h L\left|b_2\right|<1$ implies that $(\mathbf{y}_{n+2}^{(\ell)})$ is a Cauchy sequence, so it converges to a limit, $\mathbf{y}_{n+2}$ say. Further, $\mathbf{y}_{n+2}^{(\ell+1)} \rightarrow \mathbf{y}_{n+2}$ and $\mathbf{y}_{n+2}^{(\ell)} \rightarrow \mathbf{y}_{n+2}$ in (\ref{eqn:11.6}) show that $\mathbf{y}_{n+2}$ satisfies (\ref{eqn:11.4}).
\end{proof}

\begin{technique}[Embedded Runge-Kutta methods]
    The situation is more complicated with RK, since no single error constant determines local growth of the error. The approach of \textbf{embedded RK} requires, again, two (typically explicit) methods: an RK method of $s$ stages and order $p$, say, and another method, of $s+m$ stages, $m \geq 1$, and order $p+1$, such that \textit{the first $s$ stages of both methods are identical}. (This means that the cost of implementing the higher-order method is marginal, once we have computed the lower-order approximation.) 
\end{technique}

For example, consider
\[
    \begin{rcases}
        \begin{rcases}
        \begin{aligned}
            \mathbf{k}_1 &= \mathbf{f}(t_n, \mathbf{y}_n)\\ 
            \mathbf{k}_2 &= \mathbf{f}( t_n + \tfrac{1}{2}h, \mathbf{y} + \tfrac{1}{2} h \mathbf{k}_1),\hspace*{3em}\\ 
            \mathbf{y}^{[1]}_{n+1} &= \mathbf{y}_n + h \mathbf{k}_2;
        \end{aligned}
        \end{rcases} \text{ order }2\hspace*{1em}\\
        \hspace{1.1pt}
        \begin{aligned}
            \mathbf{k}_3 &= \mathbf{f}(t_n + h, \mathbf{y}_n - h \mathbf{k}_1 + 2h \mathbf{k}_2),\\ 
            \mathbf{y}^{[2]}_{n+1} &= \mathbf{y}_n + \tfrac{1}{6} h (\mathbf{k}_1 + 4 \mathbf{k}_2 + \mathbf{k}_3).
        \end{aligned}
    \end{rcases}\text{ order }3
\]
We thus estimate $\mathbf{y}\left(t_{n+1}\right)-\mathbf{y}_{n+1}^{[1]} \approx \mathbf{y}_{n+1}^{[2]}-\mathbf{y}_{n+1}^{[1]}$. (It might look paradoxical, at least at first glance, but the only purpose of the higher-order method is to provide error control for the lower-order one!)

\begin{technique}[The Zadunaisky device]
    Suppose that the ODE $\mathbf{y}^{\prime}=\mathbf{f}(t, \mathbf{y}), \mathbf{y}(0)=\mathbf{y}_0$, is solved by an arbitrary numerical method of order $p$ and that we have stored (not necessarily equidistant) past solution values $\mathbf{y}_n, \mathbf{y}_{n-1}, \ldots, \mathbf{y}_{n-p}$. We form an interpolating $p$ th degree polynomial (with vector coefficients) $\mathbf{q}$ such that $\mathbf{q}\left(t_{n-i}\right)=\mathbf{y}_{n-i}, i=0,1, \ldots, p$, and consider the differential equation
    \begin{equation}\label{eqn:11.7}
        \mathbf{z}^{\prime}=\mathbf{f}(t, \mathbf{z})+\mathbf{q}^{\prime}(t)-\mathbf{f}(t, \mathbf{q}), \quad \mathbf{z}\left(t_n\right)=\mathbf{y}_n
    \end{equation}
    There are two important observations with regard to (\ref{eqn:11.7})

    \begin{enumerate}[(1)]
        \item Since $\mathbf{q}(t)-\mathbf{y}(t)=\mathcal{O}\left(h^{p+1}\right)$, the term $\mathbf{q}^{\prime}(t)-\mathbf{f}(t, \mathbf{q})$ is usually small (because $\mathbf{y}^{\prime}(t)$ $\mathbf{f}(t, \mathbf{y}(t)) \equiv 0$). Therefore, (\ref{eqn:11.7}) is a small perturbation of the original ODE.
    
        \item The exact solution of (\ref{eqn:11.7}) is known: $\mathbf{z}(t)=\mathbf{q}(t)$. Now, having produced $\mathbf{y}_{n+1}$ with our numerical method, we proceed to evaluate $\bfz_{n+1}$ as well, \textit{using exactly the same method and implementation detail}s. We then evaluate the error in $\mathbf{z}_{n+1}$, namely $\mathbf{z}_{n+1}-\mathbf{q}\left(t_{n+1}\right)$, and use it as an estimate of the error in $\mathbf{y}_{n+1}$.
    \end{enumerate}
\end{technique}

\end{document}