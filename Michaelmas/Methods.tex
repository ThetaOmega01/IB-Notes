\documentclass[a4paper]{article}
\renewcommand{\epsilon}{\varepsilon}
\newcommand{\triposcourse}{Methods}
\input{../header.tex}
\counterwithin{equation}{section}
\graphicspath{ {./images/} }
\pgfplotsset{compat=1.17}
\begin{document}
\maketitle
\tableofcontents
\clearpage
\part{Self-adjoint ODEs}
\section{Fourier Series}
\subsection{Periodic functions}
\begin{definition}
    A function $f$ is \textit{periodic} if $ f(x+T)=f(x), \forall x $, where $T$ is the period.
\end{definition}
\begin{center}
    \includegraphics[scale=0.1]{methods1.jpeg}
\end{center}
Consider the set of functions 
\[
    g_n(x) = \cos \left( \frac{n\pi x}{L} \right),\quad h_n(x) = \sin \left( \frac{n\pi x}{L} \right).
\]
They are periodic on $ [0,2L] $ with period 2L.

Recall identities
\[
    \begin{aligned} \cos (A \pm B) &=\cos A \cos B \mp \sin A \sin B \\ \sin (A \pm B) &=\sin A \cos B \pm \cos A \sin B, \text { and so } \\ \cos A \cos B &=\frac{1}{2}[\cos (A-B)+\cos (A+B)] \\ \sin A \sin B &=\frac{1}{2}[\cos (A-B)-\cos (A+B)] \\ \sin A \cos B &=\frac{1}{2}[\sin (A+B)+\sin (A-B)] \end{aligned}
\]
Define an inner product of the set 
\[
    \langle f,g \rangle = \int_{0}^{2L} f(x)g(x)\,\mathrm{d}x.
\]
\begin{proposition}
    With this inner product, $g_n,h_n$ are mutually orthogonal on $ [0,2L) $.
\end{proposition}
\begin{proof}
    Claim that
    \begin{align}
        &\langle h_n,h_m \rangle = \begin{cases}
            L\delta_{nm} &\forall n,m\neq 0\\
            0 & m=0 \lor n=0\\
            \end{cases} \\
            &\langle g_n,g_m \rangle = \begin{cases}
            L \delta_{mn} &n,m\neq 0\\
            2L \delta_{0n} & m=0\\
            \end{cases} \\
            &\langle h_n,g_m \rangle =0
    \end{align}
    \textit{Proof of claim.} Note that for $n\neq m$,
    \begin{align*}
        \langle h_n,h_m \rangle &= \int_{0}^{2L} \sin \frac{n\pi x}{L} \sin \frac{m\pi x}{L} \,\mathrm{d}x\\ 
        &=\int_{0}^{2L} \frac{1}{2}\left( \cos \frac{(n-m)\pi x }{L}-\cos \frac{(n+m)\pi x}{L} \right) \,\mathrm{d}x\\ 
        &= \frac{1}{2} \left[ \frac{L}{(n-m)\pi}\sin \frac{(n-m)\pi x }{L}-\frac{L}{(n+m)\pi} \sin \frac{(n+m)\pi x }{L} \right]_0^{2L}\\ 
        &= \frac{1}{2}\left( \frac{L}{(n-m)\pi}\sin (2\pi(n-m))-\frac{L}{(n+m)\pi} \sin (2\pi(n+m)) \right)\\ 
        &=0.
    \end{align*}
    For $n=m$, 
    \begin{align*}
        \langle h_n,h_m \rangle &= \int_{0}^{2L} \sin^2 \frac{n\pi x}{L} \,\mathrm{d}x\\ 
        &= \int_{0}^{2L} \frac{1-\cos \frac{2n\pi x}{L}}{2} \,\mathrm{d}x\\ 
        &= L.
    \end{align*}
    This proves (1.1). The rest are similar.
\end{proof}
$g_n,h_n$ form a complete orthogonal set, i.e. they span the space of well-behaved periodic functions $ [0,2L) $ and they are linearly independent.

\subsection{Definition of Fourier series}
\begin{definition}
    We can express any well-behaved periodic function of period $2L$ as 
\begin{equation}\label{1.eq.4:FS}
    f(x) = \frac{1}{2}a_0 + \sum_{n=1}^{\infty} a_n \cos \left( \frac{n\pi x}{L} \right)+ \sum_{n=1}^{\infty} b_n \sin \left( \frac{n\pi x}{L} \right)
\end{equation}
where $a_n,b_n$ are constants such that RHS is convergent for all $x$ where $f$ is continuous. At a discontinuity $x$, the Fourier series approaches the midpoint of upper and lower limits at $x$, i.e. 
\[
    \frac{f(x_+)+f(x_-)}{2} = \frac{1}{2}a_0 + \sum_{n=1}^{\infty} a_n \cos \left( \frac{n\pi x}{L} \right)+ \sum_{n=1}^{\infty} b_n \sin \left( \frac{n\pi x}{L} \right).
\]
\end{definition}
Consider $ \langle h_n,f \rangle  $ and substitute in (1.4), we get 
\[
    \int_{0}^{2L} \sin \left( \frac{n \pi x}{L} \right) f(x) \,\mathrm{d}x = \sum_{m=1}^{\infty}Lb_m \delta_{nm} = Lb_n.
\]
Similar for $a_n$. We get
\begin{proposition}
    \begin{equation}
        \begin{aligned}
            &b_{n}=\frac{1}{L} \int_{0}^{2 L} f(x) \sin \left(\frac{n \pi x}{L}\right)\, \rmd x\\ &a_n = \frac{1}{L}\int_{0}^{2L} f(x) \cos \left( \frac{n \pi x}{L} \right) \,\mathrm{d}x
        \end{aligned}
    \end{equation}
\end{proposition}
\begin{note}
    \begin{enumerate}
        \item $a_n$ includes $n=0$ since $ \frac{1}{2}a_0 $ is the average of $f$, i.e.
        \[
            \langle f \rangle = \frac{1}{2L}\int_{0}^{2L} f(x) \,\mathrm{d}x.
        \]
        \item Range of integration can be changed as long as it has length $2L$. We usually consider $ \int_{-L}^{L} $.
        \item Can think of the Fourier series as the decomposition into harmonics. The simplest Fourier series are sines and cosines. e.g. $ \sin (3 \pi x/ L) $ has Fourier series $b_3=1$ and $ b_n=0,n\neq 3 ,a_n=0$.
    \end{enumerate}
\end{note}
\begin{example}[Sawtooth wave]
    Consider $f(x)=x,x\in [-L,L)$ and periodic elsewhere.
    \begin{center}
        \includegraphics[scale=0.1]{methods2.jpeg}
    \end{center}
    We have 
    \begin{align*}
        a_n &= \frac{1}{L}\int_{-L}^{L} x\cdot \cos \left( \frac{n\pi x}{L} \right) \,\mathrm{d}x=0,\\ 
        b_n &= \frac{2}{L} \int_{0}^{L} x\cdot \sin \left( \frac{n\pi x}{L} \right) \,\mathrm{d}x = \frac{2L}{n\pi }(-1)^{n+1}.
    \end{align*}
    So the Fourier series is 
    \begin{equation}
        f(x) = \frac{2L}{\pi } \sum_{n=1}^{\infty}\frac{(-1)^{n+1}}{n}\sin \left( \frac{n\pi x}{L} \right).
    \end{equation}
\end{example}
\begin{note}
    As $n\to \infty$ in Fourier series,
    \begin{enumerate}
        \item The FS approximation improves (convergent where continuous).
        \item FS $ \to 0 $ at $x=L$, i.e. the midpoint of continuity.
        \item FS has a persistent \textit{overshoot} at $x=L$, which is approximately 9\% and is known as the \textit{Gibbs phenonenon}. See Example Sheet Q5.
    \end{enumerate}
\end{note}

\subsection{The Dirichlet Conditions and Fourier's Theorem}
A natural question is then which functions are allowed to have a proper Fourier series.
Surprisingly, a big, yet hard to precisely characterise, class of functions has a convergent Fourier series that has the desired properties.
This class even includes some classical counterexamples in analysis.
As an applied course, we will just look at some of the sufficient conditions.
\begin{theorem}[Fourier's Theorem]
    If $f$ is a bounded periodic function with period $2L$ with a finite number of minima, maxima, and discontinuities in $[0,2L)$, then its Fourier series converges to $f$ where it is continuous and converges to the average of the two side limits.
\end{theorem}
The conditions in this theorem is known as the Dirichlet conditions.
\begin{note}
    \begin{enumerate}
        \item These conditions are hella weak compared to our conditions for a function to have e.g. a Taylor series.
        However, pathological functions like $1/x,\sin(1/x),1_{\mathbb R\setminus\mathbb Q}(x)$ are excluded from these conditions.
        \item The converse is not true, as $\sin(1/x)$ has a Fourier series we desire.
    \end{enumerate}
\end{note}
\begin{proof}
    See Jeffery \& Jeffery book.
\end{proof}
Another subject of interest is the rate of convergence of a Fourier series.
Perhaps unsurprisingly, it depends on the smoothness of the function.
\begin{theorem}
    If $f(x)$ is $p^{th}$ differentiable but $f^{(p)}$ is not continuous, then its Fourier series converges as $O(n^{-(p+1)})$ as $n\to\infty$.
\end{theorem}
\begin{example}
    \begin{enumerate}
        \item  Consider the square wave
        $$f(x)=\begin{cases}
            1\text{, for $0\le x<1$}\\
            -1\text{, for $-1\le x<0$}
        \end{cases}$$
        That extends periodically with period $2$.
        Then it has a Fourier series
        \begin{equation}
            f(x)=4\sum_{m=1}^\infty\frac{\sin[(2m-1)\pi x]}{(2m-1)\pi}
        \end{equation}
        which, as one can see both from the preceding theorem (with $p=0$) and observation, converges slowly.
        \item Consider the general ``see-saw'' wave
        $$f(x)=\begin{cases}
            x(1-\xi)\text{, for $x\in[0,\xi)$}\\
            \xi(1-x)\text{, for $x\in[\xi,1)$}
        \end{cases}$$
        which extends as an odd periodic function with period $2$.
        This has Fourier series
        \begin{equation}
            f(x)=2\sum_{m=1}^\infty\frac{\sin (n\pi\xi)\sin (n\pi x)}{(n\pi)^2}
        \end{equation}
        which converges with $p=1$ in the preceding theorem.
        In particular, $\xi=1/2$ gives
        $$2\sum_{m=1}^\infty(-1)^{m+1}\frac{\sin[(2m-1)\pi x]}{[(2m-1)\pi]^2}$$
        which can be seen, immediately, that it converges faster than the series in the previous example.
        \item Take $f(x)=x(1-x)/2$ for $x\in[0,1)$ that extends as an odd periodic function with period $2$.
        Then its Fourier series is
        \begin{equation}
            f(x)=4\sum_{m=1}^\infty\frac{\sin[(2m-1)\pi x]}{[(2m-1)\pi]^3}
        \end{equation}
        which has $p=2$.
        \item Take $f(x)=(1-x^2)^2$, then $a_n=O(n^{-4})$.
    \end{enumerate}
\end{example}
Of course, we want to integrate and differentiate a Fourier series term-by-term.
Integration, as one expect, seldom yields problems as it imposed very few restrictions on the function.

And indeed, we are just going to assume we can integrate any Fourier series term-by-term and they guarantee to yield a smoother function, which satisfies the Dirichlet conditions if the original function does.

Differentiation is more problematic when doing it term-by-term.
\begin{example}
    Take the square wave again which is known to have Fourier series
    $$4\sum_{m=1}^\infty\frac{\sin[(2m-1)\pi x]}{(2m-1)\pi}$$
    which, after term-by-term differentiation, yields
    $$4\sum_{m=1}^\infty\cos[(2m-1)\pi x]$$
    which is clearly divergent.
    This is perhaps unsurprising as the original function is not even continuous.
\end{example}
\begin{theorem}
    If $f(x)$ is differentiable and both $f,f^\prime$ satisfy Dirichlet conditions, then we can differentiate the Fourier series of $f$ term-by-term to get the Fourier series of $f^\prime$.
\end{theorem}
\begin{example}
    If we differentiate the see-saw curve with $\xi=1/2$, then we will get an offset of the Fourier series of the square wave with $ \tilde{x}=x+1/2 $.
\end{example}
\subsection{Parseval's Theorem}
There is some interesting relation between the integral of the square of a function and the square of the Fourier coefficients of that function.
If the function is nice enough to have a nice enough Fourier series, then by orthogonality,
\begin{align*}
    \int_0^{2L}f(x)^2\,\mathrm dx&=\int_0^{2L}\left( \frac{a_0}{2}+\sum_{n\ge 1}a_n\cos\frac{n\pi x}{L}+\sum_{n\ge 1}b_n\sin\frac{n\pi x}{L} \right)^2\,\mathrm dx\\
    &=\int_0^{2L}\left( \frac{a_0^2}{4}+\sum_{n\ge 1}a_n^2\cos^2\frac{n\pi x}{L}+\sum_{n\ge 1}b_n^2\sin^2\frac{n\pi x}{L} \right)\,\mathrm dx\\
    &=L\left( \frac{a_0^2}{2}+\sum_{n\ge 1}(a_n^2+b_n^2) \right).
\end{align*}
This is also called the \textit{completeness relation} as the left hand side would be greater than or equal to the right hand side if any basis functions are missing from the series.
This is known as Parseval's Theorem.
\begin{theorem}[Parseval's Theorem]\label{parseval}
    For a nice enough function $f$ with Fourier coefficients $a_n,b_n$, we have
    \begin{equation}
        \int_0^{2L}f(x)^2\,\mathrm dx=L\left( \frac{a_0^2}{2}+\sum_{n\ge 1}(a_n^2+b_n^2) \right).
    \end{equation}
\end{theorem}
\begin{proof}
    Above.
\end{proof}
\begin{example}
    Consider the sawtooth curve with $f(x)=x,x\in[-L,L)$ with period $2L$.
    Then Parseval's Theorem reveals that
    $$\frac{2}{3}L^3=\int_{-L}^Lx^2\,\mathrm dx=L\sum_{n=1}^\infty\frac{4L^2}{n^2\pi^2}=\frac{4L^3}{\pi^2}\sum_{n=1}^\infty\frac{1}{n^2}\implies \sum_{n=1}^\infty\frac{1}{n^2}=\frac{\pi^2}{6}.$$
\end{example}
\begin{remark}
    If we think of the integral of the square as $ \langle f,f \rangle = \left\| f \right\| ^2  $, the $ L^2 $ norm, then Parseval's Theorem can be thought of an analog of Pythagoras' Theorem in this space of functions.
\end{remark}

\subsection{Alternative Fourier Series}
\paragraph{Fourier sines and cosines.}
Consider a function $f:[0,L)\to\mathbb R$.
We can extend $f$ to a periodic function of period $2L$ in two ways:
\begin{enumerate}
    \item We can require the function to be odd, then $a_n=0$ for all $n$ and
    \begin{equation}
        b_n=\frac{2}{L}\int_0^Lf(x)\sin\frac{n\pi x}{L}\,\mathrm dx
    \end{equation}
    and the Fourier series would be $\sum_{n\ge 1}b_n\sin(n\pi x/L)$, which is called a \textit{Fourier sine series}.
    The sawtooth function is an example of this.
    \item We can require the function to be even, then $b_n=0$ for all $n$ and
    \begin{equation}
        a_n=\frac{2}{L}\int_0^Lf(x)\cos\frac{n\pi x}{L}\,\mathrm dx
    \end{equation}
    So the Fourier series is $a_0/2+\sum_{n\ge 1}a_n\cos(n\pi x/L)$.
    This is called a \textit{Fourier cosine series}.
    $f(x)=(1-x^2)^2$ is an example (where $L=1$).
\end{enumerate}
\paragraph{Complex Fourier series.}
The actual thing we want is to represent the Fourier series more neatly in terms of exponentials.
We know that
$$\cos\frac{n\pi x}{L}=\frac{e^{in\pi x/L}+e^{-in\pi x/L}}{2},\sin\frac{n\pi x}{L}=\frac{e^{in\pi x/L}-e^{-in\pi x/L}}{2i}$$
So by writing $c_0=a_0/2$ and
$$c_m=\begin{cases}
    (a_m-ib_m)/2&\text{for $m>0$}\\
    (a_{-m}+ib_{-m})/2&\text{for $m<0$}
\end{cases}$$
we obtain
\begin{equation}
    f(x)=\frac{a_0}{2}+\sum_{n=1}^\infty a_n\cos\frac{n\pi x}{L}+\sum_{n=1}^\infty b_n\sin\frac{n\pi x}{L}=\sum_{m=-\infty}^\infty c_me^{im\pi x/L}
\end{equation}
Equivalently, if we extend our inner product to the complex functions
$$\langle f,g\rangle=\int_{-L}^Lf(x)\overline{g(x)}\,\mathrm dx$$
then we obtain
\begin{equation}
    c_m=\frac{1}{2L}\langle f(x),e^{im\pi x/L}\rangle=\frac{1}{2L}\int_{-L}^Lf(x)e^{-im\pi x/L}\,\mathrm dx
\end{equation}
because of orthogonality
\begin{equation}
    \langle e^{im\pi x/L},e^{in\pi x/L}\rangle= \int_{-L}^{L} e^{-im\pi x/L}e^{in\pi x/L} \,\mathrm{d}x=2L\delta_{mn}
\end{equation}
By thinking them as a set of basis of a space of nice-enough functions in the way we did for $\sin$ and $\cos$.
Parseval's Theorem can then be stated as
$$\int_{-L}^L|f(x)|^2\,\mathrm dx=2L\sum_{n=-\infty}^\infty|c_n|^2$$
\subsection{Some Motivations of Fourier Series}
\begin{definition}
    The complex inner product $\langle\cdot\rangle:\mathbb C^N\times\mathbb C^N\to\mathbb C$ is defined by
    \begin{equation}
        \langle\mathbf{u},\mathbf{v}\rangle=\mathbf{u}^\dagger\mathbf{v}
    \end{equation}
\end{definition}
\begin{definition}
    An $N\times N$  matrix $A$ is \textit{self-adjoint} (or \textit{Hermitian}) if
    $$\forall\mathbf{u},\mathbf{v}\in\mathbb C^N,\langle A\mathbf{u},\mathbf{v}\rangle=\langle\mathbf{u},A\mathbf{v}\rangle.$$
\end{definition}
One can show that this is just saying $A^\dagger=A$.
It can be shown that
\begin{proposition}
    Let $A$ be a self-adjoint matrix. Then the eigenvalues $ \lambda_n $ and eigenvectors $ \bfv_n $ satisfy
    \begin{equation}
        A \bfv_n=\lambda_n\bfv_n
    \end{equation}
    and hence the followings hold:
    \begin{enumerate}[(i).]
        \item All eigenvalues are real for all $n$.
        \item Eigenvectors associated with different eigenvalues are orthogonal with respect to $\langle\cdot\rangle$.
        \item We can rescale to make an orthonormal basis of $\mathbb C^N$ of eigenvectors $\{\mathbf{v}_1,\ldots,\mathbf{v}_N\}$.
    \end{enumerate}
\end{proposition}
\paragraph{Recap of solving linear equations.}
Now, given any $\mathbf{b}$, if we want to solve for $\mathbf{x}$ in
\begin{equation}
    A\mathbf{x}=\mathbf{b}
\end{equation}
then a way to do it is to express $\mathbf{b}=\sum_nb_n\mathbf{v}_n$ and observe that if $\sum_nc_n\mathbf{v}_n$ is a solution then
$$\sum_nb_n\mathbf{v}_n=A\left(\sum_{n=1}^Nc_n\mathbf{v}_n\right)=\sum_{n=1}^Nc_n\lambda_n\mathbf{v}_n$$
where $\lambda_n$ is the eigenvalue associated with $\mathbf{v}_n$.
So if $A$ is nonsingular, then none of the $\lambda_n$ is zero and we can write $c_n=b_n/\lambda_n$ and get the solution
\begin{equation}
    \mathbf{x}=\sum_{n=1}^N\frac{b_n}{\lambda_n}\mathbf{v}_n
\end{equation}
This means we can solve an linear equation if there is a basis consisting of eigenvectors of the matrix.
\paragraph{Analogy to ODEs.}
Consider the differential operator
$$\mathcal Ly=-\frac{\mathrm d^2y}{\mathrm dx^2}$$
and suppose we want to solve
\begin{equation}
    \mathcal{L}y = f(x)\quad \text{subject to}\quad y(0)=y(L)=0.
\end{equation}
The related eigenvalue problem is then $\mathcal L y_n=\lambda_ny_n$ with $y_n(0)=y_n(L)=0$ which has solutions
\begin{equation}\label{1.eq.21}
    y_n(x)=\sin\frac{n\pi x}{L},\quad \lambda_n=\left( \frac{n\pi}{L} \right)^2.
\end{equation}
So we will want to write
\begin{align*}
    &y(x)=\sum_{n=1}^\infty c_n\sin\frac{n\pi x}{L},\\ 
    &f(x)=\sum_{n=1}^\infty b_n\sin\frac{n\pi x}{L},\\ 
    &b_n=\frac{2}{L}\int_0^Lf(x)\sin\frac{n\pi x}{L}\,\mathrm dx,
\end{align*}
assuming convergence.
Then this substitution yields
$$\sum_{n=1}^\infty b_n\sin\frac{n\pi x}{L}=\mathcal Ly=-\frac{\mathrm d^2y}{\mathrm dx^2}\left( \sum_{n=1}^\infty c_n\sin\frac{n\pi x}{L} \right)=\sum_{n=1}^\infty c_n\left( \frac{n\pi}{L} \right)^2\sin\frac{n\pi x}{L}$$
Hence, $c_n=b_n(L/(n\pi))^2$ by orthogonality, so we can get a particular solution of the problem in the form
\begin{equation}
    y(x)=\sum_{n=1}^\infty \frac{b_n}{\lambda_n}y_n = \sum_{n=1}^{\infty}b_n \left( \frac{L}{n\pi} \right)^2 \sin \frac{n\pi x}{L}.
\end{equation}
\begin{example}\label{odd_sq_fourier_ode}
    Consider the problem $ \mathcal{L}y=f(x) $ with $L=1$ and set $f$ to be the odd square wave with $f(x)=1$ for $x\in[0,1)$.
    This has Fourier series
    $$f(x)= 4\sum_{m=1}^\infty\frac{\sin[(2m-1)\pi x]}{(2m-1)\pi}.$$
    So the above discussion instantly yield a solution
    $$y(x)=\sum_{n=1}^\infty \frac{b_n}{\lambda_n}y_n=4\sum_{m=1}^\infty\frac{\sin[(2m-1)\pi x]}{[(2m-1)\pi]^3}$$
    which is the Fourier series of
    \begin{equation}
        y(x)=\frac{x(1-x)}{2}\quad \text{on}\quad[0,1)
    \end{equation}
    extending as an odd periodic function with period $2$.

    Indeed, as one can verifty, if we integrate $\mathcal L y=1$ directly with the appropriate boundary conditions, we can get basically the same solution.
\end{example}
\subsection{A Glimpse into Green's Functions}
In the problem $ \mathcal{L}y=f(x) $, fix $L=1$ and consider an odd function $f$.
We have
\begin{align}
    y(x)&=\sum_{n=1}^\infty\frac{b_n}{\lambda_n}\sin(\pi x)\nonumber\\
    &=\sum_{n=1}^\infty\frac{2}{(n\pi)^2}\left(\int_0^1f(\xi)\sin(n\pi\xi)\,\mathrm d\xi\right)\sin(n\pi x)\nonumber\\
    &=\int_0^12\sum_{n=1}^\infty\frac{\sin(n\pi x)\sin(n\pi\xi)}{(n\pi)^2}f(\xi)\,\mathrm d\xi\tag{Assume swapping limits}\\
    &=\int_0^1G(x,\xi)f(\xi)\,\mathrm d\xi
\end{align}
where
\begin{equation}
    G(x,\xi)=2\sum_{n=1}^\infty\frac{\sin(n\pi x)\sin(n\pi\xi)}{(n\pi)^2}.
\end{equation}
It is exactly the general see-saw wave
\begin{equation}
    G(x,\xi)=\begin{cases}
        x(1-\xi)\text{, for $x\in[0,\xi)$}\\
        \xi(1-x)\text{, for $x\in[\xi,1)$}
    \end{cases}
\end{equation}
This is the \textit{Green's function} for this ODE $-\rmd^2 y/\rmd x^2 =f(x)$.
One can actually solve this integral and get what we got in Example \ref{odd_sq_fourier_ode}.

\section{Sturm-Liouville Theory}
\subsection{Review of Second-Order Linear ODEs}
For a general inhomogeneous ODE $\mathcal Ly=f(x)$ where
\begin{equation}
    \mathcal Ly=\alpha(x)\frac{\mathrm d^2y}{\mathrm dx^2}+\beta(x)\frac{\mathrm dy}{\mathrm dx}+\gamma(x)y
\end{equation}
In general, the homogeneous equation
\begin{equation}
    \mathcal Ly=0
\end{equation}
has two linearly independent solutions $y_1,y_2$.
The complementary function
\begin{equation}
    y_c(x)=Ay_1+By_2
\end{equation}
for constants $A,B$ is then the general solution to $\mathcal Ly=0$ by linearity.

For the inhomogeneous problem 
\begin{equation}
    \mathcal Ly=f(x)
\end{equation}
If we can find a particular solution (aka particular integral) $y_p$ to $\mathcal Ly=f$, then
\begin{equation}
    y=y_p+y_c=y_p+Ay_1+By_2
\end{equation}
for $A,B$ constants is the general solution to $\mathcal Ly=f$ again by linearity.
Two pieces of boundary data is then needed to determine the constants $A,B$.

There are several types of boundary conditions.
\begin{itemize}
    \item The Dirichlet condition of specifying the function's value at the endpoints, \item The Neumann condition of specifying the derivative's values at the endpoints.
    \item Mixed conditions of above.
\end{itemize}

The sort of conditions we often consider are homogeneous conditions, i.e. the function vanishes at the endpoints.
The reason of it is that it allows the \textit{superposition} of solutions in a linear DE.
What if we come across a inhomogeneous condition?
We can use the complementary solution to cancel stuff out.

Sometimes, we specify initial data of the function and its derivative as boundary conditions.

Another matter of interest is the general eigenvalue problem.
To solve $\mathcal Ly=f$ using eigenvalue decompositions like we did previously, we must first solve (subject to boundary conditions) the related eigenvalue problem
\begin{equation}
    \mathcal Ly=\alpha(x)\frac{\mathrm d^2y}{\mathrm dx^2}+\beta(x)\frac{\mathrm dy}{\mathrm dx}+\gamma(x)y=-\lambda\rho(x)y
\end{equation}
where $\rho$ is nonegative.
This form often occurs after seperation of variables in a PDE.
\subsection{Self-Adjoint Operators}
\begin{definition}
    For two functions $f,g:[a,b]\to\mathbb C$ we define their inner product to be
    $$\langle f,g\rangle=\int_a^bf^*(x)g(x)\,\mathrm dx$$
\end{definition}

\begin{definition}[Sturm-Liouville form]
    An ODE $ \mathcal{L}y=f(x) $ can be written as the \textit{Sturm-Liouville form} if $ \mathcal{L} $ can be written as
    \[
        \mathcal{L}y=-(py')'+qy,
    \]
    where $ p,q$ are functions, and the ODE $ \mathcal{L}y=f(x) $ can be written as 
    \begin{equation}
        \mathcal{L}y=-(py')'+qy=\lambda w y,
    \end{equation}
    where $w$ is a nonnegative weight function.
\end{definition}

We can guarantee to rewrite the original eigenvalue problem into the Sturm-Liouville form.

\paragraph{Integrating factor} How to convert a second order linear ODE to this form?
Simply multiply the diffential equation by an integrating factor $F$ that will be specified later and we can write
\begin{align*}
    &F \alpha y''+F \beta y'+ F \gamma y = -\lambda F \rho y\\ 
    \implies&\frac{\mathrm d}{\mathrm dx}(F\alpha y^\prime)-F^\prime\alpha y^\prime-F\alpha^\prime y^\prime+F\beta y^\prime+F\gamma y=-\lambda F\rho y.
\end{align*}
So to eliminate the $y^\prime$ terms, we set
\begin{equation}
    F^\prime\alpha y^\prime+F\alpha^\prime y^\prime-F\beta y^\prime=0 \Longrightarrow  F(x)=\exp\left(\int\frac{\beta-\alpha^\prime}{\alpha}\,\mathrm dx\right)
\end{equation}
which reduced the equation to
$$(F\alpha y^\prime)^\prime+F\gamma y=-\lambda F\rho y \Longleftrightarrow -(py')'+qy=\lambda w y,$$
setting $p=F\alpha,q=-F\gamma$ and $w=F\rho\ge 0$.
\begin{example}
    Consider the Hermite equation that appears in quantum mechanics
    $$y^{\prime\prime}-2xy^\prime+2ny=0$$
    Then $\alpha=1,\beta=-2x,\gamma=0,\lambda\rho=2n$, so the above procedure translates this to the Sturm-Liouville form
    \begin{equation}\label{2.eq.9}
        \mathcal Ly=(-e^{-x^2}y^\prime)^\prime=2ne^{-x^2}y
    \end{equation}
\end{example}
\subsubsection*{Boundary conditions}

\begin{definition}[Self-adjoint operators]
    Let $\mathcal L:C\to C$ be an operator, where $C$ on a class of functions $[a,b]\to\mathbb C$ equipped with the inner product above.
    $\mathcal L$ is \textit{self-adjoint} if $\langle y_1,\mathcal Ly_2\rangle=\langle\mathcal Ly_1,y_2\rangle$ for any $y_1,y_2\in C$. Equivalently,
    \begin{equation}
        \int_{a}^{b} y_1^*(x)\mathcal{L}y_2(x) \,\mathrm{d}x = \int_{a}^{b} (\mathcal{L}y_1(x))^* y_2(x) \,\mathrm{d}x
    \end{equation}
\end{definition}

If we let $\mathcal L$ be the operator in the Strum-Liouville form, then
\begin{align}
    \langle y_1,\mathcal Ly_2\rangle-\langle\mathcal Ly_1,y_2\rangle&=\int_a^b[-y_1(py_2^\prime)^\prime+y_1qy_2+y_2(py_1^\prime)^\prime-y_2qy_1]\,\mathrm dx\nonumber\\
    &=\int_a^b[-y_1(py_2^\prime)^\prime+y_2(py_1^\prime)^\prime]\,\mathrm dx\nonumber\\
    &=\int_a^b[-(y_1(py_2^\prime)^\prime+y_1^\prime py_2^\prime)+(y_2(py_1^\prime)^\prime+y_2^\prime py_1^\prime)]\,\mathrm dx\nonumber\\
    &=\int_a^b[-(py_1y_2^\prime)^\prime+(py_1^\prime y_2)^\prime]\,\mathrm dx\nonumber\\
    \implies \langle y_1,\mathcal Ly_2\rangle-\langle\mathcal Ly_1,y_2\rangle&=[-py_1y_2^\prime+py_1^\prime y_2]_a^b
\end{align}
So for this operator to be self-adjoint, we need some good enough boundary conditions so that enough stuff vanishes.
This includes homogeneous boundary condition $y(a)=y(b)=0$ or $y^\prime(a)=y^\prime(b)=0$ or mixed $y+ky^\prime=0$ etc.

We say a Sturm-Liouville problem is \textit{regular} if the boundary conditions are homogeneous.
Periodic boundary conditions also work, where we can take $y(a)=y(b)$ and the derivatives are specified (or periodic) at the boundary.
There can also be singular points of this ODE, where $p(a)=p(b)=0$.
We can have combinations of above too.

\subsection{Properties of Self-Adjoint Operators}
Analogous to the finite dimensional case, we have
\begin{theorem}\label{self-adjoint}
    For a sufficiently nice self-adjoint operator $\mathcal L$ on a sufficiently nice space of functions:
    \begin{enumerate}[(a)]
        \item Eigenvalues of $\mathcal L$ are real.
        \item Eigenfunctions of it with different eigenvalues are orthogonal with respect to the weight $w$.
        \item We can take the eigenfunctions as a set of basis for the function space, just like Fourier series.
    \end{enumerate}
\end{theorem}
\begin{proof}[Proof of (a)]
    Given
    \begin{equation}\label{2.eigenproblem}
        \mathcal Ly=\lambda wy
    \end{equation}
    taking complex conjugate gives $\mathcal Ly^*=\lambda^*wy^*$.
    Hence as $\mathcal L$ is self-adjoint,
    $$0=\int_a^b(y^*\mathcal Ly-y\mathcal Ly^*)\,\mathrm dx=(\lambda-\lambda^*)\int_a^bw|y|^2\,\mathrm dx$$
    which means $\lambda=\lambda^*$, so $\lambda$ is real.
\end{proof}
If $\lambda$ is non-degenerate (simple), i.e. it has a one-dimensional eigenspace, then $y$ is guaranteed to be real.
Even if it has dimension $2$ (not more because the ODE is second order), we can still find two real functions as basis of the eigenspace. Hence from now on, we assume the eigenfunctions to be real.

Also, by considering $u\mathcal Lv-v\mathcal Lu=(-p(uv^\prime-u^\prime v))^\prime$, one can show that a regular Sturm-Liouville problem always has all eigenvalues simple.

\begin{proof}[Proof of (b)]
    Suppose $\mathcal Ly_m=\lambda_mwy_m$ and $\mathcal Ly_n=\lambda_nwy_n$, then
    \begin{equation}\label{2.eq.13}
        0=\int_a^by_n\mathcal Ly_m-y_m\mathcal Ly_n\,\mathrm dx=(\lambda_m-\lambda_n)\int_a^bwy_ny_m\,\mathrm dx
    \end{equation}
    But $\lambda_m$ and $\lambda_n$ are distinct.
    The claim follows.
\end{proof}

\begin{definition}
    The inner product of $y_1,y_2:[a,b]\to\mathbb C$ with respect to weight $w:[a,b]\to\mathbb R_{\ge 0}$ is
    \begin{equation}
        \langle f,g\rangle_w=\int_a^bwf^*g\,\mathrm dx=\langle wf,g\rangle=\langle f,wg\rangle
    \end{equation}
\end{definition}

So the orthogonality in (\ref{2.eq.13}) becomes 
\begin{equation}
    \langle y_n,y_m \rangle _w = 0,\quad \forall n\neq m.
\end{equation}

\begin{remark}
    Watch the weight!
\end{remark}

As an aside, we do not really need the weight function in order to formulate Sturm-Liouville theory, since we can do the transformation $\tilde{y}=\sqrt{w}y$ and replace $\mathcal Ly$ by $(1/\sqrt{w})\mathcal L(\tilde{y}/\sqrt{w})$.
Yet the analytic property is generally simpler if we keep $w$.
\begin{example}
    For the Hermite equation \ref{2.eq.9}, we can eliminate $ w $ with $ \tilde{y}=e^{-x^2/2}y $ to find 
    \[
        \tilde{\mcL} = -\tilde{y}''+(x^2-1)\tilde{y}=2n \tilde{y}.
    \]
\end{example}
\subsection{Eigenfunction expansion}
Completeness ((c) in theorem \ref{self-adjoint}) implies we can approximate any well-behaved $f(x)$ on $ [a,b] $ by the series 
\begin{equation}
    f(x)=\sum_{n=1}^\infty a_ny_n(x)
\end{equation}
where $y_n$ is a set of eigenfunctions of some self-adjoint operator.
To find the coefficients $a_n$, we can use the orthogonality (\ref{2.eq.13}) to get
\[
    \int_{a}^{b} w(x)y_m(x)\cdot f(x) \,\mathrm{d}x = \sum_{n=1}^{\infty}a_n \int_{a}^{b} wy_ny_m \,\mathrm{d}x = a_m\int_a^bwy_m^2\,\mathrm dx
\]
Hence 
\begin{equation}\label{2.eq.17}
    a_n=\left(\int_a^bwy_nf\,\mathrm dx\middle)\right/\left(\int_a^bwy_n^2\,\mathrm dx\right)
\end{equation}
It's a common practice not to normalise the eigenfunctions as it is not really always clean.
Of course, if we want, we can always write down
\[
    Y_n=y_n\left/\sqrt{\int_a^bwy_n^2\,\mathrm dx}\right.
\]
So that 
\begin{equation}
    \langle Y_n,Y_m \rangle =\delta_{nm}
\end{equation}
are orthonormal with 
\[
    f(x) = \sum_{n=1}^{\infty}A_nY_n(x)\quad \text{and}\quad A_n = \int_{a}^{b} w(x)Y_n(x)f(x) \,\mathrm{d}x
\]
\begin{example}
    Recall the particular operator already in Sturm-Liouville form $\mathcal Ly=-y^{\prime\prime}$, then (with appropriate boundary conditions) we can easily get the eigenvalues $\lambda_n=(n\pi/L)^2$ and eigenfunctions being the trigonometrics.
    This just reproduces the Fourier series.
\end{example}
\subsection{Completeness and Parseval's Identity}
We expand
\begin{align*}
    &\int_a^bw\left( f(x)-\sum_{n=1}^\infty a_ny_n \right)^2\,\mathrm dx\\
    =&\int_a^bw\left( f^2-2f\sum_{n=1}^\infty a_ny_n+\sum_{n=1}^\infty a_n^2y_n^2 \right)\,\mathrm dx\tag{Last term by orthogonality} \\    
    =&\int_a^bwf^2\,\mathrm dx-\sum_{n=1}^\infty a_n^2\int_a^bwy_n^2\,\mathrm dx\tag{By \ref{2.eq.17}}
\end{align*}
If the eigenfunctions are complete, then the series expansion converges, and thus the above expansion is 0 and 

\begin{align}\label{2.eq.19}
    \int_a^bwf^2\,\mathrm dx&=\sum_{n=1}^\infty a_n^2\int_a^bwy_n^2\,\mathrm dx\\ 
    &=\sum_{n=1}^\infty A_n^2\text{ for unit normal $Y_n$}\nonumber
\end{align}
Note that $\int_a^bwy_n^2\,\mathrm dx$ becomes $L$ for Fourier series.

If some of the eigenfunctions are missing from the series, then this gives
$$\int_a^bwf^2\,\mathrm dx\ge\sum_{n=1}^\infty A_n^2$$
This is known as Bessel's Inequality.
\paragraph{Calculating the error}
Consider the partial sums $\sum_{n\le N}a_ny_n$, then we shall have
\begin{equation}\label{2.eq.20}
    f(x) = \lim_{N \to \infty} S_N(x)
\end{equation}
Convergence is defined in terms of the \textit{mean square error}
\[
    \epsilon_N=\int_a^bw[f(x)-S_N(x)]^2\,\mathrm dx\to 0,N\to\infty
\]
This global definition of convergence in mean is not the pointwise convergence of Fourier series, which converges to all the continuous points of $f$.

An interesting question is that, while we know (maybe) the series converges as we want, if we truncate the sequence in some $N$, would the coefficients $\{a_n\}_{n\le N}$ provide the best approxmation (with respect to the error defined in this way) of that particular partial sum, or a different set of partial coefficient will yield a better result?
The answer is yes. 
\begin{claim}
    The error in partial sums (\ref{2.eq.20}) is minimised by $a_n$ in (\ref{2.eq.19}) for $ N= \infty  $ expansion.
\end{claim}
\begin{proof}
    Differentiate wrt $a_n$ gives
    $$\frac{\partial\epsilon_N}{\partial a_n}=-2\int_a^bwy_n\left( f-\sum_{k=1}^Na_ky_k \right)\,\mathrm dx=-2\int_a^bwfy_n-a_nwy_n^2\,\mathrm dx$$
    which is zero when $a_n$ is of the expression in (\ref{2.eq.17}).
    We can see it is indeed a minimum by observing that
    $$\frac{\partial^2\epsilon_N}{\partial a_n^2}=2\int_a^bwy_n^2\,\mathrm dx\ge 0$$
    This answers our question.
\end{proof}
\subsection{Legendre's Equation}
Take the usual spherical polar coordinate
$$\begin{cases}
    x=r\sin\theta\cos\phi\\
    y=r\sin\theta\sin\phi\\
    z=r\cos\theta
\end{cases}$$
where Laplace's equation $\nabla^2 u=0$ translates to
$$\frac{1}{r^2}\frac{\partial}{\partial r}\left( r^2\frac{\partial u}{\partial r} \right)+\frac{1}{r^2\sin\theta}\frac{\partial}{\partial\theta}\left( \sin\theta\frac{\partial u}{\partial\theta} \right)+\frac{1}{r^2\sin^2\theta}\frac{\partial^2u}{\partial\phi^2}=0$$
Seperation of variables $u=R(r)\Theta(\theta)\Phi(\phi)$ then gives
$$\frac{1}{\sin\theta}(\Theta^\prime\sin\theta)^\prime+\left( K-\frac{m^2}{\sin^2\theta} \right)\Theta=0$$
where $K,m$ are constants which essentially makes it an eigenvalue problem.

Now the transformation $x=\cos\theta\in[-1,1]$ and renaming $\Theta$ as $y$ then gives Legendre's Equation
\begin{equation}\label{2.eq.21:legendre}
    (1-x^2)y^{\prime\prime}-2xy^\prime+\lambda y=0
\end{equation}
where $\lambda$ is a constant which is again intepreted as an eigenvalue.
This is already in Strum-Liouville form by taking $p=1-x^2,q=0,w=1$.
Now $p=1-x^2$ vanishes at the boundary $\pm 1$, so this equation has to be self-adjoint.
We assume that $y$ is bounded near the boundary. (Regular singular point)

We now seek a power series solution to the problem.
If we set
$$y=\sum_{n=0}^\infty c_nx^n$$
Then substitution gives 
\begin{align}\label{2.eq.22}
    &(1-x^2)\sum_n n(n-1)c_nx^{n-2}-2x \sum_n c_n x^{n-1} + \lambda \sum_n c_n x^n=0\nonumber\\
    \Longrightarrow & (n+2)(n+1)c_{n+2}-n(n-1)c_n-2nc_n+\lambda c_n=0\nonumber\\ 
    \implies &c_{n+2}=\frac{n(n+1)-\lambda}{(n+1)(n+2)}c_n
\end{align}
Specifying $c_0,c_1$ gives 2 independent solution near $ x=0 $:
\begin{align*}
    y_{\text{even}}&=c_0\left( 1+\frac{(-\lambda)}{2!}x^2+\frac{(6-\lambda)(-\lambda)}{4!}x^4+\cdots \right)\\
    y_{\text{odd}}&=c_1\left( x+\frac{2-\lambda}{3!}x^3+\cdots \right)
\end{align*}
Note that $c_{n+2}/c_n\to 1$, so the both series has radius of convergence $1$ but they diverges at $x=\pm 1$.

These series may not be infinite.
If $\lambda=l(l+1)$ for some $l\in\mathbb N$, then one of these two series will terminate and give a polynomial solution.
These polynomials are called \textit{Legendre polynomials} $P_l(x)$ which are eigenfunctions of the Legendre equation (\ref{2.eq.21:legendre}).

Conventionally we normalise $P_l$ by requiring $P_l(1)=1$.
One can check that this restricts $P_l([-1,1])\subset [-1,1]$ and $|P_l(-1)|=1$.
By calculation we have
\begin{align*}
    P_{0}(x)&=1\\ 
    P_{1}(x)&=x\\ 
    P_{2}(x)&=\frac{1}{2}\left(3 x^{2}-1\right)\\ 
    P_{3}(x)&=\frac{1}{2}\left(5 x^{3}-3 x\right)
\end{align*}
Note that 
\begin{enumerate}
    \item $P_l$ has $l$ roots in $[-1,1]$.
    \item $P_l$ is odd if $l$ is odd, and even when $l$ is even.
\end{enumerate}

\subsection{Properties of Legendre polynomials}
Since Legendre polynomials come from a self-adjoint operator, they must have certain conditions, such as orthogonality.
\begin{proposition}
    For \( n \neq m \), $\displaystyle \int_{-1}^1 P_n P_m \dd{x} = 0$. 
\end{proposition}
They are also normalisable,
\begin{proposition}
    $\displaystyle 
	\int_{-1}^1 P_n^2 \dd{x} = \frac{2}{2n+1}$
\end{proposition}
We can prove this with Rodrigues' formula:
\begin{proposition}
    $\displaystyle P_n(x) = \frac{1}{2^n n!} \qty( \dv{x} )^n (x^2 - 1)^n$ 
\end{proposition}
Alternatively we could use a generating function:
\begin{proposition}
    \begin{align*}
        \sum_{n=0}^\infty P_n(x) t^n = \frac{1}{\sqrt{1 - 2xt + t^2}} & = 1 + \frac{1}{2}\qty(2xt - t^2) + \frac{3}{8}\qty(2xt - t^2)^2 + \dots \\
                                                                      & = 1 + xt + \frac{1}{2}\qty(3x^2 - 1)t^2 + \dots
    \end{align*}
\end{proposition}
There are some useful recursion relations.
\begin{proposition}\
    \begin{enumerate}[(i)]
        \item $ \ell(\ell + 1) P_{\ell + 1} = (2 \ell + 1) x P_\ell(x) - \ell P_{\ell - 1}(x) $
        \item $\displaystyle (2\ell + 1)P_\ell(x) = \dv{x} \qty[ P_{\ell + 1}(x) - P_{\ell - 1}(x) ]$.
    \end{enumerate}
\end{proposition}

\subsection{Eigenfunction expansion}
Any (well-behaved) function on \( [-1,1] \) can be expressed as
\[
	f(x) = \sum_{\ell = 0}^\infty a_\ell P_\ell(x)
\]
where
\[
	a_\ell = \frac{2\ell + 1}{2} \int_{-1}^1 f(x) P_\ell(x) \dd{x}
\]
with no boundary conditions (e.g.\ periodicity conditions) on \( f \).

\subsection{S-L Theory on inhomogeneous ODEs}

\paragraph{Solve ODEs using SL theory} Consider the problem
\[
	\mathcal L y = f(x) \equiv w(x) F(x)
\]
on \( x \in [a,b] \) assuming homogeneous boundary conditions.
Given eigenfunctions \( y_n(x) \) satisfying \( \mathcal L y_n = \lambda_n w y_n \), we wish to expand this solution as
\[
	y(x) = \sum_n c_n y_n(x)
\]
and
\[
	F(x) = \sum_n a_n y_n(x)
\]
where \( a_n \) are known and \( c_n \) are unknown:
\[
	a_n = \frac{\int_a^b w F y_n \dd{x}}{\int_a^b w y_n^2 \dd{x}}
\]
Substituting,
\[
	\mathcal L y = \mathcal L \sum_n c_n y_n = w \sum_n c_n \lambda_n y_n = w \sum_n a_n y_n
\]
By orthogonality,
\[
	c_n \lambda_n = a_n \implies c_n = \frac{a_n}{\lambda_n}
\]
In particular,
\[
	y(x) = \sum_{n=1}^\infty \frac{a_n}{\lambda_n}y_n(x)
\]
\paragraph{Generalisation} Driving force often induces a linear response term \( \widetilde\lambda w y \).
\[
	\mathcal L y - \widetilde \lambda w y = f(x)
\]
where \( \widetilde \lambda \neq \lambda \). 
The solution becomes
\[
	y(x) = \sum_{n=1}^\infty \frac{a_n}{\lambda_n - \widetilde \lambda} y_n(x)
\]

\subsection{Integral solutions and Green's function}
Recall that
\[
	y(x) = \sum_{n=1}^\infty \frac{a_n}{\lambda_n} y_n(x) = \sum_n \frac{y_n(x)}{\lambda_n  N_n} \int_a^b w(\xi) F(\xi) y_n(\xi) \dd{\xi}
\]
where
\[
	N_n = \int w y_n^2 \dd{x}
\]
This then gives
\[
	y(x) = \int_a^b \underbrace{\sum_{n=1}^\infty \frac{y_n(x) y_n(\xi)}{\lambda_n N_n}}_{G(x,\xi)} \underbrace{w(\xi) F(\xi)}_{f(\xi)} \dd{\xi} = \int_a^b G(x;\xi) f(\xi) \dd{\xi}
\]
where
\[
	G(x,\xi) = \sum_{n=1}^\infty \frac{y_n(x) y_n(\xi)}{\lambda_n N_n}
\]
is the eigenfunction expansion of the \textbf{Green's function}.
Note that the Green's function does not depend on \( f \), but only on \( \mathcal L \) and the boundary conditions.
In this sense, it acts like an inverse operator
\[
	\mathcal L^- \equiv \int \dd{\xi} G(x,\xi)
\]
analogously to how \( Ax = b \implies x = A^{-1} b \) for matrix equations.

\clearpage

\part{PDEs on bounded domains}
\section{The Wave Equation}
\subsection{Waves on an elastic string: physical derivation}
Consider a small displacement \( y(x,t) \) on a stretched string with fixed ends at \( x = 0 \) and \( x = L \), that is, with boundary conditions \( y(0,t) = y(L,t) = 0 \).
We can determine the string's motion for specified initial conditions \( y(x,0) = p(x) \) and \( \pdv{y}{t}(x,0) = q(x) \).

We derive the equation of motion governing the motion of the string by balancing forces on a string segment \( (x,x+\delta x) \) and take the limit as \( \delta x \to 0 \).
Let \( T_1 \) be the tension force acting to the left at angle \( \theta_1 \) from the horizontal.
Analogously, let \( T_2 \) be the rightwards tension force at angle \( \theta_2 \).
We assume at any point on the string that \( \abs{\pdv{y}{x}} \ll 1 \), so the angles of the forces are small.
In the \( x \) dimension,
\[
	T_1 \cos \theta_1 = T_2 \cos \theta_2 \implies T_1 \approx T_2 = T
\]
So the tension \( T \) is constant up to an error of order \( O\qty(\abs{\pdv{y}{x}
}^2) \).
In the \( y \) dimension, since \( \theta \) are small,
\[
	F_T = T_2 \sin \theta_2 - T_1 \sin \theta_1 \approx T \qty(\eval{\pdv{y}{x}}_{x + \delta x} - \eval{\pdv{y}{x}}_x) \approx T \pdv[2]{y}{x} \delta x
\]
By \( F = ma \),
\[
	F_T + F_g = (\mu \delta x) \pdv[2]{y}{t} = T \pdv[2]{y}{x} \delta x - g \mu \delta x
\]
where \( F_g \) is the gravitational force and \( \mu \) is the linear mass density.
We define the wave speed as
$ c = \sqrt{T/\mu} $
and find
\[
	\pdv[2]{y}{t} = \frac{T}{\mu} \pdv[2]{y}{x} - g = c^2 \pdv[2]{y}{x}-g
\]
We often assume gravity is negligible to produce the pure wave equation
\[
	\frac{1}{c^2} \pdv[2]{y}{t} = \pdv[2]{y}{x} 
\]

\subsection{Separation of variables}

Consider a separable form:
\[
	y(x,t) = X(x) T(t)
\]
Substituting into the wave equation,
\[
	\frac{1}{c^2} \ddot y = y'' \implies \frac{1}{c^2} X \ddot T = X'' T
\]
Then
\[
	\frac{1}{c^2}\frac{\ddot T}{T} = \frac{X''}{X}
\]
However, \( \frac{\ddot T}{T} \) depends only on \( t \) and \( \frac{X''}{X} \) depends only on \( x \).
Thus, both sides must be equal to some \textbf{separation constant} \( -\lambda \).
\[
	\frac{1}{c^2}\frac{\ddot T}{T} = \frac{X''}{X} = -\lambda \implies X'' + \lambda X = 0;\quad \ddot T + \lambda c^2 T = 0
\]

\subsection{Boundary conditions and normal modes}

First look at the spatial part of the solution.

One of \( \lambda > 0, \lambda < 0, \lambda = 0 \) must be true.
The boundary conditions restrict the possible \( \lambda \).
\begin{enumerate}[align=left]
	\item[(1) \( \lambda < 0 \)] Take \( \chi^2 = -\lambda \).
	      Then,
	      \[
		      X(x) = Ae^{\chi x} + Be^{-\chi x} = \tilde{A} \cosh (\chi x) + \tilde{B} \sinh (\chi x)
	      \]
	      The boundary conditions are \( x(0) = x(L) = 0 \), so only the trivial solution is possible: \( \tilde{A} = \tilde{B} = 0 \).
	\item[(2) \( \lambda = 0 \)] Then
	      \[
		      X(x) = Ax + B
	      \]
	      Again, the boundary conditions impose \( A = B = 0 \) giving only the trivial solution.
	\item[(3) \( \lambda > 0 \)]
	      \[
		      X(x) = A \cos \qty(\sqrt{\lambda} x) + B \sin \qty(\sqrt{\lambda} x)
	      \]
	      The boundary conditions give
	      \[
		      A = 0,\quad B \sin \qty(\sqrt{\lambda} L) = 0 \implies \sqrt{\lambda} L = n \pi
	      \]
\end{enumerate}
The following are the eigenfunctions and eigenvalues.
\[
	X_n(x) = B_n \sin \frac{n \pi x}{L};\quad \lambda_n = \qty(\frac{n \pi}{L})^2
\]
These are also called the \textbf{normal modes} of the system.
The spatial shape in \( x \) does not change in time, but the amplitude may vary.
The \textbf{fundamental mode}, or \textbf{first harmonic}, is the lowest frequency of vibration, given by
\[
	n = 1 \implies \lambda_1 = \frac{\pi^2}{L^2}
\]
The \textbf{second mode}, or \textbf{overtone}, is the first overtone, and is given by
\[
	n = 2 \implies \lambda_2 = \frac{4\pi^2}{L^2}
\]
and so on. 

\subsection{Initial conditions and temporal solutions}
Substituting \( \lambda_n \) into the time ODE,
\[
	\ddot T + \frac{n^2 \pi^2 c^2}{L^2}T = 0
\]
Hence,
\[
	T_n(t) = C_n \cos \left(\frac{n \pi c t}{L}\right) + D_n \sin \left(\frac{n \pi c t}{L}\right)
\]
Therefore, a specific solution of the wave equation satisfying the boundary conditions is (absorbing the \( B_n \) into the \( C_n, D_n \)):
\[
	y_n(x,t) = T_n(t) X_n(x) = \qty(C_n \cos \left(\frac{n \pi c t}{L}\right) + D_n \sin \left(\frac{n \pi c t}{L}\right)) \sin \left(\frac{n \pi x}{L}\right)
\]
To find a particular solution for a given set of initial conditions, we must consider a linear superposition of all possible \( y_n \).
\[
	y(x,t) = \sum_{n=1}^\infty \qty(C_n \cos \left(\frac{n \pi c t}{L}\right) + D_n \sin \left(\frac{n \pi c t}{L}\right)) \sin \left(\frac{n \pi x}{L}\right)
\]
By construction, this \( y(x,t) \) satisfies the boundary conditions, so now we can impose the initial conditions.
\[
	y(x,0) = p(x) = \sum_{n=1}^\infty C_n \sin \left(\frac{n \pi x}{L}\right)
\]
We can find the \( C_n \) using standard Fourier series techniques, since this is exactly a half-range sine series.
Further,
\[
	\pdv{y(x,0)}{t} = q(x) = \sum_{n=1}^\infty \frac{n \pi c}{L} D_n \sin \left(\frac{n \pi x}{L}\right)
\]
Again we can solve for the \( D_n \) in a similar way.
In particular,
\[
	C_n = \frac{2}{L} \int_0^L p(x) \sin \left(\frac{n \pi x}{L}\right) \dd{x}
\]
\[
	D_n = \frac{2}{n \pi c} \int_0^L q(x) \sin \left(\frac{n \pi x}{L}\right) \dd{x}
\]
\begin{example}
	Consider the initial condition of a see-saw wave parametrised by \( \xi \), and let \( L = 1 \).
	This can be visualised as plucking the string at position \( \xi \).
	\[
		y(x,0) = p(x) = \begin{cases}
			x(1-\xi) & 0 \leq x < \xi \\
			\xi(1-x) & \xi \leq x < 1
		\end{cases}
	\]
	We also define
	\[
		\pdv{y(x,0)}{t} = q(x) = 0
	\]
	The Fourier series for \( p \) is given by
	\[
		C_n = \frac{2 \sin (n \pi \xi)}{(n \pi)^2};\quad D_n = 0
	\]
	Hence the solution to the wave equation is
	\[
		y(x,t) = \sum_{n=1}^\infty \frac{2}{(n \pi)^2} \sin (n \pi \xi) \sin (n \pi x) \cos (n \pi c t)
	\]
\end{example}

\subsection{Separation of variables: algorithm}
A general strategy for solving higher-dimensional partial differential equations is as follows.
\begin{enumerate}
	\item Obtain a linear PDE system, using boundary and initial conditions.
	\item Separate variables to yield decoupled ODEs.
	\item Impose homogeneous boundary conditions to find eigenvalues and eigenfunctions.
	\item Use these eigenvalues (constants of separation) to find the eigenfunctions in the other variables.
	\item Sum over the products of separable solutions to find the general series solution.
	\item Determine coefficients for this series using the initial conditions.
\end{enumerate}
\begin{example}
	We will solve the wave equation instead in characteristic coordinates.
	Recall the sine and cosine summation identities:
	\begin{align*}
		y(x,t) & = \frac{1}{2} \sum_{n=1}^\infty \Bigg[ \qty(C_n \sin \frac{n \pi}{L}(x-ct) + D_n \cos \frac{n \pi}{L}(x-ct)) \\&
		+ \qty(C_n \sin \frac{n \pi}{L}(x+ct) - D_n \cos \frac{n \pi}{L}(x+ct)) \Bigg]                                        \\
		       & = f(x-ct) + g(x+ct)
	\end{align*}
	The standing wave solution can be interpreted as a superposition of a right-moving wave (along characteristic $ x-ct=\eta $ constant) and a left-moving wave(along characteristic $ x+ct=\xi $ constant).
	A special case is \( q(x) = 0 \), implying \( f = g = \frac{1}{2} p \).
	Then,
	\[
		y(x,t) = \frac{1}{2}\qty[p(x-ct) + p(x+ct)]
	\]
\end{example}

\end{document}