\documentclass[a4paper]{article}
\renewcommand{\epsilon}{\varepsilon}
\newcommand{\triposcourse}{Linear Algebra}
\input{../header.tex}
\graphicspath{ {./images/} }
\pgfplotsset{compat=1.17}
\begin{document}
\maketitle
\newpage
\tableofcontents
\newpage

\section{Vector Spaces}
\subsection{Vector spaces and subspaces}
Let $ \bbF $ be an arbitrary field ($ \mathbb{R}  $ or $ \mathbb{C}  $).

\begin{definition}[$ \bbF $ Vector Space]
    An \textbf{$ \bbF$ vector space} (or a \textbf{vector space over $\bbF$}) is a an abelian group $ (V,+) $ with a function $ \bbF\times V \mapsto V $, defined by $ (\lambda, \bfv) \mapsto \lambda\bfv $, such that 
    \begin{enumerate}
        \item $ \lambda(\bfv_1+\bfv_2)=\lambda\bfv_1+\lambda\bfv_2 $,
        \item $ (\lambda_1+\lambda_2)\bfv = \lambda_1\bfv+\lambda_2\bfv $,
        \item $ \lambda(\mu\bfv)=(\lambda\mu)\bfv $,
        \item $ 1\bfv=\bfv $,
    \end{enumerate}
    for all $ \lambda,\mu\in \bbF$ and $ \bfv\in V $.
\end{definition}

\begin{example}
    \begin{enumerate}
        \item $ \bbF^n,\ n\in \mathbb{N}  $ is a vector space.
        \item $ \bbR^X = \{f: X\to \mathbb{R}\} $, the set of real-valued functions on $X$, is a vector space.
        \item $ \mcM_{n,m}(\bbF) $ is a vector space.
    \end{enumerate}
\end{example}
\begin{remark}
    The axioms imply that $ \forall \bfv\in V,\ 0\cdot\bfv=0 $.
\end{remark}

\begin{definition}[Subspace]
    Let $V$ be a vector space on $\bbF$. A subset $U$ of $V$ is a \textbf{vector subspace} if 
    \begin{enumerate}
        \item $ \mathbf{0}\in U $,
        \item $ (\bfu_1,\bfu_2)\in U\times U \Rightarrow \mathbf{u}_1+\mathbf{u}_2\in U $,
        \item $ (\lambda,\bfu)\in \bbF\times U \Rightarrow \lambda\bfu\in U $.
    \end{enumerate}
    Equivalently, $U$ is a subspace of $V$ if 
    \[
        \forall (\lambda,\mu)\in \bbF\times \bbF,\ \forall (\bfu,\bfv)\in U\times U,\ \lambda\bfu+\mu\bfv\in U.
    \]
    Denote $ U\le V $.
\end{definition}
Recall from Vectors and Matrices: the two definitions are equivalent.

This property implies that $V$ and $U$ are \textbf{stable} by scalar multiplication and vector addition. We immediately get:
\begin{proposition}
    Let $ V $ be a vector space over $\bbF$. If $ U\le V $, then $U$ is a vector space over $\bbF$.
\end{proposition}
\begin{example}
    \begin{enumerate}
        \item $ \mathbb{P}(\bbR)\le C(\bbR)\le \mathbb{R} ^\mathbb{R}  $, where $ \mathbb{P}(\mathbb{R}) $ is the space of real polynomials and $ C(\mathbb{R}) $ is the space of continuous real functions.
        \item The set of vectors
        $$
        \left\{\begin{pmatrix}x_1 \\ x_2 \\ x_3\end{pmatrix} : x_1, x_2, x_3 \in \bbR, x_1 + x_2 + x_3 = t\right\}
        $$
        is a subspace of $\bbR^3$ for $t = 0$ only.
    \end{enumerate}
\end{example}
\begin{proposition}[Intersecting Subspaces]
    Let $U, W \le V$. Then $U \cap W \le V$.
\end{proposition}
\begin{proof}
    Since $0 \in U$ and $0 \in W$, we have $0 \in U \cap W$. Now if $\lambda_1, \lambda_2 \in \bbF$ and $\bfv_1, \bfv_2 \in U \cap W$, then $\lambda_1 \bfv_1 + \lambda_2 \bfv_2 \in U$ and $V$, and thus is in $U \cap V$. Thus $U \cap W \le V$.
\end{proof}
The union of two subspaces is generally \emph{not} a subspace, as it is typically not closed by addition. In fact, the union is only ever a subspace if one of the subspaces is contained in the other. 
\begin{center}
    \includegraphics[scale=0.13]{la1.jpeg}
\end{center}

\begin{definition}[Sum of vector spaces]
    Let $V$ be a vector space over $\bbF$ and let $U,W\le V$. The \textit{sum} of $U,W$ is defined as 
    \[
        U+W  =\left\{ \bfu+\bfw: (\bfu,\bfw)\in U\times W \right\}.
    \] 
\end{definition}
For example, the sum of $x$-axis and $y$-axis is $ \mathbb{R}^{2} $. By direct verification we get:

\begin{proposition}
    $U+W\le V$.
\end{proposition}
\begin{proposition}
    $U+W$ is the smallest subspace of $V$ that contains $U,W$.
\end{proposition}
\begin{proof}
    Let $X$ be a subspace of $V$ that contains $U,W$. By closure, $ \bfu+\bfw\in X $ for all $ \bfu\in U,\bfw\in W $, so $ U+W\le X $. Hence it is the smallest subspace.
\end{proof}
\subsection{Subspaces and quotient}
Same as group theory, we can define subspaces and quotients. 
\begin{definition}[Quotient]
    Let $V$ a vector space over $\bbF$, and let $U \leq V$. The \emph{quotient space} $V/U$ is the abelian group $V/U$ equipped with the scalar multiplication $\bbF \times V/U \rightarrow V/U$, $(\lambda, \bfv + U) \mapsto \lambda \bfv + U$.
\end{definition}
The multiplication is well-defined, since if $ \bfv_1+U=\bfv_2+U $ then $ \bfv_1-\bfv_2\in U $, which implies $ \lambda(\bfv_1-\bfv_2) \in U$, and thus $ \lambda\bfv_1+U=\lambda\bfv_2+U\in V/U $.
\begin{proposition}
    $V/U$ is a vector space over $\bbF$.
\end{proposition}
\begin{proof}
    Let $ \lambda_1,\lambda_2\in \bbF $ and let $ \bfv_1+U,\bfv_2+U\in V/U $. Note that 
    \begin{align*}
        \lambda_1(\bfv_1+U)+\lambda_2(\bfv_2+U) &= (\lambda_1\bfv_1+U)+(\lambda_2\bfv_2+U)\\ 
        &= \lambda_1\bfv_1+\lambda_2\bfv_2+U\in V/U.\qedhere
    \end{align*}
\end{proof}

\subsection{Spans, independence and Steinitz Exchange Lemma}
\begin{definition}[Span of a Family of Vectors]
    Let $V$ be a vector space over $\mathbb F$ and $S\subset V$.
    We define the span of $S$ to be
    $$\langle S\rangle=\operatorname{span}(S)=\left\{\sum_{i=1}^n\lambda_i\bfs_i:n\in\mathbb N,\lambda_i\in \mathbb F,\bfs_i\in S\right\}$$
\end{definition}
That is, $\langle S\rangle$ consists of all possible \textit{finite} linear combination of elements of $S$.
By convention, $\langle \varnothing\rangle=\{0\}$.
\begin{remark}
    $ \langle S \rangle  $ is the smallest vector subspace of $V$ which contains $S$.
\end{remark}
\begin{example}
    \begin{enumerate}
        \item $V=\mathbb{R}^{3}$,
        \[
            S = \left\{\begin{pmatrix}
                1 \\ 0 \\ 0
            \end{pmatrix}, \begin{pmatrix}
                0 \\ 1 \\ 2
            \end{pmatrix}, \begin{pmatrix}
                3 \\ -2 \\ -4
            \end{pmatrix}\right\}.
        \]
        Verify that 
        \[
            \langle S \rangle = \left\{ \begin{pmatrix}
                a \\ b \\ 2b
            \end{pmatrix}:(a,b)\in \mathbb{R} \right\}.
        \]
        \item $ V = \mathbb{R}^{n} $, $\bfe_i$ standard basis, then $ V = \langle (\bfe_i)_{1\le i\le n} \rangle  $.
        \item Let $V=\mathbb R^X$ and $\delta_x:X\to\mathbb R$ be such that $\delta_x(y)=1_{x=y}$.
        Then $\langle \{\delta_x\}_{x\in\mathbb R}\rangle$ are the set of functions $f\in\mathbb R^X$ that has finite support ($ \operatorname{Supp} f = \{x:f(x)\neq 0\} $).
    \end{enumerate}
\end{example}
\begin{definition}
    Let $V$ be a vector space over $\mathbb F$ and $S\subset V$.
    We say $S$ spans $V$ if $\langle S\rangle =V$.
\end{definition}
\begin{example}
    Take $V=\mathbb R^2$, then any set of two non-parallel vectors would span $V$.
\end{example}
\begin{definition}
    A vector space $V$ over $\mathbb F$ is finite dimensional if there is a finite $S\subset V$ that spans $V$.
\end{definition}
\begin{example}
    $V=\mathbb P[x]$, the set of polynomials in $\mathbb R$ and $V_n=\mathbb P_n[x]$, the set of real polynomials with degree $\le n$.
    Then $V_n=\langle\{1,x,\ldots,x^n\}\rangle$ is finite dimensional, but $V$ is not finite dimensional as any finite set of polynomials must be contained in $V_n$ where $n$ is the maximal degree of polynomials in that set.
\end{example}
If $V$ is finite-dimensional, is there a minimal number of vectors in the family required so that the family spans $V$?
\begin{definition}[(Linear) Independence]
    Let $V$ be a vector space over $F$.

    We say $\{\bfv_1,\ldots,\bfv_n\}\subset V$ are (linearly) independent (or is a free family) if for any $\lambda_1,\ldots,\lambda_n\in F$
    $$\sum_{i=1}^n\lambda_i\bfv_i=0\implies\forall i,\lambda_i=0$$
    Equivalently, this set is not linearly independent if there exists $\lambda_1,\ldots,\lambda_n\in F$ not all zero such that $\sum_{i=1}^n\lambda_i\bfv_i=0$.
\end{definition}
\begin{example}
Let $V=\mathbb R^3$ and
$$\bfv_1=(1,0,0)^\top,\bfv_2=(0,1,0)^\top,\bfv_3=(1,1,0)^\top,\bfv_4=(0,1,1)^\top$$
Then $\{\bfv_1,\bfv_2\}$ is linearly independent.
Note that $\bfv_3\in\langle\{\bfv_1,\bfv_2\}\rangle$, so $\{\bfv_1,\bfv_2,\bfv_3\}$ is not linearly independent.
On the other hand, $\bfv_4\notin\langle\{\bfv_1,\bfv_2\}\rangle$, which as one can verify means that $\{\bfv_1,\bfv_2,\bfv_4\}$ is linearly independent.
\end{example}
\begin{remark}
    If the family $\{\bfv_i\}_{1\le i\le n}$ is linearly independent, then none of $\bfv_i$ is zero.
\end{remark}
\begin{definition}[Basis]
    A subset $S\subset V$ is a basis if it is linearly independent and $\langle S\rangle=V$.
\end{definition}
\begin{remark}
    When $S$ spans $V$, we say that $S$ is a generating family of $V$.
    So a basis is just a linearly independent(we also say free) generating family.
\end{remark}
\begin{example}
    \begin{enumerate}
        \item Take $V=\mathbb R^n$, then the family $\{\bfe_i\}_{1\le i\le n}$ where $\bfe_i$ is the vector having $1$ at $i^{th}$ entry and zero otherwise is a basis. $ (\bfe_i) $ are called the \textit{canonical basis}.
        \item Take $V=\mathbb C$ over $\mathbb C$, then $\{a\}$ is a basis for any $a\neq 0$.
        \item Take also $V=\mathbb C$ but over $\mathbb R$, then $\{1,i\}$ is a basis.
        \item Take $V=\mathbb P[x]$ be the set of polynomials in $\mathbb R$ and $S=\{x^n:n\ge 0\}$.
        Then $S$ is a basis.
        Worth noting that $|S|=\infty$ in this case.
    \end{enumerate}
\end{example}
\begin{lemma}
    If $V$ is a vector space over $F$, then $\{\bfv_1,\ldots,\bfv_n\}$ is a basis of $V$ if and only if for any vector $\bfv\in V$, there is a unique decomposition
    $$\bfv=\sum_{i=1}^n\lambda_i\bfv_i,\quad \lambda_i\in \mathbb{F}.$$
\end{lemma}
\begin{remark}
    If the conditions are true, then the tuple $(\lambda_1,\ldots,\lambda_n)$ (ordered via the ordering one chose on $\bfv_i$) is called the coordinate of $\bfv$ in the basis $(\bfv_i)$.
\end{remark}
\begin{proof}
    Suppose $ (\bfv_i) $ is a basis of $V$. This implies that $ \langle \bfv_i \rangle =V $, i.e. 
    \[
        \forall \bfv\in V\quad \exists (\lambda_1,\dots,\lambda_n)\in \bbF,\quad \bfv = \sum_{i=1}^{n}\lambda_i\bfv_i.
    \]
    Suppose that $ \exists (\lambda_i') $ s.t. the above holds, then 
    \[
        \sum_{i=n}^{n} \lambda_i-\lambda'_i=0 \Longrightarrow \lambda_i=\lambda_i'
    \]
    by independence.

    Suppose now that there is a unique decomposition. Then clearly $ \langle (\bfv_i) \rangle =V $. Consider 
    \[
        \sum_{i=1}^n\lambda_i\bfv_i=0,\quad \lambda_i\in \mathbb{F}.
    \]
    Clearly it is true for $ \lambda_i=0,\forall i $. By uniqueness, this is the only decomposition and thus $ (\bfv_i) $ is a basis.
\end{proof}
\begin{lemma}
    If $S$ is a finite set that spans $V$, then a subset of $S$ is a basis of $V$.
\end{lemma}
\begin{proof}
    If $S$ is independent, then we are done.
    Otherwise, there is some $\lambda\neq 0$ and $\lambda_\bfw$ such that there is $v\in S$ with
    $$\lambda \bfv+\sum_{\bfw\in S\setminus\{\bfv\}}\lambda_\bfw\bfw=0\implies \bfv=\frac{1}{\lambda}\sum_{\bfw\in S\setminus\{\bfv\}}\lambda_\bfw\bfw\in\langle S\setminus\{\bfv\}\rangle.$$
    Therefore $S\setminus\{\bfv\}$ also spans $V$.
    We can repeat this process and, by the well-ordering of $\mathbb N$, will reach a basis.
\end{proof}
\begin{theorem}[Steinitz Exchange Lemma]\label{thm:steinitz}
    Let $V$ be a finite dimensional vector space over $\bbF$. Take $\{\bfv_1,\ldots,\bfv_m\}\subset V$ linearly independent, $\{\bfw_1,\ldots,\bfw_n\}\subset V$ a generating set, then:
    \begin{enumerate}
        \item $m\le n$.
        \item Up to relabeling, $\{\bfv_1,\ldots,\bfv_m,\bfw_{m+1},\ldots,\bfw_n\}$ spans $V$.
    \end{enumerate}
\end{theorem}
\begin{proof}
    Suppose $\{\bfv_1,\ldots,\bfv_l,\bfw_{l+1},\ldots,\bfw_n\}$ spans $V$ for some $l<m$, then
    $$\exists\alpha_i,\beta_i\in \bbF, \bfv_{l+1}=\sum_{i\le l}\alpha_i\bfv_i+\sum_{i>l}\beta_i\bfw_i$$
    But $\{\bfv_i\}$ is linearly independent, so one of the $\beta_i$ is nonzero.
    By relabelling $\beta_{l+1}\neq 0$, then $\bfw_{l+1}\in\langle\{\bfv_1,\ldots,\bfv_l,\bfv_{l+1},\bfw_{l+2}\ldots,\bfw_n\}\rangle$, therefore the set of vectors 
    \[
        \{\bfv_1,\ldots,\bfv_l,\bfv_{l+1},\bfw_{l+2}\ldots,\bfw_n\}
    \]
    also spans $V$. If we start from 0, we are done after $m$ steps by induction, and thus $m\le n$, and
    \[
        \langle \bfv_1,\ldots,\bfv_m,\bfw_{m+1},\ldots,\bfw_n \rangle =V.\qedhere
    \]
\end{proof}

\subsection{Basis, Dimension and Direct Sum}
From the Steinitz Exchange Lemma, we have
\begin{corollary}\label{dim_well_defined}
    Let $V$ be a finite dimensional vector space, then any two bases of $V$ have the same cardinality.
\end{corollary}
We call this cardinality the \textit{dimension} of $V$, denoted by $ \dim V $ or $ \dim_\bbF V $, to specify the field.
\begin{corollary}
    Let $V$ be a vector space with $\dim V=n$, then:
    \begin{enumerate}
        \item Any independent set of vectors has size at most $n$.
        The size is exactly $n$ iff this set is a basis.
        \item Any spanning set has size at least $n$.
        The size is exactly $n$ iff this set is a basis.
    \end{enumerate}
\end{corollary}
\begin{proposition}
    Let $U,W$ be subspaces of $V$.
    If they are finite dimensional, then so is $U+W$ and
    $$\dim(U+W)=\dim U+\dim V-\dim(U\cap W)$$
\end{proposition}
\begin{proof}
    Pick a basis $\bfv_1,\ldots,\bfv_l$ of $U\cap W$ and extend it to a basis $\bfv_1,\ldots,\bfv_l,\bfu_1,\ldots,\bfu_m$ of $U$ and a basis $\bfv_1,\ldots,\bfv_l,$ $\bfw_1,\ldots,\bfw_n$ of $W$. 
    
    Claim that $B=\{\bfv_i\}\cup\{\bfu_i\}\cup\{\bfw_i\}$ is easily a basis of $U+W$.
    The equality follows.

    Indeed, it is obvious that $B$ is a generating set. To show linear independence, consider 
    \[
        \sum_{i=1}^{l}\alpha_i \bfv_i+\sum_{j=1}^{m}\beta_j \bfu_j+\sum_{k=1}^{n}\gamma_k\bfw_k=0.
    \]
    Rearrange this to get 
    \[
        \begin{aligned}
           & \sum_{i=1}^{l}\alpha_i \bfv_i+\sum_{j=1}^{m}\beta_j \bfu_j=-\sum_{k=1}^{n}\gamma_k\bfw_k\\ &\Longrightarrow \sum_{k=1}^{n}\gamma_k\bfw_k\in U \cap W\\ &\Longrightarrow \sum_{k=1}^{n}\gamma_k\bfw_k=\sum_{i=1}^{l}\delta_i\bfv_i.
        \end{aligned}
    \]
    Therefore we get 
    \[
        \sum_{i=1}^{l}(\alpha_i+\delta_i)\bfv_i+\sum_{j=1}^{m}\beta_j \bfu_j=0 \Longrightarrow \beta_j=0,\alpha_i=-\delta_i.
    \]
    Hence the original equation becomes 
    \[
        \sum_{i=1}^{l}\alpha_i\bfv_i+\sum_{k=1}^{n}\gamma_k\bfw_k=0 \Longrightarrow \alpha_i=\gamma_k=0.\qedhere
    \]
\end{proof}
\begin{proposition}
    If $V$ is a finite dimensional vectyor space and $U\le V$, then $U,V/U$ are both finite dimensional and $\dim V=\dim U+\dim V/U$.
\end{proposition}
\begin{proof}
    It is obvious that $U$ is finite dimensional.
    Choose a basis $\bfu_1,\ldots,\bfu_l$ and extend it to a basis $B=\{\bfu_1,\ldots,\bfu_l,\bfw_{l+1},\ldots,\bfw_n\}$ of $V$. Claim that $B'=\{\bfw_{l+1}+U,\ldots,\bfw_n+U\}$ is a basis of $V/U$ and we are done.

    Indeed, every non-trivial element in $ V/U $ has the form $ \bfv+U $, where $ \bfv\in V \setminus U $. Since $B$ is a basis, there exists $ \alpha_i,l+1\le i\le n $ such that 
    \[
        \bfv = \sum_{i=l+1}^{n}\alpha_i\bfw_i.
    \]
    Hence $ \bfv+U = \sum_{i=l+1}^{n}\alpha_i(\bfw_i +U)$ and thus $ B' $ generates $V/U$. To show independence, note that 
    \[
        \sum_{i=l+1}^{n}\beta_i \bfw_i+U=0 \Longrightarrow \sum_{i=l+1}^{n}\beta_i \bfw_i\in U \Longrightarrow \beta_i=0.\qedhere
    \]
\end{proof}
\begin{remark}
    If $U$ is a proper subspace of $V$, written $U<V$ (meaning that $U\le V$ and $U\neq V$), then the proposition gives us $\dim V/U\neq \{0\}$, so $\dim U<\dim V$.
\end{remark}
\begin{definition}
    Let $V$ be a vector space and $U,W\le V$.
    We say $V$ is the \textit{direct sum} of $U,W$, written $V=U\oplus W$, if every element $\bfv\in V$ can be written uniquely as $\bfv=\bfu+\bfw$ for $\bfu\in U,\bfw\in W$.
    If this happens, then we say $W$ is \textit{a direct complement} of $U$ in $V$.
\end{definition}
Note that direct complement is not unique in general.
\begin{example}
    Take $U=\mathbb R\times \{0\}$, then both $W=\{0\}\times \mathbb R$ and $W'=\langle\{(1,1)^\top\}\rangle$ are direct complements of $U$.
\end{example}
\begin{lemma}\label{equivalence_of_direct_sum}
    Let $U,W\le V$, then the followings are equivalent:
    \begin{enumerate}[(i)]
        \item $V=U\oplus W$.
        \item $V=U+W$ and $U\cap W=\{0\}$.
        \item For any basis $B_1$ of $U$ and $B_2$ of $W$, $B=B_1\cup B_2$ is a basis of $V$.
    \end{enumerate}
\end{lemma}
\begin{proof}
    (ii) $\Rightarrow$ (i): Suppose $V=U+W$ and $U\cap W=\{0\}$. Then $ \forall \bfv\in V,\exists (\bfu,\bfw)\in U\times W,\bfv=\bfu+\bfw $. To show uniqueness, suppose $ \bfv=\bfu_1+\bfw_1=\bfu_2+\bfw_2 $. Then $ \bfu_1-\bfu_2=\bfw_1-\bfw_2\in U \cap W=\{0\} $, so $ \bfu_1=\bfu_2,\bfw_1=\bfw_2$.

    (i) $\Rightarrow$ (iii): Let $ B_1,B_2 $ be bases of $U,W$ respectively. Let $ B=B_1 \cup B_2 $. Clearly $ \langle B \rangle =V $. Consider 
    \[
        \sum_{\bfu\in B_1}\lambda_{\bfu}\bfu+\sum_{\bfw\in B_2}\lambda_{\bfw}\bfw=0.
    \]
    Note that $ \lambda_\bfu=\lambda_\bfw=0 $ is a solution. Since $ V =U\oplus W $, the decomposition is unique so it is the only one, and we have independence.

    (iii) $\Rightarrow$ (ii): Any $\bfv\in V$ can be written as 
    \[
        \bfv=\sum_{\bfu\in B_1}\lambda_{\bfu}\bfu+\sum_{\bfw\in B_2}\lambda_{\bfw}\bfw
    \]
    so $ V=U+W $. Now let $ \bfv\in U\cap W $, then the above equation implies $\lambda_\bfu=\lambda_\bfw=0 $ and thus $ U\cap W=\{0\} $.
\end{proof}
\begin{definition}
    Let $V_1,\ldots,V_l\le V$, then we define
    $$\sum_{i=1}^lV_i=\{\bfv_1+\cdots+\bfv_l:\bfv_j\in V_j,1\le j\le l\}$$
    The sum is direct, i.e.
    $$\sum_{i=1}^lV_i=\bigoplus_{i=1}^lV_i$$
    if and only if $\bfv_1+\cdots +\bfv_l=\bfv_1'+\cdots +\bfv_l'$ implies $\bfv_j=\bfv_j'$ for any $1\le j\le l,\bfv_j\in V_j$ and $ \bfv_j'\in V_j $.
    Equivalently,
    $$V=\bigoplus_{i=1}^lV_i\iff \forall \bfv\in V,\exists!(\bfv_1,\ldots,\bfv_l)\in V_1\times\cdots\times V_l,\bfv=\sum_{i=1}^l\bfv_i.$$
\end{definition}
Lemma \ref{equivalence_of_direct_sum} is then generalized to
\begin{proposition}
    The followings are equivalent:
    \begin{enumerate}
        \item $\sum_{i=1}^lV_i=\bigoplus_{i=1}^lV_i$
        \item For any $i$,
        $V_i\cap\left( \sum_{j\neq i}V_j \right)=\{0\}$
        \item For any bases $B_i$ of $V_i$, the union $\bigcup_iB_i$ is a basis of $\sum_iV_i$.
    \end{enumerate}
\end{proposition}

\section{Linear Maps}
\subsection{Linear maps}
\begin{definition}[Linear Map]
    Let $V,W$ are vector spaces over $\bbF$, a function $\alpha:V\to W$ is \textit{linear} if for any $\lambda_1,\lambda_2\in \bbF$ and $\bfv_1,\bfv_2\in V$,
    $$\alpha(\lambda_1\bfv_1+\lambda_2\bfv_2)=\lambda_1\alpha(\bfv_1)+\lambda_2\alpha(\bfv_2).$$
\end{definition}
\begin{example}
    \begin{enumerate}
        \item Let $M$ be an $m\times n$ matrix, then $\alpha:\mathbb R^n\to\mathbb R^m$ via $x\mapsto Mx$ is a linear map.
        \item The functional $\alpha:C([0,1])\to C([0,1])$ via
        $$\alpha[f](x)=\int_0^xf(t)\,\mathrm dt$$
        is a linear map.
        \item Fix $x\in[a,b]$, then the evaluation map $\alpha:C([a,b])\to\mathbb R$ via $f\mapsto f(x)$ is a linear map.
    \end{enumerate}
\end{example}
\begin{remark}
    The identity map is a linear map.
    Composition of linear maps is also a linear map.
\end{remark}
\begin{lemma}
    Let $V,W$ be vector spaces over $\bbF$ and $B$ a basis for $V$.
    Let $\alpha_0:B\to W$ be a function, then there is a unique linear map $\alpha:V\to W$ that extends $\alpha_0$. i.e. $\exists \text{ linear }\alpha, \forall\bfv\in B, \alpha_0(\bfv)=\alpha(\bfv) $.
\end{lemma}
\begin{proof}
    For any $(\bfb_i)\in B$, necessarily $\alpha\left(\sum_i\lambda_i\bfb_i\right)=\sum_i\lambda\alpha_0(\bfb_i)$.
    This is sufficient.
\end{proof}
\begin{remark}
    This lemma is true for infinite dimensional vector spaces as well.
    Often, to define linear map, we often just define its values on a basis and extend it by this lemma.
\end{remark}
\begin{corollary}
    Two linear maps that agree on a basis are the same.
\end{corollary}
\subsection{Isomorphisms and the Rank-Nullity Theorem}
\begin{definition}
    Let $V,W$ be vector spaces over $\bbF$.
    A linear bijection $\alpha:V\to W$ is an \textit{isomorphism} (of vector spaces).
    If such a map exists, then we say $V,W$ are isomorphic (as vector spaces), written as $V\cong W$.
\end{definition}
\begin{remark}
    If $\alpha$ is an isomorphism, so is $\alpha^{-1}$.
\end{remark}
\begin{lemma}
    $\cong$ is an equivalence relation on the class of all vector spaces over $\bbF$.
\end{lemma}
\begin{proof}
    $i:V\to V$ is an isomorphism so $ V \cong V $. If $ \alpha:V\to W $ is an isomorphism then so is $ \alpha^{-1} $, and hence $ V \cong W \Rightarrow W \cong V $. If $ \beta:U\to V,\alpha:V\to W $ are isomorphisms, then $ \beta\circ \alpha:U\to W $ is an isomorphism and thus $ U \cong V \land V \cong W \Rightarrow U \cong W $. Hence it is an equivalence relation.
\end{proof}
\begin{theorem}
    If $V$ is a vector space over $\bbF$ of dimension $n$, then $V\cong \bbF^n$.
\end{theorem}
\begin{proof}
    Take a basis $\{\bfb_1,\ldots,\bfb_n\}$ of $V$, then $ \alpha:V\to \bbF^n $, defined by
    $$\alpha(x_1\bfb_1+\cdots+x_n\bfb_n)=(x_1,\ldots,x_n)^\top$$
    is an isomorphism.
\end{proof}
\begin{remark}
    Choosing a basis of $V$ is then just equivalent to choosing an isomorphism from $V$ to $\bbF^n$.
\end{remark}
\begin{theorem}\label{isom iff same dim}
    Let $V,W$ be finite dimensional vector spaces over $\bbF$.
    Then $V\cong W$ iff $\dim V=\dim W$.
\end{theorem}
\begin{proof}
    Any basis of $V$ induces a basis of $W$ via the isomorphism, so they have the same dimension.
    Therefore are both isomorphic to $\bbF^n$ where $n=\dim V=\dim W$.
\end{proof}
\begin{definition}
    Let $\alpha:V\to W$ be a linear map.
    We define the kernel of $\alpha$ to be $\ker\alpha=\{\bfv\in V:\alpha(\bfv)=0\}$ and the image to be $\operatorname{Im}\alpha=\alpha(V)=\{\bfw\in W:\exists \bfv\in V,\alpha(\bfv)=\bfw\}$.
\end{definition}
\begin{lemma}
    $\ker\alpha\le V,\operatorname{Im}\alpha\le W$.
\end{lemma}
\begin{example}
    Take $\alpha:C^\infty(\mathbb R)\to C^\infty(\mathbb R)$ by $\alpha(f)(t)=f^{\prime\prime}(t)+f(t)$.
    Then $\ker\alpha$ is spanned by $t\mapsto e^t$ and $t\mapsto e^{-t}$ and $\operatorname{Im}\alpha=C^{\infty}(\mathbb R)$.
\end{example}
There is a similar result to first isomorphism theorem:
\begin{theorem}
    Let $V,W$ be vector spaces over $F$ and $\alpha:V\to W$ be linear, then $V/{\ker\alpha}\cong \operatorname{Im}(\alpha)$ via $\bfv+\ker\alpha\mapsto \alpha(\bfv)$.
\end{theorem}
\begin{proof}
    Let $ \bar{\alpha} $ be the desired isomorphism(to be proved).

    Firstly check that $ \bar{\alpha} $ is well-defined. Indeed, if $ \bfv+\ker \alpha=\bfv'+\ker \alpha $, then $ \bfv-\bfv'\in \ker \alpha $ and thus $ \alpha(\bfv)=\alpha(\bfv')=0 $.

    Next show that $ \bar{\alpha} $ is a bijection. Let $ \bar{\alpha}(\bfv+\ker \alpha)=0 $, then $ \alpha(\bfv)=0 \Rightarrow \bfv+\ker \alpha=0+\ker \alpha $, so it is injective. Given $ \bfw\in \Im \alpha, \exists \bfv\in V $ such that $ \alpha(\bfv)=\bfw =\bar{\alpha}(\bfv+\ker \alpha)$, so it is surjective and hence bijective.
\end{proof}
\begin{definition}
    The rank of $\alpha:V\to W$ is $\rank(\alpha)=\dim\operatorname{Im}\alpha$ and nullity is $\nullity(\alpha)=\dim\ker\alpha$. Sometimes we write $ r(\alpha),n(\alpha) $. 
\end{definition}
Hence in the finite dimensional case, we can rewrite the preceding theorem to get
\begin{theorem}[Rank-Nullity Theorem]
    Let $\alpha:V\to W$ be linear where $V$ is finite dimensional.
    Then $\dim V=\rank(\alpha)+\nullity(\alpha)$.
\end{theorem}
\begin{proof}
    Since $ V/{\ker\alpha}\cong \operatorname{Im}(\alpha) $, we have 
    \begin{align*}
        & \dim (U/\ker \alpha)=\dim (\Im \alpha)\\ 
        \implies & \dim U-\dim \ker \alpha=\dim \Im \alpha\\ 
        \implies & \dim U = \rank \alpha+ \nullity \alpha.\qedhere
    \end{align*}
\end{proof}
\begin{corollary}[Classification of Isomorphism]
    Let $V,W$ be finite dimensional vector spaces with $\dim V=\dim W$ and $\alpha:V\to W$ be linear, then the followings are equivalent:
    \begin{enumerate}[(i)]
        \item $\alpha$ is injective.
        \item $\alpha$ is surjective.
        \item $\alpha$ is an isomorphism.
    \end{enumerate}
\end{corollary}
\begin{example}
    Consider
    $$V=\left\{ \begin{pmatrix}
        x\\
        y\\
        z
    \end{pmatrix}\in\mathbb R^3:x+y+z=0\right\}$$
    We want to compute $\dim V$.
    Consider $\alpha:\mathbb R^3\to\mathbb R$ via $(x,y,z)^\top\mapsto x+y+z$, then $r(\alpha)=1$ and $n(\alpha)=V$, so $\dim V=3-1=2$.
    Geometrically, $V$ is just a plane with normal $(1,1,1)^\top$.
\end{example}

\subsection{Linear Maps \texorpdfstring{$V \rightarrow W$}{V to W} and Matrices}
\subsubsection*{Space of linear maps}
We are now going to consider linear maps between two vector spaces $V$ and $W$.

\begin{definition}[Space of Linear Maps]
    Let $V$ and $W$ be vector spaces over $\bbF$. We define
    $$
    \mathcal{L}(V, W) = \{\alpha: V \rightarrow W \mid \alpha \text{ is a linear map}\}.
    $$
\end{definition}
\begin{proposition}
    $\mathcal{L}(V, W)$ is a vector space over $\bbF$.
    
    Moreover, if $V$ and $W$ are finite dimensional then so is $\mathcal{L}(V, W)$, and $\dim \mathcal{L}(V, W) = \dim V \cdot \dim W$.
\end{proposition}
\subsubsection*{Matrices}
\begin{definition}[Matrix]
    An $m \times n$ \emph{matrix} over $\bbF$ is an array with $m$ rows and $n$ columns, with entries in $\bbF$:
    $$
    A = (a_{ij}),
    $$
    with $1 \leq i \leq m$ and $1 \leq j \leq n$.

    The \textit{space of matrices} is the set $\mathcal{M}_{m,n}(\bbF)$ of $m \times n$ matrices over $\bbF$.
\end{definition}
By checking definitions we have
\begin{proposition}
    $ \mcM_{m,n}(\bbF) $ is a vector space over $ \bbF $.
\end{proposition}
\begin{proposition}
    $ \dim \mcM_{m,n}(\bbF) =m \times n$.
\end{proposition}
\begin{proof}
    We find an explicit basis. Pick $ 1\le i\le m,1\le j\le n $, define the \textit{elementary matrices} as
    \[
        E_{ij}=1 \text{ for entry } ij \text{ and } 0 \text{ elsewhere}.
    \]
    The set $\{E_{i, j} \mid 1 \leq i \leq m, 1 \leq j \leq m\}$ is clearly spanning and is also clearly linearly independent, so it is a basis. Counting elements, we see that this set has size $m \times n$, as required.
\end{proof}
\subsubsection*{Representation of linear maps by matrices}
Let $V$, $W$ be vector spaces over $\bbF$, and let $\alpha: V \rightarrow W$ be a linear map. 

Let $B = \{\bfv_1, \dots, \bfv_n\}$ be a basis for $V$, and $C = \{\bfw_1, \dots, \bfw_m\}$ be a basis for $W$. Then if $\bfv \in V$, we can write
$$
\bfv = \sum_{j = 1}^n \lambda_j \bfv_j = \begin{pmatrix}
    \lambda_1 \\ \vdots \\ \lambda_n
\end{pmatrix} \in \bbF^n,
$$
which are the `coordinates of $\bfv$' with the respect to the basis $B$, which we will call $[\bfv]_B$.

Similarly, for $\bfw \in W$, we can let $[\bfw]_C$ be the `coordinates' of $\bfw$ in the basis $C$.

\begin{definition}[Matrix of $\alpha$ in the $B, C$ Basis]
    We define $[\alpha]_{B, C}$, the \emph{matrix of $\alpha$} with respect to $B$ and $C$ by
    $$
    [\alpha]_{B, C} = \begin{pmatrix}
        [\alpha(\bfv_1)]_C, [\alpha(\bfv_2)]_C, \cdots, [\alpha(\bfv_n)]_C
    \end{pmatrix} \in \mathcal{M}_{m, n}(\bbF).
    $$ 
\end{definition}
\begin{lemma}
    For all $\v \in V$, we have $[\alpha(\v)]_C = [\alpha]_{B,C}  [\v]_B$.
\end{lemma}
\begin{proof}
    Let $\v \in V$, and let $\v = \sum_{j = 1}^n \lambda_j \v_j$. Then
    $$
    \alpha(\v) = \sum_{j = 1}^n \lambda_j \alpha(\v_j) = \sum_{j = 1}^n \lambda_j \sum_{i = 1}^m a_{ij} \w_i = \sum_{i = 1}^m \left(\sum_{j = 1}^n a_{ij} \lambda_j\right) \w_i,
    $$
    as required.
\end{proof}
By checking definitions,
\begin{lemma}
    Let $\alpha: V \rightarrow W$ and $\beta: U \rightarrow W$ be linear maps. Then if $A$ is a basis of $U$, $B$ is a basis of $V$ and $C$ is a basis of $W$, then
    $$
    [\alpha \circ \beta]_{A, C} = [\alpha]_{B, C}  [\beta]_{A, B}.
    $$
\end{lemma}
\begin{proposition}
    If $V$ and $W$ are vector spaces over $\bbF$ such that $\dim V = n$ and $\dim W = m$, then $\mathcal{L}(V, W) \cong \mathcal{M}_{m, n}(\bbF)$.
\end{proposition}
\begin{proof}
    Fix a basis $B, C$ of $V$ and $W$ respectively.
    Then consider the linear map
    $
    \theta: \mathcal{L}(V, W) \rightarrow \mathcal{M}_{m, n}(\bbF)
    $ given by $\alpha \mapsto [\alpha]_{B, C}$.
    This map is clearly linear. Let $ A=(a_{ij}) $, then the linear map 
    \[
        \alpha: \bfv_j\to \sum_{i=1}^{m}a_{ij}\w_i
    \]
    on bases $B,C$. Then $ \theta(\alpha)=A $ so it is surjective. Since a linear map is uniquely determined by the action on basis, $ \theta $ is surjective and hence bijective. Thus $\theta$ is an isomorphism and $ \mcL(V,W) \cong \mcM_{m,n}(\bbF) $. 

    Alternatively, the proposition is direct by theorem \ref{isom iff same dim}.
\end{proof}
\begin{remark}
    If $B$ is a basis of $V$, $C$ a basis of $W$, and 
    \[
        \epsilon_B:\begin{aligned}
            V&\to \bbF^n\\ 
            \bfv&\to [\bfv]_B
        \end{aligned}\quad \epsilon_C:\begin{aligned}
            W&\to \bbF^m\\ 
            \w &\to [\w]_C
        \end{aligned}
    \]
    then the following diagram commute:
    \begin{center}
        $\begin{tikzcd}
            V & W \\
            {\mathbb{F}^n} & {\mathbb{F}^m}
            \arrow["\alpha", from=1-1, to=1-2]
            \arrow["{\epsilon_B}"', from=1-1, to=2-1]
            \arrow["{[\alpha]_{B,C}}"', from=2-1, to=2-2]
            \arrow["{\epsilon_C}", from=1-2, to=2-2]
        \end{tikzcd}$
    \end{center}
\end{remark}

\begin{example}
    Let $V$ and $W$ be finite dimensional vector spaces.
    Consider the linear map $\alpha: V \rightarrow W$, and suppose $Y \leq V$. Consider the image of $Y$ under $\alpha$, $Z=\alpha(Y) = \{ \w \in W \mid \w= \alpha(\bfy), \bfy \in Y\}$.

    Define the sets $B' = \{\v_1, \dots, \v_k\}$ and $B'' = \{\v_{k + 1}, \dots, \v_n\}$ such that the union of disjoint sets $B = B' \cup B''$ is a basis for $V$, and $B'$ is a basis of $Y$ (that is, we extend using Steinitz exchange lemma).
    
    Similarity we define $C' = \{\w_1, \dots, \w_\ell\}$ and $C'' = \{\w_{\ell + 1}, \dots, \w_m\}$ such that the union of disjoint sets $C = C' \cup C''$ is a basis for $W$ and $C'$ is a basis for $\alpha(Y)$.

    Then we have the matrix
    $$ 
    [\alpha]_{B, C} = \begin{pmatrix}
        \alpha(\v_1) & \cdots & \alpha(\v_k) & \alpha(\v_{k + 1}) & \cdots & \alpha(\v_n)
    \end{pmatrix},
    $$
    and this will look like the block matrix
    $$
    \begin{pmatrix}
        A & B \\
        0 & C \\
    \end{pmatrix}
    $$
    where $A$ is $k \times \ell$ such that $ A = [\alpha\upharpoonright_Y]_{B',C'} $. 
    Also $ \alpha $ induces a well-defined map $ \bar{\alpha}:V/Y\to W/Z $ such that $ \v+Y\mapsto \alpha(\bfv)+Z $, with matrix given by $ [\bar{\alpha}]_{B'',C''}=C $.
\end{example}

\end{document}