\documentclass[a4paper]{article}
\renewcommand{\epsilon}{\varepsilon}
\newcommand{\triposcourse}{Linear Algebra}
\input{../header.tex}
\graphicspath{ {./images/} }
\pgfplotsset{compat=1.17}
\begin{document}
\maketitle
\newpage
\tableofcontents
\newpage

\section{Vector Spaces}
\subsection{Vector spaces and subspaces}
Let $ \bbF $ be an arbitrary field ($ \mathbb{R}  $ or $ \mathbb{C}  $).

\begin{definition}[$ \bbF $ Vector Space]
    An \textbf{$ \bbF$ vector space} (or a \textbf{vector space over $\bbF$}) is a an abelian group $ (V,+) $ with a function $ \bbF\times V \mapsto V $, defined by $ (\lambda, \bfv) \mapsto \lambda\bfv $, such that 
    \begin{enumerate}
        \item $ \lambda(\bfv_1+\bfv_2)=\lambda\bfv_1+\lambda\bfv_2 $,
        \item $ (\lambda_1+\lambda_2)\bfv = \lambda_1\bfv+\lambda_2\bfv $,
        \item $ \lambda(\mu\bfv)=(\lambda\mu)\bfv $,
        \item $ 1\bfv=\bfv $,
    \end{enumerate}
    for all $ \lambda,\mu\in \bbF$ and $ \bfv\in V $.
\end{definition}

\begin{example}
    \begin{enumerate}
        \item $ \bbF^n,\ n\in \mathbb{N}  $ is a vector space.
        \item $ \bbR^X = \{f: X\to \mathbb{R}\} $, the set of real-valued functions on $X$, is a vector space.
        \item $ \mcM_{n,m}(\bbF) $ is a vector space.
    \end{enumerate}
\end{example}
\begin{remark}
    The axioms imply that $ \forall \bfv\in V,\ 0\cdot\bfv=0 $.
\end{remark}

\begin{definition}[Subspace]
    Let $V$ be a vector space on $\bbF$. A subset $U$ of $V$ is a \textbf{vector subspace} if 
    \begin{enumerate}
        \item $ \mathbf{0}\in U $,
        \item $ (\bfu_1,\bfu_2)\in U\times U \Rightarrow \mathbf{u}_1+\mathbf{u}_2\in U $,
        \item $ (\lambda,\bfu)\in \bbF\times U \Rightarrow \lambda\bfu\in U $.
    \end{enumerate}
    Equivalently, $U$ is a subspace of $V$ if 
    \[
        \forall (\lambda,\mu)\in \bbF\times \bbF,\ \forall (\bfu,\bfv)\in U\times U,\ \lambda\bfu+\mu\bfv\in U.
    \]
    Denote $ U\le V $.
\end{definition}
Recall from Vectors and Matrices: the two definitions are equivalent.

This property implies that $V$ and $U$ are \textbf{stable} by scalar multiplication and vector addition. We immediately get:
\begin{proposition}
    Let $ V $ be a vector space over $\bbF$. If $ U\le V $, then $U$ is a vector space over $\bbF$.
\end{proposition}
\begin{example}
    \begin{enumerate}
        \item $ \mathbb{P}(\bbR)\le C(\bbR)\le \mathbb{R} ^\mathbb{R}  $, where $ \mathbb{P}(\mathbb{R}) $ is the space of real polynomials and $ C(\mathbb{R}) $ is the space of continuous real functions.
        \item The set of vectors
        $$
        \left\{\begin{pmatrix}x_1 \\ x_2 \\ x_3\end{pmatrix} : x_1, x_2, x_3 \in \bbR, x_1 + x_2 + x_3 = t\right\}
        $$
        is a subspace of $\bbR^3$ for $t = 0$ only.
    \end{enumerate}
\end{example}
\begin{proposition}[Intersecting Subspaces]
    Let $U, W \le V$. Then $U \cap W \le V$.
\end{proposition}
\begin{proof}
    Since $0 \in U$ and $0 \in W$, we have $0 \in U \cap W$. Now if $\lambda_1, \lambda_2 \in \bbF$ and $\bfv_1, \bfv_2 \in U \cap W$, then $\lambda_1 \bfv_1 + \lambda_2 \bfv_2 \in U$ and $V$, and thus is in $U \cap V$. Thus $U \cap W \le V$.
\end{proof}
The union of two subspaces is generally \emph{not} a subspace, as it is typically not closed by addition. In fact, the union is only ever a subspace if one of the subspaces is contained in the other. 
\begin{center}
    \includegraphics[scale=0.13]{la1.jpeg}
\end{center}

\begin{definition}[Sum of vector spaces]
    Let $V$ be a vector space over $\bbF$ and let $U,W\le V$. The \textit{sum} of $U,W$ is defined as 
    \[
        U+W  =\left\{ \bfu+\bfw: (\bfu,\bfw)\in U\times W \right\}.
    \] 
\end{definition}
For example, the sum of $x$-axis and $y$-axis is $ \mathbb{R}^{2} $. By direct verification we get:

\begin{proposition}
    $U+W\le V$.
\end{proposition}
\begin{proposition}
    $U+W$ is the smallest subspace of $V$ that contains $U,W$.
\end{proposition}
\begin{proof}
    Let $X$ be a subspace of $V$ that contains $U,W$. By closure, $ \bfu+\bfw\in X $ for all $ \bfu\in U,\bfw\in W $, so $ U+W\le X $. Hence it is the smallest subspace.
\end{proof}
\subsection{Subspaces and quotient}
Same as group theory, we can define subspaces and quotients. 
\begin{definition}[Quotient]
    Let $V$ a vector space over $\bbF$, and let $U \leq V$. The \emph{quotient space} $V/U$ is the abelian group $V/U$ equipped with the scalar multiplication $\bbF \times V/U \rightarrow V/U$, $(\lambda, \bfv + U) \mapsto \lambda \bfv + U$.
\end{definition}
The multiplication is well-defined, since if $ \bfv_1+U=\bfv_2+U $ then $ \bfv_1-\bfv_2\in U $, which implies $ \lambda(\bfv_1-\bfv_2) \in U$, and thus $ \lambda\bfv_1+U=\lambda\bfv_2+U\in V/U $.
\begin{proposition}
    $V/U$ is a vector space over $\bbF$.
\end{proposition}
\begin{proof}
    Let $ \lambda_1,\lambda_2\in \bbF $ and let $ \bfv_1+U,\bfv_2+U\in V/U $. Note that 
    \begin{align*}
        \lambda_1(\bfv_1+U)+\lambda_2(\bfv_2+U) &= (\lambda_1\bfv_1+U)+(\lambda_2\bfv_2+U)\\ 
        &= \lambda_1\bfv_1+\lambda_2\bfv_2+U\in V/U.\qedhere
    \end{align*}
\end{proof}

\subsection{Spans, independence and Steinitz Exchange Lemma}
\begin{definition}[Span of a Family of Vectors]
    Let $V$ be a vector space over $\mathbb F$ and $S\subset V$.
    We define the span of $S$ to be
    $$\langle S\rangle=\operatorname{span}(S)=\left\{\sum_{i=1}^n\lambda_i\bfs_i:n\in\mathbb N,\lambda_i\in \mathbb F,\bfs_i\in S\right\}$$
\end{definition}
That is, $\langle S\rangle$ consists of all possible \textit{finite} linear combination of elements of $S$.
By convention, $\langle \varnothing\rangle=\{0\}$.
\begin{remark}
    $ \langle S \rangle  $ is the smallest vector subspace of $V$ which contains $S$.
\end{remark}
\begin{example}
    \begin{enumerate}
        \item $V=\mathbb{R}^{3}$,
        \[
            S = \left\{\begin{pmatrix}
                1 \\ 0 \\ 0
            \end{pmatrix}, \begin{pmatrix}
                0 \\ 1 \\ 2
            \end{pmatrix}, \begin{pmatrix}
                3 \\ -2 \\ -4
            \end{pmatrix}\right\}.
        \]
        Verify that 
        \[
            \langle S \rangle = \left\{ \begin{pmatrix}
                a \\ b \\ 2b
            \end{pmatrix}:(a,b)\in \mathbb{R} \right\}.
        \]
        \item $ V = \mathbb{R}^{n} $, $\bfe_i$ standard basis, then $ V = \langle (\bfe_i)_{1\le i\le n} \rangle  $.
        \item Let $V=\mathbb R^X$ and $\delta_x:X\to\mathbb R$ be such that $\delta_x(y)=1_{x=y}$.
        Then $\langle \{\delta_x\}_{x\in\mathbb R}\rangle$ are the set of functions $f\in\mathbb R^X$ that has finite support ($ \operatorname{Supp} f = \{x:f(x)\neq 0\} $).
    \end{enumerate}
\end{example}
\begin{definition}
    Let $V$ be a vector space over $\mathbb F$ and $S\subset V$.
    We say $S$ spans $V$ if $\langle S\rangle =V$.
\end{definition}
\begin{example}
    Take $V=\mathbb R^2$, then any set of two non-parallel vectors would span $V$.
\end{example}
\begin{definition}
    A vector space $V$ over $\mathbb F$ is finite dimensional if there is a finite $S\subset V$ that spans $V$.
\end{definition}
\begin{example}
    $V=\mathbb P[x]$, the set of polynomials in $\mathbb R$ and $V_n=\mathbb P_n[x]$, the set of real polynomials with degree $\le n$.
    Then $V_n=\langle\{1,x,\ldots,x^n\}\rangle$ is finite dimensional, but $V$ is not finite dimensional as any finite set of polynomials must be contained in $V_n$ where $n$ is the maximal degree of polynomials in that set.
\end{example}
If $V$ is finite-dimensional, is there a minimal number of vectors in the family required so that the family spans $V$?
\begin{definition}[(Linear) Independence]
    Let $V$ be a vector space over $F$.

    We say $\{\bfv_1,\ldots,\bfv_n\}\subset V$ are (linearly) independent (or is a free family) if for any $\lambda_1,\ldots,\lambda_n\in F$
    $$\sum_{i=1}^n\lambda_i\bfv_i=0\implies\forall i,\lambda_i=0$$
    Equivalently, this set is not linearly independent if there exists $\lambda_1,\ldots,\lambda_n\in F$ not all zero such that $\sum_{i=1}^n\lambda_i\bfv_i=0$.
\end{definition}
\begin{example}
Let $V=\mathbb R^3$ and
$$\bfv_1=(1,0,0)^\top,\bfv_2=(0,1,0)^\top,\bfv_3=(1,1,0)^\top,\bfv_4=(0,1,1)^\top$$
Then $\{\bfv_1,\bfv_2\}$ is linearly independent.
Note that $\bfv_3\in\langle\{\bfv_1,\bfv_2\}\rangle$, so $\{\bfv_1,\bfv_2,\bfv_3\}$ is not linearly independent.
On the other hand, $\bfv_4\notin\langle\{\bfv_1,\bfv_2\}\rangle$, which as one can verify means that $\{\bfv_1,\bfv_2,\bfv_4\}$ is linearly independent.
\end{example}
\begin{remark}
    If the family $\{\bfv_i\}_{1\le i\le n}$ is linearly independent, then none of $\bfv_i$ is zero.
\end{remark}
\begin{definition}[Basis]
    A subset $S\subset V$ is a basis if it is linearly independent and $\langle S\rangle=V$.
\end{definition}
\begin{remark}
    When $S$ spans $V$, we say that $S$ is a generating family of $V$.
    So a basis is just a linearly independent(we also say free) generating family.
\end{remark}
\begin{example}
    \begin{enumerate}
        \item Take $V=\mathbb R^n$, then the family $\{\bfe_i\}_{1\le i\le n}$ where $\bfe_i$ is the vector having $1$ at $i^{th}$ entry and zero otherwise is a basis. $ (\bfe_i) $ are called the \textit{canonical basis}.
        \item Take $V=\mathbb C$ over $\mathbb C$, then $\{a\}$ is a basis for any $a\neq 0$.
        \item Take also $V=\mathbb C$ but over $\mathbb R$, then $\{1,i\}$ is a basis.
        \item Take $V=\mathbb P[x]$ be the set of polynomials in $\mathbb R$ and $S=\{x^n:n\ge 0\}$.
        Then $S$ is a basis.
        Worth noting that $|S|=\infty$ in this case.
    \end{enumerate}
\end{example}
\begin{lemma}
    If $V$ is a vector space over $F$, then $\{\bfv_1,\ldots,\bfv_n\}$ is a basis of $V$ if and only if for any vector $\bfv\in V$, there is a unique decomposition
    $$\bfv=\sum_{i=1}^n\lambda_i\bfv_i,\quad \lambda_i\in \mathbb{F}.$$
\end{lemma}
\begin{remark}
    If the conditions are true, then the tuple $(\lambda_1,\ldots,\lambda_n)$ (ordered via the ordering one chose on $\bfv_i$) is called the coordinate of $\bfv$ in the basis $(\bfv_i)$.
\end{remark}
\begin{proof}
    Suppose $ (\bfv_i) $ is a basis of $V$. This implies that $ \langle \bfv_i \rangle =V $, i.e. 
    \[
        \forall \bfv\in V\quad \exists (\lambda_1,\dots,\lambda_n)\in \bbF,\quad \bfv = \sum_{i=1}^{n}\lambda_i\bfv_i.
    \]
    Suppose that $ \exists (\lambda_i') $ s.t. the above holds, then 
    \[
        \sum_{i=n}^{n} \lambda_i-\lambda'_i=0 \Longrightarrow \lambda_i=\lambda_i'
    \]
    by independence.

    Suppose now that there is a unique decomposition. Then clearly $ \langle (\bfv_i) \rangle =V $. Consider 
    \[
        \sum_{i=1}^n\lambda_i\bfv_i=0,\quad \lambda_i\in \mathbb{F}.
    \]
    Clearly it is true for $ \lambda_i=0,\forall i $. By uniqueness, this is the only decomposition and thus $ (\bfv_i) $ is a basis.
\end{proof}
\begin{lemma}
    If $S$ is a finite set that spans $V$, then a subset of $S$ is a basis of $V$.
\end{lemma}
\begin{proof}
    If $S$ is independent, then we are done.
    Otherwise, there is some $\lambda\neq 0$ and $\lambda_\bfw$ such that there is $v\in S$ with
    $$\lambda \bfv+\sum_{\bfw\in S\setminus\{\bfv\}}\lambda_\bfw\bfw=0\implies \bfv=\frac{1}{\lambda}\sum_{\bfw\in S\setminus\{\bfv\}}\lambda_\bfw\bfw\in\langle S\setminus\{\bfv\}\rangle.$$
    Therefore $S\setminus\{\bfv\}$ also spans $V$.
    We can repeat this process and, by the well-ordering of $\mathbb N$, will reach a basis.
\end{proof}
\begin{theorem}[Steinitz Exchange Lemma]\label{thm:steinitz}
    Let $V$ be a finite dimensional vector space over $\bbF$. Take $\{\bfv_1,\ldots,\bfv_m\}\subset V$ linearly independent, $\{\bfw_1,\ldots,\bfw_n\}\subset V$ a generating set, then:
    \begin{enumerate}
        \item $m\le n$.
        \item Up to relabeling, $\{\bfv_1,\ldots,\bfv_m,\bfw_{m+1},\ldots,\bfw_n\}$ spans $V$.
    \end{enumerate}
\end{theorem}
\begin{proof}
    Suppose $\{\bfv_1,\ldots,\bfv_l,\bfw_{l+1},\ldots,\bfw_n\}$ spans $V$ for some $l<m$, then
    $$\exists\alpha_i,\beta_i\in \bbF, \bfv_{l+1}=\sum_{i\le l}\alpha_i\bfv_i+\sum_{i>l}\beta_i\bfw_i$$
    But $\{\bfv_i\}$ is linearly independent, so one of the $\beta_i$ is nonzero.
    By relabelling $\beta_{l+1}\neq 0$, then $\bfw_{l+1}\in\langle\{\bfv_1,\ldots,\bfv_l,\bfv_{l+1},\bfw_{l+2}\ldots,\bfw_n\}\rangle$, therefore the set of vectors 
    \[
        \{\bfv_1,\ldots,\bfv_l,\bfv_{l+1},\bfw_{l+2}\ldots,\bfw_n\}
    \]
    also spans $V$. If we start from 0, we are done after $m$ steps by induction, and thus $m\le n$, and
    \[
        \langle \bfv_1,\ldots,\bfv_m,\bfw_{m+1},\ldots,\bfw_n \rangle =V.\qedhere
    \]
\end{proof}

\subsection{Basis, Dimension and Direct Sum}
From the Steinitz Exchange Lemma, we have
\begin{corollary}\label{dim_well_defined}
    Let $V$ be a finite dimensional vector space, then any two bases of $V$ have the same cardinality.
\end{corollary}
We call this cardinality the \textit{dimension} of $V$, denoted by $ \dim V $ or $ \dim_\bbF V $, to specify the field.
\begin{corollary}
    Let $V$ be a vector space with $\dim V=n$, then:
    \begin{enumerate}
        \item Any independent set of vectors has size at most $n$.
        The size is exactly $n$ iff this set is a basis.
        \item Any spanning set has size at least $n$.
        The size is exactly $n$ iff this set is a basis.
    \end{enumerate}
\end{corollary}
\begin{proposition}
    Let $U,W$ be subspaces of $V$.
    If they are finite dimensional, then so is $U+W$ and
    $$\dim(U+W)=\dim U+\dim V-\dim(U\cap W)$$
\end{proposition}
\begin{proof}
    Pick a basis $\bfv_1,\ldots,\bfv_l$ of $U\cap W$ and extend it to a basis $\bfv_1,\ldots,\bfv_l,\bfu_1,\ldots,\bfu_m$ of $U$ and a basis $\bfv_1,\ldots,\bfv_l,$ $\bfw_1,\ldots,\bfw_n$ of $W$. 
    
    Claim that $B=\{\bfv_i\}\cup\{\bfu_i\}\cup\{\bfw_i\}$ is easily a basis of $U+W$.
    The equality follows.

    Indeed, it is obvious that $B$ is a generating set. To show linear independence, consider 
    \[
        \sum_{i=1}^{l}\alpha_i \bfv_i+\sum_{j=1}^{m}\beta_j \bfu_j+\sum_{k=1}^{n}\gamma_k\bfw_k=0.
    \]
    Rearrange this to get 
    \[
        \begin{aligned}
           & \sum_{i=1}^{l}\alpha_i \bfv_i+\sum_{j=1}^{m}\beta_j \bfu_j=-\sum_{k=1}^{n}\gamma_k\bfw_k\\ &\Longrightarrow \sum_{k=1}^{n}\gamma_k\bfw_k\in U \cap W\\ &\Longrightarrow \sum_{k=1}^{n}\gamma_k\bfw_k=\sum_{i=1}^{l}\delta_i\bfv_i.
        \end{aligned}
    \]
    Therefore we get 
    \[
        \sum_{i=1}^{l}(\alpha_i+\delta_i)\bfv_i+\sum_{j=1}^{m}\beta_j \bfu_j=0 \Longrightarrow \beta_j=0,\alpha_i=-\delta_i.
    \]
    Hence the original equation becomes 
    \[
        \sum_{i=1}^{l}\alpha_i\bfv_i+\sum_{k=1}^{n}\gamma_k\bfw_k=0 \Longrightarrow \alpha_i=\gamma_k=0.\qedhere
    \]
\end{proof}
\begin{proposition}
    If $V$ is a finite dimensional vectyor space and $U\le V$, then $U,V/U$ are both finite dimensional and $\dim V=\dim U+\dim V/U$.
\end{proposition}
\begin{proof}
    It is obvious that $U$ is finite dimensional.
    Choose a basis $\bfu_1,\ldots,\bfu_l$ and extend it to a basis $B=\{\bfu_1,\ldots,\bfu_l,\bfw_{l+1},\ldots,\bfw_n\}$ of $V$. Claim that $B'=\{\bfw_{l+1}+U,\ldots,\bfw_n+U\}$ is a basis of $V/U$ and we are done.

    Indeed, every non-trivial element in $ V/U $ has the form $ \bfv+U $, where $ \bfv\in V \setminus U $. Since $B$ is a basis, there exists $ \alpha_i,l+1\le i\le n $ such that 
    \[
        \bfv = \sum_{i=l+1}^{n}\alpha_i\bfw_i.
    \]
    Hence $ \bfv+U = \sum_{i=l+1}^{n}\alpha_i(\bfw_i +U)$ and thus $ B' $ generates $V/U$. To show independence, note that 
    \[
        \sum_{i=l+1}^{n}\beta_i \bfw_i+U=0 \Longrightarrow \sum_{i=l+1}^{n}\beta_i \bfw_i\in U \Longrightarrow \beta_i=0.\qedhere
    \]
\end{proof}
\begin{remark}
    If $U$ is a proper subspace of $V$, written $U<V$ (meaning that $U\le V$ and $U\neq V$), then the proposition gives us $\dim V/U\neq \{0\}$, so $\dim U<\dim V$.
\end{remark}
\begin{definition}
    Let $V$ be a vector space and $U,W\le V$.
    We say $V$ is the \textit{direct sum} of $U,W$, written $V=U\oplus W$, if every element $\bfv\in V$ can be written uniquely as $\bfv=\bfu+\bfw$ for $\bfu\in U,\bfw\in W$.
    If this happens, then we say $W$ is \textit{a direct complement} of $U$ in $V$.
\end{definition}
Note that direct complement is not unique in general.
\begin{example}
    Take $U=\mathbb R\times \{0\}$, then both $W=\{0\}\times \mathbb R$ and $W'=\langle\{(1,1)^\top\}\rangle$ are direct complements of $U$.
\end{example}
\begin{lemma}\label{equivalence_of_direct_sum}
    Let $U,W\le V$, then the followings are equivalent:
    \begin{enumerate}[(i)]
        \item $V=U\oplus W$.
        \item $V=U+W$ and $U\cap W=\{0\}$.
        \item For any basis $B_1$ of $U$ and $B_2$ of $W$, $B=B_1\cup B_2$ is a basis of $V$.
    \end{enumerate}
\end{lemma}
\begin{proof}
    (ii) $\Rightarrow$ (i): Suppose $V=U+W$ and $U\cap W=\{0\}$. Then $ \forall \bfv\in V,\exists (\bfu,\bfw)\in U\times W,\bfv=\bfu+\bfw $. To show uniqueness, suppose $ \bfv=\bfu_1+\bfw_1=\bfu_2+\bfw_2 $. Then $ \bfu_1-\bfu_2=\bfw_1-\bfw_2\in U \cap W=\{0\} $, so $ \bfu_1=\bfu_2,\bfw_1=\bfw_2$.

    (i) $\Rightarrow$ (iii): Let $ B_1,B_2 $ be bases of $U,W$ respectively. Let $ B=B_1 \cup B_2 $. Clearly $ \langle B \rangle =V $. Consider 
    \[
        \sum_{\bfu\in B_1}\lambda_{\bfu}\bfu+\sum_{\bfw\in B_2}\lambda_{\bfw}\bfw=0.
    \]
    Note that $ \lambda_\bfu=\lambda_\bfw=0 $ is a solution. Since $ V =U\oplus W $, the decomposition is unique so it is the only one, and we have independence.

    (iii) $\Rightarrow$ (ii): Any $\bfv\in V$ can be written as 
    \[
        \bfv=\sum_{\bfu\in B_1}\lambda_{\bfu}\bfu+\sum_{\bfw\in B_2}\lambda_{\bfw}\bfw
    \]
    so $ V=U+W $. Now let $ \bfv\in U\cap W $, then the above equation implies $\lambda_\bfu=\lambda_\bfw=0 $ and thus $ U\cap W=\{0\} $.
\end{proof}
\begin{definition}
    Let $V_1,\ldots,V_l\le V$, then we define
    $$\sum_{i=1}^lV_i=\{\bfv_1+\cdots+\bfv_l:\bfv_j\in V_j,1\le j\le l\}$$
    The sum is direct, i.e.
    $$\sum_{i=1}^lV_i=\bigoplus_{i=1}^lV_i$$
    if and only if $\bfv_1+\cdots +\bfv_l=\bfv_1'+\cdots +\bfv_l'$ implies $\bfv_j=\bfv_j'$ for any $1\le j\le l,\bfv_j\in V_j$ and $ \bfv_j'\in V_j $.
    Equivalently,
    $$V=\bigoplus_{i=1}^lV_i\iff \forall \bfv\in V,\exists!(\bfv_1,\ldots,\bfv_l)\in V_1\times\cdots\times V_l,\bfv=\sum_{i=1}^l\bfv_i.$$
\end{definition}
Lemma \ref{equivalence_of_direct_sum} is then generalized to
\begin{proposition}
    The followings are equivalent:
    \begin{enumerate}
        \item $\sum_{i=1}^lV_i=\bigoplus_{i=1}^lV_i$
        \item For any $i$,
        $V_i\cap\left( \sum_{j\neq i}V_j \right)=\{0\}$
        \item For any bases $B_i$ of $V_i$, the union $\bigcup_iB_i$ is a basis of $\sum_iV_i$.
    \end{enumerate}
\end{proposition}

\section{Linear Maps}
\subsection{Linear maps}
\begin{definition}[Linear Map]
    Let $V,W$ are vector spaces over $\bbF$, a function $\alpha:V\to W$ is \textit{linear} if for any $\lambda_1,\lambda_2\in \bbF$ and $\bfv_1,\bfv_2\in V$,
    $$\alpha(\lambda_1\bfv_1+\lambda_2\bfv_2)=\lambda_1\alpha(\bfv_1)+\lambda_2\alpha(\bfv_2).$$
\end{definition}
\begin{example}
    \begin{enumerate}
        \item Let $M$ be an $m\times n$ matrix, then $\alpha:\mathbb R^n\to\mathbb R^m$ via $x\mapsto Mx$ is a linear map.
        \item The functional $\alpha:C([0,1])\to C([0,1])$ via
        $$\alpha[f](x)=\int_0^xf(t)\,\mathrm dt$$
        is a linear map.
        \item Fix $x\in[a,b]$, then the evaluation map $\alpha:C([a,b])\to\mathbb R$ via $f\mapsto f(x)$ is a linear map.
    \end{enumerate}
\end{example}
\begin{remark}
    The identity map is a linear map.
    Composition of linear maps is also a linear map.
\end{remark}
\begin{lemma}
    Let $V,W$ be vector spaces over $\bbF$ and $B$ a basis for $V$.
    Let $\alpha_0:B\to W$ be a function, then there is a unique linear map $\alpha:V\to W$ that extends $\alpha_0$. i.e. $\exists \text{ linear }\alpha, \forall\bfv\in B, \alpha_0(\bfv)=\alpha(\bfv) $.
\end{lemma}
\begin{proof}
    For any $(\bfb_i)\in B$, necessarily $\alpha\left(\sum_i\lambda_i\bfb_i\right)=\sum_i\lambda\alpha_0(\bfb_i)$.
    This is sufficient.
\end{proof}
\begin{remark}
    This lemma is true for infinite dimensional vector spaces as well.
    Often, to define linear map, we often just define its values on a basis and extend it by this lemma.
\end{remark}
\begin{corollary}
    Two linear maps that agree on a basis are the same.
\end{corollary}
\subsection{Isomorphisms and the Rank-Nullity Theorem}
\begin{definition}
    Let $V,W$ be vector spaces over $\bbF$.
    A linear bijection $\alpha:V\to W$ is an \textit{isomorphism} (of vector spaces).
    If such a map exists, then we say $V,W$ are isomorphic (as vector spaces), written as $V\cong W$.
\end{definition}
\begin{remark}
    If $\alpha$ is an isomorphism, so is $\alpha^{-1}$.
\end{remark}
\begin{lemma}
    $\cong$ is an equivalence relation on the class of all vector spaces over $\bbF$.
\end{lemma}
\begin{proof}
    $i:V\to V$ is an isomorphism so $ V \cong V $. If $ \alpha:V\to W $ is an isomorphism then so is $ \alpha^{-1} $, and hence $ V \cong W \Rightarrow W \cong V $. If $ \beta:U\to V,\alpha:V\to W $ are isomorphisms, then $ \beta\circ \alpha:U\to W $ is an isomorphism and thus $ U \cong V \land V \cong W \Rightarrow U \cong W $. Hence it is an equivalence relation.
\end{proof}
\begin{theorem}
    If $V$ is a vector space over $\bbF$ of dimension $n$, then $V\cong \bbF^n$.
\end{theorem}
\begin{proof}
    Take a basis $\{\bfb_1,\ldots,\bfb_n\}$ of $V$, then $ \alpha:V\to \bbF^n $, defined by
    $$\alpha(x_1\bfb_1+\cdots+x_n\bfb_n)=(x_1,\ldots,x_n)^\top$$
    is an isomorphism.
\end{proof}
\begin{remark}
    Choosing a basis of $V$ is then just equivalent to choosing an isomorphism from $V$ to $\bbF^n$.
\end{remark}
\begin{theorem}\label{isom iff same dim}
    Let $V,W$ be finite dimensional vector spaces over $\bbF$.
    Then $V\cong W$ iff $\dim V=\dim W$.
\end{theorem}
\begin{proof}
    Any basis of $V$ induces a basis of $W$ via the isomorphism, so they have the same dimension.
    Therefore are both isomorphic to $\bbF^n$ where $n=\dim V=\dim W$.
\end{proof}
\begin{definition}
    Let $\alpha:V\to W$ be a linear map.
    We define the kernel of $\alpha$ to be $\ker\alpha=\{\bfv\in V:\alpha(\bfv)=0\}$ and the image to be $\operatorname{Im}\alpha=\alpha(V)=\{\bfw\in W:\exists \bfv\in V,\alpha(\bfv)=\bfw\}$.
\end{definition}
\begin{lemma}
    $\ker\alpha\le V,\operatorname{Im}\alpha\le W$.
\end{lemma}
\begin{example}
    Take $\alpha:C^\infty(\mathbb R)\to C^\infty(\mathbb R)$ by $\alpha(f)(t)=f^{\prime\prime}(t)+f(t)$.
    Then $\ker\alpha$ is spanned by $t\mapsto e^t$ and $t\mapsto e^{-t}$ and $\operatorname{Im}\alpha=C^{\infty}(\mathbb R)$.
\end{example}
There is a similar result to first isomorphism theorem:
\begin{theorem}
    Let $V,W$ be vector spaces over $F$ and $\alpha:V\to W$ be linear, then $V/{\ker\alpha}\cong \operatorname{Im}(\alpha)$ via $\bfv+\ker\alpha\mapsto \alpha(\bfv)$.
\end{theorem}
\begin{proof}
    Let $ \bar{\alpha} $ be the desired isomorphism(to be proved).

    Firstly check that $ \bar{\alpha} $ is well-defined. Indeed, if $ \bfv+\ker \alpha=\bfv'+\ker \alpha $, then $ \bfv-\bfv'\in \ker \alpha $ and thus $ \alpha(\bfv)=\alpha(\bfv')=0 $.

    Next show that $ \bar{\alpha} $ is a bijection. Let $ \bar{\alpha}(\bfv+\ker \alpha)=0 $, then $ \alpha(\bfv)=0 \Rightarrow \bfv+\ker \alpha=0+\ker \alpha $, so it is injective. Given $ \bfw\in \Im \alpha, \exists \bfv\in V $ such that $ \alpha(\bfv)=\bfw =\bar{\alpha}(\bfv+\ker \alpha)$, so it is surjective and hence bijective.
\end{proof}
\begin{definition}
    The rank of $\alpha:V\to W$ is $\rank(\alpha)=\dim\operatorname{Im}\alpha$ and nullity is $\nullity(\alpha)=\dim\ker\alpha$. Sometimes we write $ r(\alpha),n(\alpha) $. 
\end{definition}
Hence in the finite dimensional case, we can rewrite the preceding theorem to get
\begin{theorem}[Rank-Nullity Theorem]
    Let $\alpha:V\to W$ be linear where $V$ is finite dimensional.
    Then $\dim V=\rank(\alpha)+\nullity(\alpha)$.
\end{theorem}
\begin{proof}
    Since $ V/{\ker\alpha}\cong \operatorname{Im}(\alpha) $, we have 
    \begin{align*}
        & \dim (U/\ker \alpha)=\dim (\Im \alpha)\\ 
        \implies & \dim U-\dim \ker \alpha=\dim \Im \alpha\\ 
        \implies & \dim U = \rank \alpha+ \nullity \alpha.\qedhere
    \end{align*}
\end{proof}
\begin{corollary}[Classification of Isomorphism]
    Let $V,W$ be finite dimensional vector spaces with $\dim V=\dim W$ and $\alpha:V\to W$ be linear, then the followings are equivalent:
    \begin{enumerate}[(i)]
        \item $\alpha$ is injective.
        \item $\alpha$ is surjective.
        \item $\alpha$ is an isomorphism.
    \end{enumerate}
\end{corollary}
\begin{example}
    Consider
    $$V=\left\{ \begin{pmatrix}
        x\\
        y\\
        z
    \end{pmatrix}\in\mathbb R^3:x+y+z=0\right\}$$
    We want to compute $\dim V$.
    Consider $\alpha:\mathbb R^3\to\mathbb R$ via $(x,y,z)^\top\mapsto x+y+z$, then $r(\alpha)=1$ and $n(\alpha)=V$, so $\dim V=3-1=2$.
    Geometrically, $V$ is just a plane with normal $(1,1,1)^\top$.
\end{example}

\subsection{Linear Maps \texorpdfstring{$V \rightarrow W$}{V to W} and Matrices}
\subsubsection*{Space of linear maps}
We are now going to consider linear maps between two vector spaces $V$ and $W$.

\begin{definition}[Space of Linear Maps]
    Let $V$ and $W$ be vector spaces over $\bbF$. We define
    $$
    \mathcal{L}(V, W) = \{\alpha: V \rightarrow W \mid \alpha \text{ is a linear map}\}.
    $$
\end{definition}
\begin{proposition}
    $\mathcal{L}(V, W)$ is a vector space over $\bbF$.
    
    Moreover, if $V$ and $W$ are finite dimensional then so is $\mathcal{L}(V, W)$, and $\dim \mathcal{L}(V, W) = \dim V \cdot \dim W$.
\end{proposition}
\subsubsection*{Matrices}
\begin{definition}[Matrix]
    An $m \times n$ \emph{matrix} over $\bbF$ is an array with $m$ rows and $n$ columns, with entries in $\bbF$:
    $$
    A = (a_{ij}),
    $$
    with $1 \leq i \leq m$ and $1 \leq j \leq n$.

    The \textit{space of matrices} is the set $\mathcal{M}_{m,n}(\bbF)$ of $m \times n$ matrices over $\bbF$.
\end{definition}
By checking definitions we have
\begin{proposition}
    $ \mcM_{m,n}(\bbF) $ is a vector space over $ \bbF $.
\end{proposition}
\begin{proposition}
    $ \dim \mcM_{m,n}(\bbF) =m \times n$.
\end{proposition}
\begin{proof}
    We find an explicit basis. Pick $ 1\le i\le m,1\le j\le n $, define the \textit{elementary matrices} as
    \[
        E_{ij}=1 \text{ for entry } ij \text{ and } 0 \text{ elsewhere}.
    \]
    The set $\{E_{i, j} \mid 1 \leq i \leq m, 1 \leq j \leq m\}$ is clearly spanning and is also clearly linearly independent, so it is a basis. Counting elements, we see that this set has size $m \times n$, as required.
\end{proof}
\subsubsection*{Representation of linear maps by matrices}
Let $V$, $W$ be vector spaces over $\bbF$, and let $\alpha: V \rightarrow W$ be a linear map. 

Let $B = \{\bfv_1, \dots, \bfv_n\}$ be a basis for $V$, and $C = \{\bfw_1, \dots, \bfw_m\}$ be a basis for $W$. Then if $\bfv \in V$, we can write
$$
\bfv = \sum_{j = 1}^n \lambda_j \bfv_j = \begin{pmatrix}
    \lambda_1 \\ \vdots \\ \lambda_n
\end{pmatrix} \in \bbF^n,
$$
which are the `coordinates of $\bfv$' with the respect to the basis $B$, which we will call $[\bfv]_B$.

Similarly, for $\bfw \in W$, we can let $[\bfw]_C$ be the `coordinates' of $\bfw$ in the basis $C$.

\begin{definition}[Matrix of $\alpha$ in the $B, C$ Basis]
    We define $[\alpha]_{B, C}$, the \emph{matrix of $\alpha$} with respect to $B$ and $C$ by
    $$
    [\alpha]_{B, C} = \begin{pmatrix}
        [\alpha(\bfv_1)]_C, [\alpha(\bfv_2)]_C, \cdots, [\alpha(\bfv_n)]_C
    \end{pmatrix} \in \mathcal{M}_{m, n}(\bbF).
    $$ 
\end{definition}
\begin{lemma}
    For all $\v \in V$, we have $[\alpha(\v)]_C = [\alpha]_{B,C}  [\v]_B$.
\end{lemma}
\begin{proof}
    Let $\v \in V$, and let $\v = \sum_{j = 1}^n \lambda_j \v_j$. Then
    $$
    \alpha(\v) = \sum_{j = 1}^n \lambda_j \alpha(\v_j) = \sum_{j = 1}^n \lambda_j \sum_{i = 1}^m a_{ij} \w_i = \sum_{i = 1}^m \left(\sum_{j = 1}^n a_{ij} \lambda_j\right) \w_i,
    $$
    as required.
\end{proof}
By checking definitions,
\begin{lemma}
    Let $\alpha: V \rightarrow W$ and $\beta: U \rightarrow W$ be linear maps. Then if $A$ is a basis of $U$, $B$ is a basis of $V$ and $C$ is a basis of $W$, then
    $$
    [\alpha \circ \beta]_{A, C} = [\alpha]_{B, C}  [\beta]_{A, B}.
    $$
\end{lemma}
\begin{proposition}
    If $V$ and $W$ are vector spaces over $\bbF$ such that $\dim V = n$ and $\dim W = m$, then $\mathcal{L}(V, W) \cong \mathcal{M}_{m, n}(\bbF)$.
\end{proposition}
\begin{proof}
    Fix a basis $B, C$ of $V$ and $W$ respectively.
    Then consider the linear map
    $
    \theta: \mathcal{L}(V, W) \rightarrow \mathcal{M}_{m, n}(\bbF)
    $ given by $\alpha \mapsto [\alpha]_{B, C}$.
    This map is clearly linear. Let $ A=(a_{ij}) $, then the linear map 
    \[
        \alpha: \bfv_j\to \sum_{i=1}^{m}a_{ij}\w_i
    \]
    on bases $B,C$. Then $ \theta(\alpha)=A $ so it is surjective. Since a linear map is uniquely determined by the action on basis, $ \theta $ is surjective and hence bijective. Thus $\theta$ is an isomorphism and $ \mcL(V,W) \cong \mcM_{m,n}(\bbF) $. 

    Alternatively, the proposition is direct by theorem \ref{isom iff same dim}.
\end{proof}
\begin{remark}
    If $B$ is a basis of $V$, $C$ a basis of $W$, and 
    \[
        \epsilon_B:\begin{aligned}
            V&\to \bbF^n\\ 
            \bfv&\to [\bfv]_B
        \end{aligned}\quad \epsilon_C:\begin{aligned}
            W&\to \bbF^m\\ 
            \w &\to [\w]_C
        \end{aligned}
    \]
    then the following diagram commute:
    \begin{center}
        $\begin{tikzcd}
            V & W \\
            {\mathbb{F}^n} & {\mathbb{F}^m}
            \arrow["\alpha", from=1-1, to=1-2]
            \arrow["{\epsilon_B}"', from=1-1, to=2-1]
            \arrow["{[\alpha]_{B,C}}"', from=2-1, to=2-2]
            \arrow["{\epsilon_C}", from=1-2, to=2-2]
        \end{tikzcd}$
    \end{center}
\end{remark}

\begin{example}
    Let $V$ and $W$ be finite dimensional vector spaces.
    Consider the linear map $\alpha: V \rightarrow W$, and suppose $Y \leq V$. Consider the image of $Y$ under $\alpha$, $Z=\alpha(Y) = \{ \w \in W \mid \w= \alpha(\bfy), \bfy \in Y\}$.

    Define the sets $B' = \{\v_1, \dots, \v_k\}$ and $B'' = \{\v_{k + 1}, \dots, \v_n\}$ such that the union of disjoint sets $B = B' \cup B''$ is a basis for $V$, and $B'$ is a basis of $Y$ (that is, we extend using Steinitz exchange lemma).
    
    Similarity we define $C' = \{\w_1, \dots, \w_\ell\}$ and $C'' = \{\w_{\ell + 1}, \dots, \w_m\}$ such that the union of disjoint sets $C = C' \cup C''$ is a basis for $W$ and $C'$ is a basis for $\alpha(Y)$.

    Then we have the matrix
    $$ 
    [\alpha]_{B, C} = \begin{pmatrix}
        \alpha(\v_1) & \cdots & \alpha(\v_k) & \alpha(\v_{k + 1}) & \cdots & \alpha(\v_n)
    \end{pmatrix},
    $$
    and this will look like the block matrix
    $$
    \begin{pmatrix}
        A & B \\
        0 & C \\
    \end{pmatrix}
    $$
    where $A$ is $k \times \ell$ such that $ A = [\alpha\restriction_Y]_{B',C'} $. 
    Also $ \alpha $ induces a well-defined map $ \bar{\alpha}:V/Y\to W/Z $ such that $ \v+Y\mapsto \alpha(\bfv)+Z $, with matrix given by $ [\bar{\alpha}]_{B'',C''}=C $.
\end{example}
\subsection{Change of basis}
Suppose we have two bases $ B = \qty{\bfv_1, \dots, \bfv_n}, B' = \qty{\bfv_1', \dots, \bfv_n'} $ of $ V $ and corresponding $ C, C' $ for $ W $.
If we have a linear map $ [\alpha]_{B,C} $, we are interested in finding the components of this linear map in another basis, that is,
\[
	[\alpha]_{B,C} \mapsto [\alpha]_{B',C'}
\]
\begin{definition}
	The \textbf{change of basis} matrix $ P $ from $ B' $ to $ B $ is
	\[
		P = \begin{pmatrix}
			[\bfv_1']_B & \cdots & [\bfv_n']_B
		\end{pmatrix}
	\]
	which is the identity map in $ B' $, written
	\[
		P = [I]_{B', B}
	\]
\end{definition}
\begin{lemma}
	For a vector $ v $,
	\[
		[\bfv]_B = P [\bfv]_{B'}
	\]
\end{lemma}
\begin{proof}
	We have
	\[
		[\alpha(\bfv)]_C = [\alpha]_{B,C} \cdot [\bfv]_C
	\]
	Since $ P = [I]_{B', B} $,
	\[
		[I(\bfv)]_B = [I]_{B', B} \cdot [\bfv]_{B'} \implies [\bfv]_B = P[\bfv]_{B'}
	\]
	as required.
\end{proof}
\begin{remark}
	$ P $ is an invertible $ n \times n $ square matrix.
	In particular,
	\[
		P^{-1} = [I]_{B,B'}
	\]
	Indeed,
	\[
		I_n = [I \cdot I]_{B,B} = [I]_{B',B} \cdot [I]_{B',B}
	\]
	where $ I_n $ is the $ n \times n $ identity matrix.
\end{remark}
\begin{proposition}
	If $ \alpha $ is a linear map from $ V $ to $ W $, and $ P = [I]_{B',B}, Q = [I]_{C',C} $, we have
	\[
		A' = [\alpha]_{B',C'} = [I]_{C,C'}[\alpha]_{B,C}[I]_{B,'B} = Q^{-1}AP
	\]
	where $ A = [\alpha]_{B,C}, A' = [\alpha]_{B',C'} $.
\end{proposition}
\begin{proof}
	\begin{align*}
		[\alpha(\bfv)]_C                     &= Q [\alpha(\bfv)]_{C'}= Q [\alpha]_{B',C'} [\bfv]_{B'} = QA [\mathbf{v}]_{B'} \\
		[\alpha(\bfv)]_C                     & = [\alpha]_{B,C} [\bfv]_B        = AP[\bfv]_{B'}                  \\
		\implies \forall \bfv,\ QA[\bfv]_{B'} & = AP[\bfv]_{B'}                  \\
		\implies QA                     & = AP \implies A'=Q^{-1}AP.\qedhere
	\end{align*}
\end{proof}

\begin{definition}
	Matrices $ A, A' $ are called \textbf{equivalent} if
	\[
		A' = Q^{-1}AP
	\]
	for some invertible matrices $ Q\in \mathcal{M}_{m \times m}, P\in \mathcal{M}_{n \times n} $.
\end{definition}
\begin{remark}
	This defines an equivalence relation on $ M_{m,n}(F) $.
	\begin{itemize}
		\item $ A = I_m^{-1} A I_n $;
		\item $ A' = Q^{-1} AP \implies A = Q A' P^{-1} $;
		\item $ A' = Q^{-1}AP, A'' = (Q')^{-1}A'P' \implies A'' = (QQ')^{-1}A(PP') $.
	\end{itemize}
\end{remark}
\begin{proposition}
	Let $ \alpha \colon V \to W $ be a linear map.
	Then there exists a basis $ B $ of $ V $ and a basis $ C $ of $ W $ such that
	\[
		[\alpha]_{B,C} = \begin{pmatrix}
			I_r & 0 \\
			0   & 0
		\end{pmatrix}
	\]
	so the components of the matrix are exactly the identity matrix of size $ r $ in the top-left corner, and zeroes everywhere else.
\end{proposition}
\begin{proof}
    Keep in mind that
    \begin{quotation}
        \textit{If you choose a basis, choose it wisely.}
    \end{quotation}
	Fix $ r \in \mathbb N $ such that $ \dim \ker \alpha = n - r $,
	and construct a basis $ \qty{\bfv_{r+1}, \dots, \bfv_n} $ of the kernel.
	Extend this to a basis of the entirety of $ V $, that is, $ \qty{\bfv_1,\dots,\bfv_n} $.
    We want to show that
	\[
		\qty{\alpha(\bfv_1), \dots, \alpha(\bfv_r)}
	\]
	is a basis of $ \Im \alpha $.

	Indeed, it spans $W$:
	\[
        \bfv         = \sum_{i=1}^n \lambda_i \bfv_i,         \quad
		\alpha(\bfv) = \sum_{i=1}^n \lambda_i \alpha(\bfv_i)
		          = \sum_{i=1}^r \lambda_i \alpha(\bfv_i)
    \]
	Then if $ \bfy \in \Im \alpha $, there exists $ \bfv $ such that $ \alpha(\bfv) = \bfy $, i.e. $ \mathbf{y} $ is in the span.

	Further, it is linearly independent:
	\begin{align*}
		\sum_{i=1}^r \lambda_i \alpha(\bfv_i) = 0&\implies
		\alpha\qty(\sum_{i=1}^r \lambda_i \bfv_i)= 0 \implies
		\sum_{i=1}^r \lambda_i \bfv_i \in \ker \alpha \\
        &\implies \sum_{i=1}^r \lambda_i \bfv_i = \sum_{i=r+1}^n \lambda_i \bfv_i \\
		&\implies \sum_{i=1}^r \lambda_i \bfv_i - \sum_{i=r+1}^n \lambda_i \bfv_i  = 0.
	\end{align*}
	But since $ \qty{\bfv_1, \dots, \bfv_n} $ is a basis, $ \lambda_i = 0 $ for all $ i $.
	Hence $ \qty{\alpha(\bfv_i)} $ is a basis of $ \Im \alpha $.

	Now, extend this basis to the whole of $ W $ to form
	\[
		\qty{\alpha(\bfv_1), \dots, \alpha(\bfv_r), \bfw_{r+1}, \dots, \bfw_n}
	\]
	Now,
	\begin{align*}
		[\alpha]_{BC} & = \begin{pmatrix}
			\alpha(\bfv_1) & \cdots & \alpha(\bfv_r) & \alpha(\bfv_{r+1}) & \cdots & \alpha(\bfv_n)
		\end{pmatrix} \\
		              & = \begin{pmatrix}
			I_r & 0 \\
			0   & 0
		\end{pmatrix}\qedhere
	\end{align*}
\end{proof}
\begin{remark}
	This also proves the rank-nullity theorem:
	\[
		\rank \alpha + \nullity \alpha = n
	\]
\end{remark}
\begin{corollary}
	Any $ m \times n $ matrix $ A $ is equivalent to a matrix of the form
	\[
		\begin{pmatrix}
			I_r & 0 \\
			0   & 0
		\end{pmatrix}
	\]
	where $ r = \rank A $.
\end{corollary}

\subsection{Rank of matrices}\ \vspace*{-1.5em}
\begin{definition}
	Let $ A \in M_{m,n}(\bbF) $.
	Then, the \textbf{column rank} of $ A $, $ r_c(A) $, is the dimension of the subspace of $ \mathbb{F}^n $ spaned by the column vectors.
	\[
		r_c(A) = \dim \operatorname{span} \qty{\bfc_1, \dots, \bfc_n}.
	\]

    Similarly, the \textbf{row rank} of $A$ is defined by $ r(A^\top) $.
\end{definition}

\begin{remark}
	If $ \alpha $ is a linear map, represented in bases $ B, C $ by the matrix $ A $, then
	\[
		\rank \alpha = r_c(A). 
	\]
    i.e. column rank $=$ rank.
\end{remark}

\begin{proposition}
	Two matrices are equivalent if and only if they have the same column rank:
	\[
		r_c(A) = r_c(A')
	\]
\end{proposition}
\begin{proof}
	If the matrices are equivalent, then $ A = [\alpha]_{BC}, A' = [\alpha]_{B',C'} $.
	Then
	\[
		r_c(A) = r_c(\alpha) = r_c(A')
	\]
	Conversely, if $ r_c(A) = r_c(A') = r $, then $ A, A' $ are equivalent to
	\[
		\begin{pmatrix}
			I_r & 0 \\
			0   & 0
		\end{pmatrix}
	\]
	which indicates $ A, A' $ are equivalent.
\end{proof}

\begin{theorem}
    $ r_c(A) = r_c(A^\top) $. 
\end{theorem}
\begin{proof}
    Let $ r = r_c(A) $. We know that $A$ is equivalent to the matrix 
    \[
        Q^{-1}AP = \begin{pmatrix}
			I_r & 0 \\
			0   & 0
		\end{pmatrix}_{m\times n}
    \]
    Take the top
    \[
		P^\top A^\top \qty(Q^{-1})^\top = (Q^{-1}AP)^\top = \begin{pmatrix}
			I_r & 0 \\
			0   & 0
		\end{pmatrix}_{m \times n}^\top = \begin{pmatrix}
			I_r & 0 \\
			0   & 0
		\end{pmatrix}_{n \times m}
	\]
    which proves the theorem. 
\end{proof}

Hence we can just write $ r(A) $, without subscript.

\subsection{Elementary operations and elementary matrices}
\paragraph{Aside: change of basis in the same space} Consider the following special case of changing basis.
If $ \alpha \colon V \to V $ is linear, $ \alpha $ is called an \textbf{endomorphism}.
If $ B = C, B' = C' $ then the special case of the change of basis formula is
\[
	[\alpha]_{B',B'} = P^{-1} [\alpha]_{B,B} P
\]
Then, we say square matrices $ A, A' $ are \textbf{similar} or \textbf{conjugate} if there exists $ P $ such that $ A' = P^{-1} A P $.

\begin{definition}
	An \textbf{elementary column operation} is
	\begin{enumerate}[(i)]
		\item swap columns $ i, j $
		\item replace column $ i $ by $ \lambda $ multiplied by the column
		\item add $ \lambda \times$ column $ i $ to column $ j \ (i\neq j)$
	\end{enumerate}
\end{definition}

\begin{note}
    \begin{itemize}
        \item Elementary row operations are defined similarly. 
        \item The elementary operations are invertible. 
    \end{itemize}
\end{note}

These operations can be described through the action of \textbf{elementary matrices}: 
\begin{definition}
    The elementary matrices are defined by 
    \begin{enumerate}[(i)]
        \item Swap columns $i,j$  
        \[
            T_{ij} = \begin{pmatrix}
                I_n & 0 & 0   \\
                0   & A & 0   \\
                0   & 0 & I_m
            \end{pmatrix};\quad A = \begin{pmatrix}
                0 & 0   & 1 \\
                0 & I_k & 0 \\
                1 & 0   & 1
            \end{pmatrix}
        \]
        \item Multiply column $i$ by $\lambda$  
        \[
            M_{i,\lambda} = \begin{pmatrix}
                I_n & 0       & 0   \\
                0   & \lambda & 0   \\
                0   & 0       & I_m
            \end{pmatrix}
        \]
        \item Add a multiple of a column,
        \[
            C_{ij,\lambda} = I + \lambda E_{ij},\quad E_{ij} \text{ is the standard basis}
        \]
    \end{enumerate}
\end{definition}

An elementary column (or row) operation can be performed by multiplying $ A $ by the corresponding elementary matrix from the right (on the left for row operations).

This provides a constructive proof that any $ n \times n $ matrix is equivalent to
\[
	\begin{pmatrix} I_r & 0 \\ 0 & 0 \end{pmatrix}
\]
\begin{proof}
    Start with $A$. If all entries are 0, we are done. 

    Pick $ a_{ij} = \lambda\neq 0 $: 
    \begin{itemize}
        \item Swap rows $i$ and $1$ 
        \item Swap columns $i$ and $1$ 
    \end{itemize}
    We get $ \lambda = a_{11} $, and by multiplying $ 1/\lambda $ we get $ a_{11}= 1 $. 

    Now the matrix becomes 
    \[
        \begin{pmatrix}
            1 &  0 \\
            0 &  \tilde{A} \\
        \end{pmatrix},\quad \tilde{A}\in \mathcal{M}_{(n-1),(n-1)} 
    \]
    and we can proceed inductively. After a finite step we get 
    \[
        \begin{pmatrix} I_r & 0 \\ 0 & 0 \end{pmatrix} = Q^{-1}AP = \underbrace{E'_\ell \cdots E'_1}_{\text{row operations}} A \underbrace{E_1 \cdots E_{l}}_{\text{column operations}}\qedhere
    \]
\end{proof}

\subsection{Gauss' pivot algorithm, finding inverses}

If we use only row operations, we can reduce each matrix to the so-called \textbf{row echelon form}: 
\[
    \begin{pmatrix}
        1 & 0 & \cdots & 0 &  b \\
        0 & \cdots & 1 & 0 &  c \\
        0 & \cdots & 0 & 1 &  d \\
    \end{pmatrix}
\]
where the `1's are pivots. The algorithm is described as follows 
\begin{enumerate}
    \item Assume $a_{i1}\neq 0$ for some $i$
    \item Swap rows $i$ and $1$ 
    \item Divide first row by $ \lambda = a_{i 1} $ to get $a_{11}=1$ 
    \item Use 1 to clear the rest of the first column
    \item Move to the second column and repeat
\end{enumerate}

This process is exactly the process used to solve a linear system of equations (Gaussian elimination).

\begin{lemma}
	If $ A $ is an $ n \times n $ square invertible matrix, then we can obtain $ I_n $ using only row elementary operations, or only column elementary operations.
\end{lemma}
\begin{proof}
	We show an algorithm that constructs this $ I_n $.
	This is exactly going to invert the matrix, since the resultant operations can be combined to get the inverse matrix.
	We will show here the proof for column operations.
	We argue by induction on the number of rows.
	Suppose we can make the form
	\[
		\begin{pmatrix} I_k & 0 \\ A & B \end{pmatrix}
	\]
	We want to obtain the same structure with $ k+1 $ rows.
	We claim that there exists $ j > k $ such that $ a_{k+1,j} \neq 0 $.
	Indeed, otherwise we can show that the vector
	\[
		\begin{pmatrix} 0 \\ \vdots \\ 1 \\ \vdots \\ 0 \end{pmatrix} = \delta_{k+1,i}
	\]
	is not in the span of the column vectors of $ A $.
	This contradicts the invertibility of the matrix.
	Now, we will swap columns $ k+1, j $ and divide this column by $ \lambda $.
	We can now use this 1 to clear the rest of the $ k+1 $ row.

	Inductively, we have found $ A E_1 \dots E_n = I_n $ where $ E_n $ are elementary.
	Thus, we can find $ A^{-1} $.
\end{proof}
\begin{proposition}
	Any invertible square matrix is a product of elementary matrices.
\end{proposition}
The proof is exactly the proof of the lemma above.


\section{Dual spaces and dual maps}
\subsection{Dual spaces}\ \vspace*{-1.5em}
\begin{definition}
	Let $ V $ be an $ \mathbb{F} $-vector space.
	Then $ V^* $ is the \textbf{dual} of $ V $, defined by
	\[
		V^* = \mcL(V,\mathbb F) = \qty{\alpha \colon V \to \mathbb F}
	\]
	where the $ \alpha $ are linear.
	If $ \alpha \colon V \to \mathbb F $ is linear, then we say $ \alpha $ is a linear form.
	So the dual of $ V $ is the set of linear forms on $ V $.
\end{definition}
\begin{example}
	The trace $ \operatorname{tr} \colon M_{n,n}(\mathbb F) \to \mathbb F $ is a linear form on $ M_{n,n}(\mathbb F) $.
\end{example}
\begin{example}
	Consider functions $ [0,1] \to \mathbb R $.
	We can define $ T_f \colon C^\infty([0,1], \mathbb R) \to \mathbb R $ such that $ \phi \mapsto \int_0^1 f(x) \phi(x) \dd{x} $.
	Then $ T_f $ is a linear form on $ C^{\infty}([0,1], \mathbb R) $.
	We can then reconstruct $ f $ given $ T_f $.
	This mathematical formulation is called distribution.
\end{example}
\begin{lemma}
	Let $ V $ be an $ \mathbb F $-vector space with a finite basis $ B = \qty{\bfe_1, \dots, \bfe_n} $.
	Then there exists a basis $ B^* $ for $ V^* $ given by
	\[
		B^* = \qty{\varepsilon_1, \dots, \varepsilon_n}; \quad \varepsilon_j \qty( \sum_{i=1}^n a_i \bfe_i ) = a_j
	\]
	We call $ B^* $ the \textbf{dual basis} for $ B $.
\end{lemma}
We know
\[
    \varepsilon_j \qty( \sum_{i=1}^n a_i \bfe_i ) = a_j
\]
Equivalently,
\[
    \varepsilon_j (\bfe_i) = \delta_{ij}
\]
\begin{proof}
	First, we will show that the set of linear forms as defined is linearly independent.
	For all $ i $,
	\[
        \sum_{j=1}^n \lambda_j \varepsilon_j= 0 \implies
		\qty( \sum_{j=1}^n \lambda_j \varepsilon_j ) \bfe_i = 0 \implies
		\sum_{j=1}^n \lambda_j \varepsilon_j(e_i)                   = 0 \implies
		\lambda_i = 0.
    \]
	Now we show that the set spans $ V^* $.
	Suppose $ \alpha \in V^* $, $ x \in V $.
	\[
        \alpha(x) = \alpha\qty(\sum_{j=1}^n \lambda_j \bfe_j)
		           = \sum_{i=1}^n \lambda_j \alpha(\bfe_j)
    \]
	Conversely, let
	\[
		\sum_{i=1}^n \alpha(\bfe_j) \varepsilon_j \in V^*
	\]
	Then
	\begin{align*}
		\qty( \sum_{i=1}^n \alpha(\bfe_j) \varepsilon_j) (\bfx) & = \sum_{j=1}^n \alpha(\bfe_j) \varepsilon_j\qty(\sum_{k=1}^n \lambda_k \bfe_k) \\
        & = \sum_{j=1}^n \alpha(\bfe_j) \sum_{k=1}^n \lambda_k \varepsilon_j(\bfe_k)     \\
        & = \sum_{j=1}^n \alpha(\bfe_j) \sum_{k=1}^n \lambda_k \delta_{jk}            \\
        & = \sum_{j=1}^n \alpha(\bfe_j) \lambda_j                                     \\
        & = \alpha(\bfx)
	\end{align*}
	We have then shown that
	\[
		\alpha = \sum_{j=1}^n \alpha(\bfe_j) \varepsilon_j
	\]
	as required.
\end{proof}
\begin{corollary}
	If $ V $ is finite-dimensional, then $ \dim V = \dim V^* $.
\end{corollary}
\begin{remark}
	It is sometimes convenient to think of $ V^* $ as the spaces of row vectors of length $ \dim V $ over $ \bbF $.
	For instance, consider the basis $ B = (\bfe_1, \dots, \bfe_n) $, so $ \bfx = \sum_{i=1}^n x_i \bfe_i $.
	Then we can pick $ (\varepsilon_1, \dots, \varepsilon_n) $ a basis of $ V^* $, so $ \alpha = \sum_{i=1}^n \alpha_i \varepsilon_i $.
	Then
	\[
		\alpha(\bfx) = \sum_{i=1}^n \alpha_i \varepsilon_i(\bfx) = \sum_{i=1}^n \alpha_i \varepsilon\qty(\sum_{j=1}^n x_j \bfe_j) = \sum_{i=1}^n \alpha_i x_i
	\]
	This is exactly
	\[
		\begin{pmatrix} \alpha_1 & \cdots & \alpha_n \end{pmatrix} \begin{pmatrix} x_1 \\ \vdots \\ x_n \end{pmatrix}
	\]
	which essentially defines a scalar product between the two spaces.
\end{remark}

\subsection{Annihilators}\ \vspace*{-1.5em}
\begin{definition}
	Let $ U \subseteq V $.
	Then the annihilator of $ U $ is
	\[
		U^0 = \qty{\alpha \in V^* \colon \forall \bfu \in U, \alpha(\bfu) = 0}
	\]
\end{definition}
\begin{lemma}
	\begin{enumerate}[(i)]
		\item $ U^0 \leq V^* $;
		\item If $ U \leq V $ and $ \dim V < \infty $, then $ \dim V = \dim U + \dim U^0 $.
	\end{enumerate}
\end{lemma}
\begin{proof}
	\begin{enumerate}[(i)]
		\item First, note that $ 0 \in U^0 $ since $ \alpha(0) = 0 $ by linearity.
		      If $ \alpha, \alpha' \in U^0 $, then for all $ \bfu \in U $,
		      \[
			      (\alpha + \alpha')(\bfu) = \alpha(\bfu) + \alpha'(\bfu) = 0
		      \]
		      Further, for all $ \lambda \in F $,
		      \[
			      (\lambda \alpha)(\bfu) = \lambda \alpha(\bfu) = 0
		      \]
		      Hence $ U^0 \leq V^* $.
		\item Let $ (\bfe_1, \dots, \bfe_k) $ be a basis of $ U $ and extend it to a basis
		\[
            B = (\bfe_1, \dots, \bfe_k, \bfe_{k+1}, \dots, \bfe_n)
        \]
		      Let $ (\varepsilon_1, \dots, \varepsilon_n) $ be the dual basis $ B^* $.
		      We then will prove that
		      \[
			      U^0 = \langle\varepsilon_{k+1}, \dots, \varepsilon_n\rangle
		      \]
		      If $ i > k $, then $ \varepsilon_i(e_k) = \delta_{ik} = 0 $.
		      Hence $ \varepsilon_i \in U^0 $.
		      Thus $ \langle{\varepsilon_{k+1}, \dots, \varepsilon_n}\rangle \subset U^0 $.
		      Let $ \alpha \in U^0 $.
		      Then $ \alpha = \sum_{i=1}^n \alpha_i \varepsilon_i $.
		      For $ i \leq k $, $ \alpha \in U^0 $ hence $ \alpha(e_i) = 0 $.
		      Hence,
		      \[
			      \alpha = \sum_{i=k+1}^n \alpha_i \varepsilon_i
		      \]
		      Thus
		      \[
			      \alpha \in \langle{\varepsilon_{k+1}, \dots, \varepsilon_n}\rangle
		      \]
		      as required.\qedhere
	\end{enumerate}
\end{proof}

\subsection{Dual maps}\ \vspace{-1.5em}
\begin{lemma}
	Let $ V, W $ be $ \bbF $-vector spaces.
	Let $ \alpha \in \mcL(V,W) $.
	Then there exists a unique $ \alpha^* \in \mcL(W^*, V^*) $ such that
	\[
		\varepsilon \mapsto \varepsilon \circ \alpha
	\]
	It is called the \textbf{dual map}.
\end{lemma}
\begin{proof}
	$ \varepsilon(\alpha) \colon V \to \bbF $ linear by linearity of $ \varepsilon, \alpha $,	so $ \varepsilon \circ \alpha \in V^* $.
	Now we show $ \alpha^* $ is linear:
	\[
		\alpha^*(\theta_1 + \theta_2) = (\theta_1 + \theta_2)(\alpha) = \theta_1 \circ \alpha + \theta_2 \circ \alpha = \alpha^*(\theta_1) + \alpha^*(\theta_2)
	\]
	and
	\[
		\alpha^*(\lambda \theta) = \lambda \alpha^*(\theta)
	\]
	as required.
	Hence $ \alpha^* \in L(W^*, V^*) $.
\end{proof}
\begin{proposition}
	Let $ V, W $ be finite-dimensional $ \bbF $-vector spaces with bases $ B, C $ respectively.
	Then
	\[
		[\alpha^*]_{C^*, B^*} = [\alpha]^\top_{B, C}
	\]
	Thus, we can think of the dual map as the \textit{adjoint} of $ \alpha $.
\end{proposition}
\begin{proof}
	Let $ B = (b_1, \dots, b_n) $, $ C = (c_1, \dots, c_m) $, $ B^* = (\beta_1, \dots, \beta_n) $, $ C^* = (\gamma_1, \dots, \gamma_m) $.
	Let $ [\alpha]_{B,C} = (a_{ij}) $.
	Then
	\begin{align*}
		\alpha^*(\gamma_r)(b_s) & = \gamma_r \circ \alpha(b_s)= \gamma_r \qty( \sum_t a_{ts} c_t ) \\
		                            & = \sum_t a_{ts} \gamma_r(c_t) = \sum_t a_{ts} \delta_{tr}          \\
		                            & = a_{rs}
	\end{align*}
	We can conversely write $ [\alpha^*]_{C^*, B^*} = (m_{ij}) $ and
	\begin{align*}
		\alpha^*(\gamma_r)      & = \sum_{i=1}^n m_{ir} \beta_i      \\
		\alpha^*(\gamma_r)(b_s) & = \sum_{i=1}^n m_{ir} \beta_i(b_s)= \sum_{i=1}^n m_{ir} \delta_{is}  \\
		                            & = m_{sr}
	\end{align*}
	which gives $ a_{rs} = m_{sr} $, as required.
\end{proof}

\subsection{Properties of the dual map, double dual}

Let $ \alpha \in \mcL(V,W) $, and $ \alpha^* \in L(W^*, V^*) $.
Let $ B $ and $ C $ be bases of $ V, W $ respectively, and $ B^*, C^* $ be their duals.
We have proven that
\[
	[\alpha]_{B,C} = [\alpha^*]^\top_{B,C}
\]
\begin{lemma}
	Suppose that $ E = (e_1, \dots, e_n) $ and $ F = (f_1, \dots, f_n) $ are bases of $ V $.
	Let $ P = [I]_{F, E} $ be a change of basis matrix from $ F $ to $ E $.
	The bases $ E^* = (\varepsilon_1, \dots, \varepsilon_n) $, $ F^* = (\eta_1, \dots, \eta_n) $ are the corresponding dual bases.
	Then, the change of basis matrix from $ F^* $ to $ E^* $ is
	\[
		\qty(P^{-1})^\top
	\]
\end{lemma}
\begin{proof}
	Consider
	\[
		[I]_{F^*, E^*} = [I]^\top_{E, F} = \qty([I]_{F, E}^{-1})^\top = \qty(P^{-1})^\top
	\]
\end{proof}
\begin{lemma}
	Let $ V, W $ be $ \bbF $-vector spaces.
	Let $ \alpha \in \mcL(V, W) $.
	Let $ \alpha^* $ be the corresponding dual map.
	Then, denoting $ N(\alpha) $ for the kernel of $ \alpha $,
	\begin{enumerate}[(i)]
		\item $ N(\alpha^*) = (\Im \alpha)^0 $, so $ \alpha^* $ is injective if and only if $ \alpha $ is surjective.
		\item $ \Im \alpha^* \leq (N(\alpha))^0 $, with equality if $ V, W $ are finite-dimensional.
		      In this finite-dimensional case, $ \alpha^* $ is surjective if and only if $ \alpha $ is injective.
	\end{enumerate}
\end{lemma}
\begin{remark}
	In many applications, it is often simpler to understand the dual map $ \alpha^* $ than it is to understand $ \alpha $.
\end{remark}
\begin{proof}
	\begin{enumerate}[(i)]
        \item Let $ \epsilon\in W^* $. Then
        \begin{align*}
            \epsilon\in N(\alpha^*) &\iff \alpha^*(\epsilon) = 0 \iff \epsilon\circ \alpha = 0 \\ 
            &\iff \forall \mathbf{v}\in V, \ \epsilon(\alpha(\mathbf{v})) = 0\\ 
            &\iff \epsilon\in (\Im(\alpha))^0. 
        \end{align*}
        \item Let $ \varepsilon \in \Im \alpha^* $.
        Then $ \alpha^*(\phi) = \varepsilon $ for some $ \phi \in W^* $.
        Then, for all $ \bfu \in N(\alpha) $, $ \varepsilon(\bfu) = \qty(\alpha^*(\phi))(\bfu) = \phi \circ \alpha(\bfu) = \phi(\alpha(\bfu)) = 0 $.
        So $ \Im \alpha^* \leq (N(\alpha))^0 $.
    
        In the finite-dimensional case, we can compare the dimension of these two spaces.
        \begin{align*}
            \dim \Im \alpha^* &= r(\alpha^*) = r\qty([\alpha^*]_{C^*, B^*}) \\ 
            &= r\qty([\alpha]_{B,C}^\top) = r\qty([\alpha]_{B,C}) = r(\alpha) = \dim \Im \alpha
        \end{align*}
        Due to the rank-nullity theorem, $ \dim \Im \alpha^* = \dim V - \dim N(\alpha) = \dim\qty[(N(\alpha))^0] $.
        Hence,
        \[
            \Im \alpha^* \leq (N(\alpha))^0;\quad \dim \Im \alpha^* = \dim (N(\alpha))^0
        \]
        The dimensions are equal, and one is a subspace of the other, hence the spaces are equal.
    \end{enumerate}
\end{proof}

\begin{definition}
	Let $ V $ be an $ \bbF $-vector space.
	Let $ V^* $ be the dual of $ V $.
	The \textbf{double dual} or \text{bidual} of $ V $ is
	\[
		V^{* *} = \mcL(V^*, \bbF) = (V^*)^*
	\]
\end{definition}
\begin{remark}
	In general, there is no obvious relation between $ V $ and $ V^* $.
	However, the following useful facts hold about $ V $ and $ V^{* *} $.
	\begin{enumerate}
		\item There is a \textit{canonical embedding} from $ V $ to $ V^{* *} $.
		      In particular, there exists $ i $ in $ \mcL(V, V^{* *}) $ which is injective.
		\item There are examples of infinite-dimensional spaces where $ V \simeq V^{* *} $.
		      These are called reflexive spaces.
		      Such spaces are investigated in the study of Banach spaces.
	\end{enumerate}
\end{remark}
\begin{theorem}
	$ V $ embeds into $ V^{* *} $.
\end{theorem}
\begin{proof}
	Choose a vector $ \bfv \in V $ and define the linear form $ \hat v \in \mcL(V^*, F) $ such that
	\[
		\hat v(\varepsilon) = \varepsilon(\mathbf{v})
	\]
	So clearly $ \hat v $ is linear.
	We want to show $ \hat v \in V^{* *} $.
	If $ \varepsilon \in V^*, \varepsilon(v) \in F $.
	Further, $ \lambda_1, \lambda_2 \in F $ and $ \varepsilon_1, \varepsilon_2 \in V^* $ give
	\[
		\hat v (\lambda_1 \varepsilon_1 + \lambda_2 \varepsilon_2) = (\lambda_1 \varepsilon_1 + \lambda_2 \varepsilon_2)(v) = \lambda_1 \varepsilon_1(v) + \lambda_2 \varepsilon_2(v) = \lambda_1 \hat v(\varepsilon_1) + \lambda_2 \hat v(\varepsilon_2)
	\]
\end{proof}
\begin{theorem}
	If $ V $ is finite-dimensional, then $ i \colon V \to V^{* *} $ given by $ i(\bfv) = \hat v $ is an isomorphism.
\end{theorem}
\begin{proof}
	We will show $ i $ is linear.
	If $ \bfv_1, \bfv_2 \in V, \lambda_1, \lambda_2 \in F $, then
	\[
		i(\lambda_1 \bfv_1 + \lambda_2 \bfv_2) (\varepsilon) = \varepsilon(\lambda_1 \bfv_1 + \lambda_2 \bfv_2) = \lambda_1 \varepsilon(\bfv_1) + \lambda_2 \varepsilon(\bfv_2) = \lambda_1 \hat v_1(\varepsilon) + \lambda_2 \hat v_2(\varepsilon)
	\]
	Now, we will show that $ i $ is injective for finite-dimensional $ V $.
	Let $ \bfe \in V \setminus \qty{0} $.
	We will show that $ \bfe \not\in \ker i $
    .
	We extend $ \bfe $ into a basis $ (\bfe, \bfe_2, \dots, \bfe_n) $ of $ V $.
	Now, let $ (\varepsilon, \varepsilon_2, \dots, \varepsilon_n) $ be the dual basis.
	Then $ \hat e(\varepsilon) = \varepsilon(\bfe) = 1 $.
	In particular, $ \hat e \neq 0 $.
	Hence $ \ker i = \qty{0} $, so it is injective.

	We now show that $ i $ is an isomorphism.
	We need to simply compute the dimension of the image under $ i $.
	Certainly, $ \dim V = \dim V^* = \dim (V^*)^* = \dim V^{* *} $.
	Since $ i $ is injective, $ \dim V = \dim V^{* *} $.
	So $ i $ is surjective as required.
\end{proof}
\begin{lemma}
	Let $ V $ be a finite-dimensional $ \bbF $-vector space.
	Let $ U \leq V $.
	Then,
	\[
		i(U)=\hat U = U^{00}
	\]
	After identifying $ V $ and $ V^{* *} $, we typically say
	\[
		U = U^{00}
	\]
	although this is is incorrect notation and not an equality.
\end{lemma}
\begin{proof}
	We will show that $ \hat U \leq U^{00} $.
	Indeed, let $ \bfu \in U $, then by definition
	\[
		\forall \varepsilon \in U^0, \varepsilon(\bfu) = 0 \implies \hat u(\varepsilon) = 0
	\]
	Hence $ \hat u \in U^{00} $ and so $ \hat U \leq U^{00} $.

	Now, we will compute dimension:	$ \dim U^{00} = \dim V - \dim U^0 = \dim U $.
	Since $ \hat U \simeq U $, their dimensions are the same, so $ U^{00} = \hat U $.
\end{proof}
\begin{remark}
	Due to this identification of $ V^{* *} $ and $ V $, we can define
	\[
		T \leq V^*, T^0 = \qty{\bfv \in V \colon \forall \theta \in T, \theta(\bfv) = 0}
	\]
\end{remark}
\begin{lemma}
	Let $ V $ be a finite-dimensional $ \bbF $-vector space.
	Let $ U_1, U_2 $ be subspaces of $ V $.
	Then
	\begin{enumerate}[(i)]
		\item $ (U_1 + U_2)^0 = U_1^0 \cap U_2^0 $;
		\item $ (U_1 \cap U_2)^0 = U_1^0 + U_2^0 $
	\end{enumerate}
\end{lemma}
\begin{proof}
	Let $ \theta \in V^* $.
	Then $ \theta \in (U_1 + U_2)^0 \iff \forall \bfu_1 \in U_1, \bfu_2 \in U_2, \theta(\bfu_1 + \bfu_2) = 0 $.
	Hence $ \theta(\bfu) = 0 $ for all $ \bfu \in U_1 \cup U_2 $ by linearity.
	Hence $ \theta \in U_1^0 \cap U_2^0 $.
	Now, take the annihilator of (i) and $ U^{00} = U $ to complete part (ii).
\end{proof}

\section{Bilinear forms}\ \vspace{-1.5em}
\begin{definition}
	Let $ U, V $ be $ \bbF $-vector spaces.
	Then $ \varphi \colon U \times V \to F $ is a \textit{bilinear form} if it is linear in both components:
    \begin{align*}
        \varphi(\mathbf{u},\cdot)&: V \to \mathbb{F} \in V^* \quad \text{given }\mathbf{u}\\
        \varphi(\cdot, \mathbf{v})&: U \to \mathbb{F} \in U^* \quad \text{given }\mathbf{v}
    \end{align*}
\end{definition}

\begin{example}
	\begin{enumerate}
        \item Consider the map $ V \times V^* \to F $ given by
        \[
            (v, \theta) \mapsto \theta(v)
        \]
    
        \item The scalar product on $ U = V = \mathbb R^n $ is given by
        \[
            \psi(\bfx, \bfy) = \sum_{i=1}^n x_i y_i
        \]
    
        \item Let $ U = V = C([0,1], \mathbb R) $ and consider the inner product
        \[
            \varphi(f,g) = \int_0^1 f(t)g(t) \dd{t}
        \]
    \end{enumerate}
\end{example}

\begin{definition}
	If $ B = (\bfe_1, \dots, \bfe_m) $ is a basis of $ U $ and $ C = (\bff_1, \dots, \bff_n) $ is a basis of $ V $, and $ \varphi \colon U \times V \to F $ is a bilinear form,.
    
    The matrix of $ \varphi $ in $B,C$ is
	\[
		[\varphi]_{B, C} = \qty( \varphi(\bfe_i, \bff_j) )_{1 \leq i \leq m\atop 1 \leq j \leq n}
	\]
\end{definition}

\begin{lemma}
	We can link $ \varphi $ with its matrix in a given basis as follows.
	\[
		\varphi(\bfu,\bfv) = [\bfu]_B^\top [\varphi]_{B, C} [\bfv]_C
	\]
\end{lemma}

\begin{proof}
	Let $ \bfu = \sum_{i=1}^m \lambda_i \bfe_i $ and $ v = \sum_{j=1}^n \mu_j \mathbf{f}_j $.
	Then
	\begin{align*}
        \varphi(\bfu,\bfv) &= \varphi\qty( \sum_{i=1}^m \lambda_i \mathbf{e}_i, \sum_{j=1}^n \mu_j \mathbf{f}_j ) \\ 
        &= \sum_{i=1}^m \sum_{j=1}^n \lambda_i \mu_j \varphi(\mathbf{e}_i,\mathbf{f}_j) = [u]_B^\top [\varphi]_{B,C} [v]_C. \qedhere
    \end{align*}
\end{proof}

\begin{definition}
	A blinear map $ \varphi: U\times V \to \mathbb{F} $ determines two linear maps: 
	\[
		\varphi_L \colon U \to V^* \quad \varphi_L(\bfu) \colon V \to \bbF \quad \bfv \mapsto \varphi(\bfu,\bfv)
	\]
	\[
		\varphi_R \colon V \to U^* \quad \varphi_R(\bfv) \colon U \to \bbF \quad \bfu \mapsto \varphi(\bfu,\bfv)
	\]
	In particular,
	\[
		\varphi_L(\bfu)(\bfv) = \varphi(\bfu,\bfv) = \varphi_R(\bfv)(\bfu)
	\]
\end{definition}

\begin{lemma}
	Let $ B = (\bfe_1, \dots, \bfe_m) $ be a basis of $ U $, and let $ B^* = (\varepsilon_1, \dots, \varepsilon_m) $ be its dual; and let $ C = (\bff_1, \dots, \bff_n) $ be a basis of $ V $, and let $ C^* = (\eta_1, \dots, \eta_n) $ be its dual.
	Let $ A = [\varphi]_{B,C} $.
	Then
	\[
		[\varphi_R]_{C, B^*} = A,\quad [\varphi_L]_{B, C^*} = A^\top.
	\]
\end{lemma}
\begin{proof}
    Assuming summation convention.
    \begin{itemize}
        \item $ \varphi_L(\mathbf{e}_i)(\mathbf{f}_j) = \varphi(\mathbf{e}_j,\mathbf{f}_j) = A_{ij} $, so $ \varphi_L(\mathbf{e}_i) = A_{ij}\eta_j $, i.e. $  [\varphi_L]_{B, C^*} = A^\top $. 
        \item $ \varphi_R(\mathbf{f}_j)(\mathbf{e}_i) = \varphi(\mathbf{e}_i,\mathbf{f}_j) = A_{ij} $, so $ \varphi_R(\mathbf{f}_j) = A_{ij}\epsilon_i $ i.e. $ [\varphi_R]_{C, B^*} = A $. \qedhere
    \end{itemize}
\end{proof}
\begin{definition}
	$ \ker \varphi_L $ is called the \textbf{left kernel} of $ \varphi $.
	$ \ker \varphi_R $ is the \textbf{right kernel} of $ \varphi $.
\end{definition}
\begin{definition}
	We say that $ \varphi $ is \textbf{non-degenerate} if $ \ker \varphi_L = \ker \varphi_R = \qty{0} $.
	Otherwise, $ \varphi $ is \textbf{degenerate}.
\end{definition}

\begin{lemma}
	Let $ B $ be a basis of $ U $, and let $ C $ be a basis of $ V $, where $ U, V $ are finite-dimensional.
	Let $ \varphi \colon U \times V \to F $ be a bilinear form.
	Let $ A = [\varphi]_{B,C} $.
	Then, $ \varphi $ is non-degenerate if and only if $ A $ is invertible.
\end{lemma}
\begin{corollary}
	If $ \varphi $ is non-degenerate, then $ \dim U = \dim V $.
\end{corollary}
\begin{proof}
	Suppose $ \varphi $ is non-degenerate.
	Then $ \ker \varphi_L = \ker \varphi_R = \qty{0} $.
	This is equivalent to saying that $ n(A^\top) = n(A) = 0 $.
	By rank-nullity theorem, $ r(A^\top) = \dim U $ and $ r(A) = \dim V $.
	This is equivalent to saying that $ A $ is invertible.
	Note that this forces $ \dim U = \dim V $.
\end{proof}

\begin{remark}
	The canonical example of a non-degenerate bilinear form is the scalar product $ \mathbb R^n \times \mathbb R^n \to \mathbb R $ represented by the identity matrix in the standard basis.
\end{remark}

\begin{corollary}
	If $ U $ and $ V $ are finite-dimensional with $ \dim U = \dim V $, then choosing a non-degenerate bilinear form $ \varphi \colon U \times V \to \bbF $ is equivalent to choosing an isomorphism $ \varphi_L \colon U \simeq V^* $.
\end{corollary}
\begin{definition}
	If $ T \subset U $, we define
	\[
		T^\perp = \qty{ \bfv \in V \colon \forall \bft \in T, \varphi(\bft,\bfv) = 0 }
	\]
	Further, if $ S \subset V $, we define
	\[
		^\perp S = \qty{ \bfu \in U \colon \forall \bfs \in S, \varphi(\bfu,\bfs) = 0 }
	\]
	These are called the \textbf{orthogonals} of $ T $ and $ S $.
\end{definition}

\begin{proposition}
	Let $ B, B' $ be bases of $ U $ and $ P = [I]_{B', B} $, let $ C, C' $ be bases of $ V $ and $ Q = [I]_{C', C} $, and finally let $ \varphi \colon U \times V \to \bbF $ be a bilinear form.
	Then
	\[
		[\varphi]_{B', C'} = P^\top [\varphi]_{B,C} Q
	\]
\end{proposition}
\begin{proof}
	We have $ \varphi(\bfu,\bfv) = [\bfu]_B^\top [\varphi]_{B,C} [\bfv]_C $.
	Changing coordinates, we have
	\[
		\varphi(\bfu,\bfv) = (P [\bfu]_{B'})^\top [\varphi]_{B,C} (Q [\bfv]_{C'}) = [\bfu]_{B'}^\top (P^\top [\varphi]_{B,C} Q) [\bfv]_{C'}. \qedhere
	\]
\end{proof}

\begin{lemma}
	The \textbf{rank} of a bilinear form $ \varphi $, denoted $ r(\varphi) $, is the rank of any matrix representing $ \varphi $.
	This quantity is well-defined.
\end{lemma}
\begin{remark}
	$ r(\varphi) = r(\varphi_R) = r(\varphi_L) $, since $ r(A) = r(A^\top) $.
\end{remark}
\begin{proof}
	For any invertible matrices $ P, Q $, $ r(P^\top A Q) = r(A) $.
\end{proof}

\section{Determinant and trace}
\subsection{Trace}\ \vspace{-1.5em}
\begin{definition}
	The \textit{trace} of a square matrix $ A \in M_{n,n}(\bbF) \equiv M_n(\bbF) $ is defined by
	\[
		\tr A = \sum_{i=1}^n a_{ii}
	\]
	The trace is a linear form.
\end{definition}
\begin{lemma}
	$ \tr (AB) = \tr (BA) $ for any matrices $ A, B \in M_n(\bbF) $.
\end{lemma}
\begin{proof}
	We have
	\[
		\tr (AB) = \sum_{i=1}^n \sum_{j=1}^n a_{ij} b_{ji} = \sum_{j=1}^n \sum_{i=1}^n b_{ji} a_{ij} = \tr (BA)\qedhere
	\]
\end{proof}
\begin{corollary}
	Similar matrices have the same trace.
\end{corollary}
\begin{proof}
	\[
		\tr(P^{-1}AP) = \tr (A P^{-1} P) = \tr A\qedhere
	\]
\end{proof}

\begin{definition}
	If $ \alpha \colon V \to V $ is linear, we can define the trace of $ \alpha $ as
	\[
		\tr \alpha = \tr [\alpha]_B
	\]
	for any basis $ B $.
	This is well-defined by the corollary above.
\end{definition}
\begin{lemma}
	If $ \alpha \colon V \to V $ is linear, $ \alpha^* \colon V^* \to V^* $ satisfies
	\[
		\tr \alpha = \tr \alpha^*
	\]
\end{lemma}
\begin{proof}
	\[
		\tr \alpha = \tr [\alpha]_B = \tr [\alpha]_B^\top = \tr [\alpha^*]_{B^*} = \tr \alpha^*\qedhere
	\]
\end{proof}

\subsection{Determinants}
Recall the following facts about permutations and transpositions.
$ S_n $ is the group of permutations of the set $ \qty{1, \dots, n} $; the group of bijections $ \sigma \colon \qty{1, \dots, n} \to \qty{1, \dots, n} $.
A transposition $ \tau_{k \ell} = (k, \ell) $ is defined by $ k \mapsto \ell, \ell \mapsto k, x \mapsto x $ for $ x \neq k, \ell $.
Any permutation $ \sigma $ can be decomposed as a product of transpositions.
This decomposition is not necessarily unique, but the parity of the number of transpositions is well-defined.
We say that the signature of a permutation, denoted $ \varepsilon \colon S_n \to \qty{-1, 1} $, is $ 1 $ if the decomposition has even parity and $ -1 $ if it has odd parity.
We can then show that $ \varepsilon $ is a homomorphism.

\begin{definition}
	Let $ A \in \mcM_n(\bbF) $.
	We define
	\[
		\det A = \sum_{\sigma \in S_n} \varepsilon(\sigma) A_{\sigma(1) 1} \dots A_{\sigma(n) n}
	\]
\end{definition}

\begin{example}
	Let $ n = 2 $.
	Then,
	\[
		A = \begin{pmatrix} a_{11} & a_{12} \\ a_{21} & a_{22} \end{pmatrix} \implies \det A = a_{11} a_{22} - a_{12} a_{21}
	\]
\end{example}
\begin{lemma}
	If $ A = (a_{ij}) $ is an upper (or lower) triangular matrix (with zeroes on the diagonal), then $ \det A = 0 $.
\end{lemma}
\begin{proof}
	Let $ (a_{ij}) = 0 $ for $ i > j $.
	Then
	\[
		\det A = \sum_{\sigma \in S_n} \varepsilon(\sigma) a_{\sigma(1) 1} \dots a_{\sigma(n) n}
	\]
	For the summand to be nonzero, $ \sigma(j) \leq j $ for all $ j $.
	Thus,
	\[
		\det A = a_{1 1} \dots a_{n n} = 0\qedhere
	\]
\end{proof}

\begin{lemma}
	Let $ A \in M_n(F) $.
	Then, $ \det A = \det A^\top $.
\end{lemma}
\begin{proof}
	\begin{align*}
		\det A & = \sum_{\sigma \in S_n} \varepsilon(\sigma) a_{\sigma(1) 1} \dots a_{\sigma(n) n}      \\
		       & = \sum_{\sigma^{-1} \in S_n} \varepsilon(\sigma) a_{\sigma(1) 1} \dots a_{\sigma(n) n} \\
		       & = \sum_{\sigma \in S_n} \varepsilon(\sigma^{-1}) a_{1 \sigma(1)} \dots a_{n \sigma(n)} \\
		       & = \sum_{\sigma \in S_n} \varepsilon(\sigma) a_{1 \sigma(1)} \dots a_{n \sigma(n)}      \\
		       & = \det A^\top\qedhere
	\end{align*}
\end{proof}

\begin{definition}
	A \textbf{volume form} $ d $ on $ \bbF^n $ is a function 
	\[
		d \colon \underbrace{F^n \times \dots \times F^n}_{n \text{ times}} \to \bbF
	\]
	satisfying
	\begin{enumerate}[(i)]
		\item $ d $ is multilinear: for all $ i \in \qty{1, \dots, n} $ and for all $ \bfv_1, \dots, \bfv_{i-1}, \bfv_{i+1}, \dots, \bfv_n \in F^n $, the map $ \bbF^n \to \bbF $ defined by
		      \[
			      \bfv \mapsto (\bfv_1, \dots, \bfv_{i-1}, \bfv, \bfv_{i+1}, \dots, \bfv_n)
		      \]
		      is linear.
		      In other words, this map is an element of $ (\bbF^n)^* $.
		\item $ d $ is alternating: for $ \bfv_i = \bfv_j $ for some $ i \neq j $, $ d = 0 $.
	\end{enumerate}
\end{definition}

So an alternating multilinear form is a volume form.
We want to show that, up to multiplication by a scalar, the determinant is the only volume form.

\begin{lemma}
	The map $ (\bbF^n)^n \to \bbF $ defined by $ (A^{(1)}, \dots, A^{(n)}) \mapsto \det A $ is a volume form, where $ A^{(i)} $ is the $i$th column.
\end{lemma}
\begin{proof}
	Multilinear: 
	Fix $ \sigma \in S_n $, and consider $ \prod_{i=1}^n a_{\sigma(i) i} $.
	It contains exactly one term in each column of $ A $,
	so the map $ (A^{(1)}, \dots, A^{(n)}) \mapsto \prod_{i=1}^n a_{\sigma(i) i} $ is multilinear.
	This then implies that the determinant multilinear.

	Alternating:
	Suppose there are $k\neq \ell$ such that $ A^{(k)} = A^{(\ell)} $. 
	Let $ \tau = ( k \ell ) $ be the transposition exchanging $ k $ and $ \ell $.
	Then, for all $ i, j \in \qty{1, \dots, n} $, $ a_{ij} = a_{i \tau(j)} $.
	We can decompose permutations into two disjoint sets: $ S_n = A_n \cup \tau A_n $, where $ A_n $ is the alternating group of order $ n $.
	Now, 
	\begin{align*}
		\det A &= \sum_{\sigma\in A_n} \prod_{i=1}^{n} a_{i \sigma(i)} - \sum_{\sigma\in A_n} \prod_{i=1}^{n} a_{i \tau\sigma(i)}\\ 
		&= \sum_{\sigma\in A_n} \prod_{i=1}^{n} a_{i \sigma(i)} - \sum_{\sigma\in A_n} \prod_{i=1}^{n} a_{i \sigma(i)} = 0,
	\end{align*}
	which proves that the map is alternating, and hence completes the proof. 
\end{proof}

\begin{lemma}
	Let $ d $ be a volume form.
	Then, swapping two entries changes the sign.
\end{lemma}
\begin{proof}
	Take the sum of these two results:
	\begin{align*}
		d(\bfv_1, \dots, \bfv_i, \dots, \bfv_j, \dots, \bfv_n) & + d(\bfv_1, \dots, \bfv_j, \dots, \bfv_i, \dots, \bfv_n)               \\
		                                           & = d(\bfv_1, \dots, \bfv_i, \dots, \bfv_j, \dots, \bfv_n)               \\
		                                           & + d(\bfv_1, \dots, \bfv_j, \dots, \bfv_i, \dots, \bfv_n)               \\
		                                           & + d(\bfv_1, \dots, \bfv_i, \dots, \bfv_i, \dots, \bfv_n)               \\
		                                           & + d(\bfv_1, \dots, \bfv_j, \dots, \bfv_j, \bfv_n)                      \\
		                                           & = 2 d(\bfv_1, \dots, \bfv_i + \bfv_j, \dots, \bfv_i + \bfv_j, \dots, \bfv_n) \\
		                                           & = 0
	\end{align*}
	as required.
\end{proof}

\begin{corollary}
	If $ \sigma \in S_n $ and $ d $ is a volume form,
	\[
		d(\bfv_{\sigma(1)}, \dots, \bfv_{\sigma(n)}) = \varepsilon(\sigma) d(\bfv_1, \dots, \bfv_n)
	\]
\end{corollary}
\begin{proof}
	We can decompose $ \sigma $ as a product of transpositions $ \prod_{i=1}^{n_\sigma} \tau_i $.
\end{proof}

\begin{theorem}
	Let $ d $ be a volume form on $ \bbF^n $.
	Let $ A $ be a matrix whose columns are $ A^{(i)} $.
	Then
	\[
		d(A^{(1)}, \dots, A^{(n)}) = \det A \cdot d(\bfe_1, \dots, \bfe_n)
	\]
	So there is a unique volume form up to a constant multiple.
	Hence $ \det A $ is the only volume form such that $ d(\bfe_1, \dots, \bfe_n) = 1 $.
\end{theorem}

\begin{proof}
	\[
		d(A^{(1)}, \dots, A^{(n)}) = d\qty(\sum_{i=1}^n a_{i1} \bfe_i, A^{(2)}, \dots, A^{(n)})
	\]
	Since $ d $ is multilinear,
	\[
		d(A^{(1)}, \dots, A^{(n)}) = \sum_{i=1}^n a_{i1} d\qty(\bfe_i, A^{(2)}, \dots, A^{(n)})
	\]
	Inductively on all columns,
	\begin{align*}
		d(A^{(1)}, \dots, A^{(n)}) &= \sum_{i=1}^n \sum_{j=1}^n a_{i1} a_{j2} d\qty(\bfe_i, \bfe_j, A^{(3)}, \dots, A^{(n)}) \\ 
		&= \dots = \sum_{1 \leq i_1, \dots \leq n} \prod_{k=1}^n a_{i_k k} d(\bfe_{i_1}, \dots \bfe_{i_n})
	\end{align*}
	Since $ d $ is alternating, for $ d(\bfe_{i_1}, \dots, \bfe_{i_n}) $ to be nonzero, the $ i_k $ must be different, so this corresponds to a permutation $ \sigma \in S_n $.
	\[
		d(A^{(1)}, \dots, A^{(n)}) = \sum_{\sigma \in S_n} \prod_{k=1}^n a_{\sigma(k) k} \varepsilon(\sigma) d(\bfe_1, \dots, \bfe_n)
	\]
	which is exactly the determinant up to a constant multiple.
\end{proof}

\subsection{Some properties of determinant}

\begin{lemma}
	Let $ A, B \in \mcM_n(\bbF) $.
	Then $ \det(AB) = \det(A) \det(B) $.
\end{lemma}
\begin{proof}
	Given $ A $, we define the volume form $ d_A \colon (\bbF^n)^n \to \bbF $ by
	\[
		d_A(\bfv_1, \dots, \bfv_n) \mapsto \det(A \bfv_1, \dots, A \bfv_n)
	\]
	$ \bfv_i \mapsto A \bfv_i $ is linear, and the determinant is multilinear, so $ d_A $ is multilinear.
	If $ i \neq j $ and $ \bfv_i = \bfv_j $, then $ \det(\dots, A \bfv_i, \dots, A \bfv_j, \dots) = 0 $ so $ d_A $ is alternating.
	Hence $ d_A $ is a volume form.

	Hence there exists a constant $ C_A $ such that $ d_A(\bfv_1, \dots, \bfv_n) = C_A \det(\bfv_1, \dots, \bfv_n) $.
	We can compute $ C_A $ by considering the basis vectors; $ A \bfe_i = A_i $ where $ A_i $ is the $ i $th column vector of $ A $.
	Then,
	\[
		C_A = d_A(\bfe_1, \dots, \bfe_n) = \det(A\bfe_1, \dots, A\bfe_n) = \det A
	\]
	Hence,
	\[
		\det(AB) = d_A(B) = \det A \det B\qedhere
	\]
\end{proof}

\begin{definition}
	Let $ A \in \mcM_n(\bbF) $.
	We say that
	\begin{enumerate}[(i)]
		\item $ A $ is \textbf{singular} if $ \det A = 0 $,
		\item $ A $ is \textbf{non-singular} if $ \det A \neq 0 $.
	\end{enumerate}
\end{definition}

\begin{lemma}
	If $ A $ is invertible, it is non-singular.
\end{lemma}
\begin{proof}
	If $ A $ is invertible, there exists $ A^{-1} $.
	Then, since the determinant is a homomorphism,
	\[
		\det(A A^{-1}) = \det I = 1
	\]
	Thus $ \det A \det A^{-1} = 1 $ and hence neither of these determinants can be zero.
\end{proof}
\begin{theorem}
	Let $ A \in \mcM_n(\bbF) $.
	The following are equivalent.
	\begin{enumerate}[(i)]
		\item $ A $ is invertible;
		\item $ A $ is non-singular;
		\item $ r(A) = n $.
	\end{enumerate}
\end{theorem}
\begin{proof}
	(i) implies (ii).
	(i) and (iii) are equivalent by the rank-nullity theorem.
	So it suffices to show that (ii) implies (iii).

	Suppose $ r(A) < n $.
	Then we will show $ A $ is singular.
	
	We have $ \dim \spn(A_1, \dots, A_n) < n $.
	Therefore, since there are $ n $ vectors, $ (A_1, \dots, A_n) $ is not free.
	So there exist scalars $ \lambda_i $ not all zero such that $ \sum_i \lambda_i A_i = 0 $.
	Choose $ j $ such that $ \lambda_j \neq 0 $.
	Then,
	\[
		A_j = -\frac{1}{\lambda_j} \sum_{i \neq j} \lambda_i A_i
	\]
	So we can compute the determinant of $ A $ by
	\[
		\det A = \det(A_1, \dots, -\frac{1}{\lambda_j} \sum_{i \neq j} \lambda_i A_i, \dots, A_n)
	\]
	Since the determinant is alternating and linear in the $ j $th entry, its value is zero.
	So $ A $ is singular as required.
\end{proof}

\begin{remark}
	The above theorem gives necessary and sufficient conditions for invertibility of a set of $ n $ linear equations with $ n $ unknowns.
\end{remark}

\subsection{Determinant of linear maps}\ \vspace{-1.5em}

\begin{lemma}
	Similar matrices have the same determinant.
\end{lemma}
\begin{proof}
	\[
		\det (P^{-1} A P) = \det(P^{-1}) \det A \det P = \det A \det (P^{-1} P) = \det A\qedhere
	\]
\end{proof}

\begin{definition}
	If $ \alpha :V\to V $ is an endomorphism, then we define
	\[
		\det \alpha = \det [\alpha]_{B}
	\]
	where $ B $ is any basis of the vector space.
	This value does not depend on the choice of basis.
\end{definition}

\begin{theorem}
	$ \det \colon \mcL(V,V) \to \bbF $ satisfies the following properties.
	\begin{enumerate}[(i)]
		\item $ \det I = 1 $;
		\item $ \det (\alpha\beta) = \det\alpha \det\beta $;
		\item $ \det \alpha \neq 0 $ if and only if $ \alpha $ is invertible, and in this case, $ \det(\alpha^{-1}) \det \alpha = 1 $.
	\end{enumerate}
\end{theorem}
This is simply a reformulation of the previous theorem for matrices.
The proof is simple, and relies on the invariance of the determinant under a change of basis.

\subsection{Determinant of block-triangular matrices}\ \vspace{-1.5em}
\begin{lemma}
	Let $ A \in \mcM_k(\bbF) $, $ B \in \mcM_\ell(\bbF) $, $ C \in \mcM_{k, \ell}(\bbF) $.
	Consider the matrix
	\[
		M = \begin{pmatrix}
			A & C \\
			0 & B
		\end{pmatrix},\quad M\in \mathcal{M}_{n}(\mathbb{F}),\quad n = k+\ell
	\]
	Then $ \det M = \det A \det B $.
\end{lemma}
\begin{proof}
	Let $ M = (m_{ij}) $.
	We need to compute
	\[
		\det M = \sum_{\sigma \in S_n} \varepsilon(\sigma) \prod_{i=1}^n m_{\sigma(i) i}
	\]
	Observe that $ m_{\sigma(i) i} = 0 $ if $ i \leq k $ and $ \sigma(i) > k $.
	Thus, we need only sum over $ \sigma\in S_n $ such that 
	\begin{enumerate}
		\item $ \forall j\in [1,k],\ \sigma(j)\in [1,k] $,
		\item $ \forall j\in [k+1, n], \ \sigma(j)\in [k+1,n] $. 
	\end{enumerate}
	In other words, we may restrict the sum to $ \sigma $ of the form 
	\begin{align*}
		& \sigma_1 : \{1,\dots,k\} \to \{1,\dots,k\} \equiv \sigma_{1 \{1,\dots,k\}}\\ 
		& \sigma_2 : \{k+1,\dots,n\} \to \{k+1,\dots,n\} \equiv \sigma_{1 \{k+1,\dots,n\}}
	\end{align*}
	Hence,
	\begin{align*}
		\det M & = \sum_{\sigma \in S_n} \varepsilon(\sigma) \prod_{i=1}^n m_{\sigma(i) i}                                                         \\
		       & = \sum_{\sigma_1 \in S_k} \sum_{\sigma_2 \in S_{n-k}} \varepsilon(\sigma_1) \varepsilon(\sigma_2) \prod_{i=1}^k m_{\sigma_1(i) i} \prod_{i=k+1}^n m_{\sigma_2(i) i} \\
		       & = \sum_{\sigma_1 \in S_k} \varepsilon(\sigma_1) \prod_{i=1}^k m_{\sigma(i) i} \sum_{\sigma_2 \in S_{n-k}} \varepsilon(\sigma_2) \prod_{i=k+1}^n m_{\sigma(i) i} \\
		       & = \det A \det B\qedhere
	\end{align*}
\end{proof}

\begin{corollary}
	Let $ A_1,A_2,\dots,A_k $ be square matrices. Then 
	\[
		\det \begin{pmatrix}
			A_1 &  &  &  * \\
			 & A_2 &  &   \\
			 &  & \ddots &   \\
			0 &  &  &  A_k \\
		\end{pmatrix}= \det A_1 \det A_2\cdots \det A_k. 
	\]
\end{corollary}
\begin{proof}
By induction. 
\end{proof}
In particular, this implies that an upper-triangular matrix with diagonal elements $ \lambda_i $ has determinant $ \prod_i \lambda_i $.
\begin{remark}
	\begin{enumerate}
		\item In general,
		\[
			\det \begin{pmatrix}
				A &  B \\
				C &  D \\
			\end{pmatrix}\neq \det A\det D - \det B \det C. 
		\]
		\item One can check that the volumn form in $ \mathbb{R}^{3} $, up to a multiplication constant, is given by 
		\begin{align*}
			& \mathbb{R}^{3} \times \mathbb{R}^{3} \times \mathbb{R}^{3} \to \mathbb{F}\\ 
			& (\mathbf{a},\mathbf{b},\mathbf{c}) \mapsto (\mathbf{a} \times \mathbf{b})\cdot \mathbf{c}
		\end{align*}
		It is indeed a volume. 
	\end{enumerate}
\end{remark}

\section{Adjugate matrix}

\subsection{Column and row expansions}
Let $ A \in \mcM_n(\bbF) $ with column vectors $ A^{(i)} $.
We know that
\[
	\det(A^{(1)}, \dots, A^{(j)}, \dots, A^{(k)}, \dots, A^{(n)}) = -\det(A^{(1)}, \dots, A^{(k)}, \dots, A^{(j)}, \dots, A^{(n)})
\]
Using the fact that $ \det A = \det A^\top $ we can similarly see that swapping two rows will invert the sign of the determinant.

\begin{remark}
	We could have proven all of the properties of the determinant above by using the decomposition of $ A $ into elementary matrices.
\end{remark}

\begin{definition}
	Let $ A \in \mcM_n(\bbF) $.
	Let $ i, j \in \qty{1, \dots, n} $.
	We define the \textbf{minor} $ A_{\widehat{ij}} \in \mcM_{n-1}(\bbF) $ to be the matrix obtained by removing the $ i $th row and the $ j $th column.
\end{definition}

\begin{example}
	$ A = \begin{pmatrix}
		1 & 2 &  -7 \\
		2 & 1 &  0 \\
		-3 & 6 &  1 \\
	\end{pmatrix}, A_{\widehat{32}} = \begin{pmatrix}
		1 &  -7 \\
		2 &  0 \\
	\end{pmatrix} $. 
\end{example}

\begin{lemma}[Expansion of the determinant]
	Let $ A \in \mcM_n(\bbF) $.
	\begin{enumerate}[(i)]
		\item Let $ j \in \qty{1, \dots, n} $.
		      The determinant of $ A $ is given by the \textit{column expansion with respect to the $ j $th column}:
		      \[
			      \det A = \sum_{i=1}^n (-1)^{i+j} a_{ij} \det A_{\widehat{ij}}
		      \]
		\item Let $ i \in \qty{1, \dots, n} $.
		      The same determinant is also given by the \textit{row expansion with respect to the $ i $th row}:
		      \[
			      \det A = \sum_{j=1}^n (-1)^{i+j} a_{ij} \det A_{\widehat{ij}}
		      \]
	\end{enumerate}
	This is a process of reducing the computation of $ n \times n $ determinants to $ (n-1) \times (n-1) $ determinants.
\end{lemma}

\begin{proof}
	We will prove case (i), the column expansion with respect to the $ j $th column.
	Then (ii) will follow from the top of the matrix.

	Pick $ j \in \qty{1, \dots, n} $.
	Write $ A^{(j)} = \sum_{i=1}^n a_{ij} \bfe_i $ wrt the standard basis. Expand $A^{(j)}$ in the determinant gives
	\begin{align*}
		\det A & = \det\qty(A^{(1)}, \dots, \sum_{i=1}^n a_{ij} \bfe_i, \dots, A^{(n)})                                    \\
		       & = \sum_{i=1}^n a_{ij} \det\qty(A^{(1)}, \dots, \bfe_i, \dots, A^{(n)})                                    \\
		       & = \sum_{i=1}^n a_{ij} (-1)^{j-1} \det\qty(\bfe_i, A^{(1)}, \dots, A^{(n)})                                \\
		       & = \sum_{i=1}^n a_{ij} (-1)^{j-1} (-1)^{i-1} \det\qty(\bfe_1, \overline A^{(1)}, \dots, \overline A^{(n)})
	\end{align*}
	where $ \overline{A}^{(j)} $ are the columns with the $i$th row swapped with the first row. 

	The matrix now becomes 
	\[
		\begin{pmatrix}
			1 & a_{i 1} & \cdots & a_{i j-1} &  a_{i j+1} & \cdots & a_{i n} \\
			0 &  &  &  &  &  &  \\
			\vdots &  &  & A_{\widehat{ij}} &  &  & \\
			\vdots &  &  &  &  &  & \\
			0 &  &  &  &  &  & \\
		\end{pmatrix}
	\]
	Hence,
	\[
		\det A = \sum_{i=1}^n (-1)^{i+j} a_{ij} \det A_{\widehat{ij}}
	\]
	as required.
\end{proof}
We have proven that
\[
	\det (A^{(1)}, \dots, e_i, \dots, A^{(n)}) = (-1)^{i+j} \det A_{\widehat{ij}}
\]

\subsection{Adjugates}\ \vspace{-1.5em}
\begin{definition}
	Let $ A \in \mcM_n(\bbF) $.
	The \textbf{adjugate matrix} of $ A $, denoted $ \adj A $, is the $ n \times n $ matrix given by
	\[
		(\adj A)_{ij} = (-1)^{i+j} \det A_{\widehat{ji}}
	\]
	Hence,
	\[
		\det (A^{(1)}, \dots, A^{(j-1)}, \bfe_i, , A^{(j+1)},\dots, A^{(n)}) = (\adj A)_{ji}.
	\]
\end{definition}

\begin{theorem}
	Let $ A \in \mathcal{M}_n (\mathbb{F}) $. Then $ \adj(A) A = (\det A) I $. 
	
	In particular, when $ A $ is invertible,
	\[
		A^{-1} = \frac{\adj A}{\det A}
	\]
\end{theorem}
\begin{proof}
	We just proved:
	\begin{align*}
		\det A &= \sum_{i=1}^n (-1)^{i+j} a_{ij} \det A_{\widehat{ij}} \\
		&= \sum_{i=1}^n (\adj A)_{ji} a_{ij} = ((\adj A) A)_{jj}
	\end{align*}
	So the diagonal terms match.
	
	Off the diagonal,
	\[
		0 = \det(A^{(1)}, \dots, \underbrace{A^{(k)}}_{\mathclap{j\text{th position}}}, \dots, A^{(k)}, \dots, A^{(n)})
	\]
	By linearity,
	\begin{align*}
		0 & = \det\qty(A^{(1)}, \dots, \underbrace{\sum_{i=1}^n a_{ik} \bfe_i}_{\mathclap{j\text{th position}}}, \dots, A^{(k)}, \dots, A^{(n)}) \\
		  & = \sum_{i=1}^n a_{ik} \det\qty(A^{(1)}, \dots, \underbrace{\bfe_i}_{\mathclap{j\text{th position}}}, \dots, A^{(k)}, \dots, A^{(n)}) \\
		  & = \sum_{i=1}^n a_{ik} (\adj A)_{ji}                                                                                               \\
		  & = ((\adj A) A)_{jk}. \qedhere
	\end{align*}
\end{proof}

\subsection{Cramer's rule}\ \vspace{-1.5em}
\begin{proposition}[Cramer's rule]
	Let $ A\in \mathcal{M}_n(\mathbb{F}) $ be invertible. 
	Let $ \bfb \in \bbF^n $.
	Then the unique solution to $ A\bfx = \bfb $ is given by
	\[
		x_i = \frac{1}{\det A} \det (A_{\hat{i}b}),\quad 1\le i\le n
	\]
	where $ A_{\hat{i}b} $ is obtained by replacing the $ i$th column of $ A $ by $ b $.
	Algorithmically, this avoids computation of $A^{-1}$. 
\end{proposition}

\begin{proof}
	Let $ A $ be invertible.
	Then there exists a unique $ \bfx \in \bbF^n $ such that $A \bfx = \bfb $.
	Then
	\begin{align*}
		\det (A_{\hat{i}b}) & = \det(A^{(1)}, \dots, A^{(i-1)}, \bfb, A^{(i+1)}, \dots, A^{(n)})                            \\
		                        & = \det\qty(A^{(1)}, \dots, A^{(i-1)}, \sum_{j=1}^n x_j A^{(j)}, A^{(i+1)}, \dots, A^{(n)}) \\
		                        & = \det\qty(A^{(1)}, \dots, A^{(i-1)}, x_i A^{(i)}, A^{(i+1)}, \dots, A^{(n)})              \\
		                        & = x_i \det A
	\end{align*}
	The result follows.
\end{proof}

\section{Eigenvectors, eigenvalues, and trigonal matrices}
\subsection{Eigenvectors and eigenvalues}
Let $ V $ be an $ \bbF $-vector space with $ \dim V = n < \infty $, and let $ \alpha $ be an endomorphism of $ V $.
We wish to find a basis $ B $ of $ V $ such that, in this basis, $ [\alpha]_B \equiv [\alpha]_{B,B} $ has a simple (e.g. diagonal, triangular) form.

Recall that if $ B' $ is another basis and $ P $ is the change of basis matrix, $ [\alpha]_{B'} = P^{-1} [\alpha]_B P $.

Equivalently, given a square matrix $ A \in \mcM_n(\bbF) $ we want to conjugate it to a matrix with a ``simple'' form. 

\begin{definition}
	\begin{enumerate}[(i)]
		\item $ \alpha\in \mcL(V) $ is \textbf{diagonalisable} if there exists a basis $B$ of $V$ such that $ [\alpha]_{B} $ is diagonal. 
		\[
			[\alpha]_B = \begin{pmatrix}
				\lambda_1 & \cdots &  0 \\
				\vdots  & \ddots  &  \vdots  \\
				0 & \cdots  &  \lambda_n \\
			\end{pmatrix}
		\]
		\item $ \alpha\in \mcL(V) $ is \textbf{triangulable} if there exists a basis $B$ of $V$ wuch that $ [\alpha]_B $ is triangular.
		\[
			[\alpha]_B = \begin{pmatrix}
				\lambda_1 & \cdots &  * \\
				\vdots  & \ddots  &  \vdots  \\
				0 & \cdots  &  \lambda_n \\
			\end{pmatrix}
		\]
	\end{enumerate}
\end{definition}

\begin{remark}
	A matrix is diagonalisable(triangulable) if and only if it is conjugate to a diagonal(triangular) matrix. 
\end{remark}

\begin{definition}
	\begin{enumerate}[(i)]
		\item $ \lambda\in \mathbb{F} $ is an \textbf{eigenvalue} of $ \alpha\in \mcL(V) $ if and only if: 
		\[
			\exists \mathbf{v}\in V \setminus \{0\}\quad \alpha(\mathbf{v}) = \lambda \mathbf{v}
		\]
		\item $ \mathbf{v}\in V $ is an \textbf{eigenvector} of $\alpha$ if and only if: 
		\[
			\mathbf{v}\neq 0\quad \text{and}\quad \exists \lambda\in \mathbb{F}\quad \alpha(\mathbf{v}) = \lambda \mathbf{v}
		\]
		\item $ V_{\lambda} = \{\mathbf{v}\in V: \alpha(\mathbf{v}) = \lambda \mathbf{v}\} $ is the \textbf{eigenspace} associated to $\lambda$. 
	\end{enumerate}
\end{definition}

\begin{lemma}
	$ \lambda $ is an eigenvalue if and only if $ \det(\alpha - \lambda I) = 0 $.
\end{lemma}
\begin{proof}
	If $ \lambda $ is an eigenvalue, there exists a nonzero vector $ \bfv $ such that $ \alpha(\bfv) = \lambda \bfv $, so $ (\alpha - \lambda)(\bfv) = 0 $.
	So the kernel is non-trivial.
	So $ \alpha - \lambda I $ is not injective, so it is not surjective by the rank-nullity theorem.
	Hence this matrix is not invertible, so it has zero determinant.
\end{proof}

\begin{remark}
	If $ \alpha(\bfv_j) = \lambda \bfv_j $ for $ j \in \qty{1, \dots, m} $, we can complete the family $ \bfv_j $ into a basis $ (\bfv_1, \dots, \bfv_n) $ of $ V $.
	Then in this basis, the first $ m $ columns of the matrix $ \alpha $ has diagonal entries $ \lambda_j $.
\end{remark}
\subsection{Polynomials}
Recall the following facts about polynomials on a field, for instance
\[
	f(t) = a_n t^n + \dots + a_1 t + a_0
\]
We say that the degree of $ f $, written $ \deg f $ is $ n $.
The degree of $ f + g $ is at most the maximum degree of $ f $ and $ g $.
$ \deg (fg) = \deg f + \deg g $.

Let $ \bbF[t] $ be the vector space of polynomials with coefficients in $ F $.
If $ \lambda $ is a root of $ f $, then $ (t-\lambda) $ divides $ F $. Indeed, $ f(\lambda) = a_n \lambda^n + \dots + a_1 \lambda + a_0 = 0 $,
which implies that
\[
	f(t) = f(t) - f(\lambda) = a_n(t^n - \lambda^n) + \dots + a_1(t - \lambda)
\]
But note that, for all $ n $,
\[
	t^n - \lambda^n = (1-\lambda)(t^{n-1} + \lambda t^{n-2} + \dots + \lambda^{n-2} t + \lambda^{n-1}).\qed
\]

\begin{remark}
	We say that $ \lambda $ is a root of \textit{multiplicity} $ k $ if $ (t-\lambda)^k $ divides $ f $ but $ (t-\lambda)^{k+1} $ does not.
\end{remark}

\begin{corollary}
	A nonzero polynomial of degree $ n $ has at most $ n $ roots, counted with multiplicity.
\end{corollary}
\begin{corollary}
	If $ f_1, f_2 $ are two polynomials of degree less than $ n $ such that $ f_1(t_i) = f_2(t_i) $ for $ i \in \qty{1, \dots, n} $ and $ t_i $ distinct, then $ f_1 \equiv f_2 $.
\end{corollary}
\begin{proof}
	$ f_1 - f_2 $ has degree less than $ n $, but has $ n $ roots.
	Hence it is zero.
\end{proof}

\begin{theorem}[Fundamental Theorem of Algebra]
	Any polynomial $ f \in \mathbb C[t] $ of positive degree has a complex root.
	When counted with multiplicity, $ f $ has exactly $ \deg f $ roots when counted with multiplicity. 
\end{theorem}
\begin{corollary}
	Any polynomial $ f \in \mathbb C[t] $ can be factorised into an amount of linear factors equal to its degree.
\end{corollary}

\subsection{Characteristic polynomials}\ \vspace{-1.5em}
\begin{definition}
	Let $ \alpha $ be an endomorphism.
	The \textbf{characteristic polynomial} of $ \alpha $ is
	\[
		\chi_\alpha(\lambda) = \det(\alpha - \lambda I)
	\]
\end{definition}

\begin{remark}
	\begin{enumerate}
		\item $ \det (A-\lambda I) $ is a polynomial of $\lambda$ from definition. 
		\item Conjugate matrices have the samce characteristic polynomial. 
		
		Indeed, $ \det( P^{-1}AP - \lambda I) = \det ( P^{-1}(A-\lambda I)P) = \det(A - \lambda I) $.
	\end{enumerate}
\end{remark}

\begin{theorem}\label{thm:test if triangulable}
	Let $ \alpha \in \mcL(V) $.
	$ \alpha $ is triangulable if and only if $ \chi_\alpha $ can be written as a product of linear factors over $ \bbF $ 
	\[
		\chi_\alpha(t) = c \prod_{i=1}^{n}(t-\lambda I)
	\]
	In particular, all complex matrices are triangulable.
\end{theorem}
\begin{proof}
	($\Rightarrow$) Suppose $\alpha$ is triangulable. Then 
	\begin{align*}
		& [\alpha]_B = \begin{pmatrix}
			a_1 & \cdots &  * \\
			\vdots  & \ddots  &  \vdots  \\
			0 & \cdots  &  a_n \\
		\end{pmatrix}\\ 
		\implies & \chi_\alpha(t) = \begin{pmatrix}
			a_1-t & \cdots &  * \\
			\vdots  & \ddots  &  \vdots  \\
			0 & \cdots  &  a_n-t \\
		\end{pmatrix} = \prod_{i=1}^{n} (a_i - t)
	\end{align*}

	$(\Leftarrow)$ We argue by induction on $ \dim V = n $ 
	\begin{itemize}
		\item $n=1$, $ \chi_\alpha(t) $ is obviously linear.
		\item Suppose it is true for $n=k>1$. By assumption, $ \chi_\alpha $ has a root $ \lambda $, i.e. $\alpha$ has an eigenvalue $\lambda$. 
		\item Let $U = V_\lambda$ be the corresponding eigenspace for $\lambda$. 
		\item Let $ \{\mathbf{v}_1,\dots,\mathbf{v}_k\} $ be a basis of $U$, and extend to a basis $ \{\mathbf{v}_1,\dots,\mathbf{v}_k,\mathbf{v}_{k+1},\dots,\mathbf{v}_n\} $ of $V$. 
		\item Let $ W = \spn\qty{\bfv_{k+1}, \dots, \bfv_n} $, and then $ V = V_\lambda \oplus W $.
		\item Then
		\[
			[\alpha]_B = \begin{pmatrix}
				\lambda I & * \\
				0         & C
			\end{pmatrix}
		\]
		where $ * $ is arbitrary, and $ C $ is a block of size $ (n-k) \times (n-k) $.
		\item Then $ \alpha $ induces an endomorphism $ \overline \alpha \colon V/U \to V/U $ with respect to the basis $ (\bfv_{k+1}, \dots, \bfv_n) $, where $ U = V_\lambda $.
		\item By induction on the dimension, we can find a basis $ (\bfw_{k+1}, \dots, \bfw_n) $ for which $ C $ has a triangular form.
		Then the basis $ (\bfv_1, \dots, \mathbf{v}_k, \bfw_{k+1}, \dots, \bfw_n) $ is a basis for which $ \alpha $ is triangular.
	\end{itemize}
\end{proof}

\begin{lemma}
	Let $ n = \dim V $, and $ V $ be a vector space over $ \mathbb R $ or $ \mathbb C $.
	Let $ \alpha $ be an endomorphism on $ V $.
	Then
	\[
		\chi_\alpha(t) = (-1)^n t^n + c_{n-1} t^{n-1} + \dots + c_0
	\]
	with
	\[
		c_0 = \det A;\quad c_{n-1} = (-1)^{n-1} \tr A
	\]
\end{lemma}
\begin{proof}
	\[
		\chi_\alpha(t) = \det(\alpha - t I) \implies \chi_\alpha(0) = \det(\alpha)
	\]
	Further, for $ \mathbb R, \mathbb C $ we know that $ \alpha $ is triangulable over $ \mathbb C $.
	Hence $ \chi_\alpha(t) $ is the determinant of a triangular matrix;
	\[
		\chi_\alpha(t) = \prod_{i=1}^n (a_i - t)
	\]
	Hence
	\[
		c_{n-1} = (-1)^{n-1} a_i
	\]
	Since the trace is invariant under a change of basis, this is exactly the trace as required.
\end{proof}


\subsection{Diagonalisation}
Let $ p(t) $ be a polynomial over $ \bbF $.
We will write
\[
	p(t) = a_n t^n + \dots + a_0
\]
For a matrix $ A \in \mcM_n(\bbF) $, we write
\[
	p(A) = a_n A^n + \dots + a_0 \in \mcM_n(\bbF)
\]
For an endomorphism $ \alpha \in \mcL(V) $,
\[
	p(\alpha) = a_n \alpha^n + \dots + a_0 I \in \mcL(V);\quad \alpha^k \equiv \underbrace{\alpha \circ \dots \circ \alpha}_{k \text{ times}}
\]

\begin{theorem}[Sharp criterion of diagonalisability]
	Let $V$ be a finite-dimensional vector space over $\mathbb{F}$. Let $\alpha\in \mcL(V)$. Then $\alpha$ is diagonalisable if and only if there exists a polynomial $ p $ which is a product of \textit{distinct} linear factors, such that $ p(\alpha) = 0 $.
	In other words, there exist distinct $ \lambda_1, \dots, \lambda_k $ such that
	\[
		p(t) = \prod_{i=1}^n (t - \lambda_i) \implies p(\alpha) = 0
	\]
\end{theorem}

\begin{proof}
	($ \Rightarrow $) Suppose that $\alpha$ is diagonalisable with distinct eigenvalues $ \lambda_1,\dots \lambda_k $. Let
	\[
		p(t)= \prod_{i=1}^k (t - \lambda_i). 
	\]
	Let $ B $ be the basis of eigenvectors of $V$ and let $ \mathbf{v}\in B $. Then $\alpha(\mathbf{v}) = \lambda_i \mathbf{v}$ for some $i$, so 
	\[
		(\alpha - \lambda_i I) \mathbf{v} = 0 \implies p(\alpha)(\mathbf{v}) = \prod_{i=1}^k (\alpha - \lambda_i I) (\mathbf{v}) = 0. 
	\]
	Since this holds for all $\mathbf{v}\in B$, $ p(\alpha) = 0. $

	($ \Leftarrow $) Suppose $ p(\alpha) = 0 $ for some polynomial 
	\[
		p(t) = \prod_{i=1}^k (t - \lambda_i), \quad \lambda_i\neq \lambda_j,\ \forall i\neq j. 
	\]
	Let $ V_{\lambda_i} = \ker(\alpha - \lambda_i I) $. 
	\begin{claim}
		$\displaystyle V = \bigoplus_{i=1}^{k} V_{\lambda_i}$. 
	\end{claim}
	Indeed, let 
	\[
		q_{j}(t) = \prod_{\substack{i = 1 \\ i\neq j}}^{k} \frac{(t-\lambda_i)}{\lambda_j-\lambda_i}\quad 1\le j\le k,
	\]
	then 
	\[
		q_j (\lambda_i) = \delta_{ij}. 
	\]
	Define 
	\[
		q(t) = q_{1}(t) + q_2(t) + \cdots + q_k(t). 
	\]
	Then $q(t)$ has degree at most $k-1$ and 
	\begin{align*}
		q(\lambda_i) = 1,\ \forall 1\le i\le k &\implies \forall t,\ q(t) = 1\\ 
		& \implies q_1(t)+\cdots + q_k(t) = 1, \ \forall t. 
	\end{align*}
	Let $ \pi_j = q_j ( \alpha)\in \mcL(V) $. These are called the `projection operators'. By construction, 
	\[
		\sum_{j=1}^{k} \pi_j(\alpha) = q(\alpha) = I. 
	\]
	This means that for all $ \mathbf{v}\in V $, 
	\[
		\mathbf{v} = q(\alpha) (\mathbf{v}) = \sum_{j=1}^{k} \pi_j(\mathbf{v}) = \sum_{j=1}^{k} q_j(\alpha)(\mathbf{v}) =\sum_{j=1}^{k} \pi_j(\mathbf{v}) . 
	\]
	Observe that 
	\begin{align*}
		(\alpha - \lambda_j I) q_j(\alpha)(\bfv) & = \frac{1}{\prod_{i \neq j} (\lambda_j - \lambda_i)} (\alpha - \lambda_j I) \qty[\prod_{i \neq j} (\alpha - \lambda_i)] (\mathbf{v}) \\
		                                      & = \frac{1}{\prod_{i \neq j} (\lambda_j - \lambda_i)} \prod_{i=1}^k (\alpha - \lambda_i I)(\bfv)                                \\
		                                      & = \frac{1}{\prod_{i \neq j} (\lambda_j - \lambda_i)} p(\alpha)(\bfv) = 0
	\end{align*}
	Since $ q_j(\alpha) = \pi_j, $
	\[
		(\alpha - \lambda_j I) \pi_j(\bfv) = 0 \implies \pi_j(\bfv) \in \ker(\alpha - \lambda_j I) = V_{\lambda_j}
	\]
	and this completes the proof that 
	\[
		V = \sum_{j=1}^{k} V_{\lambda_j}. 
	\]
	It remains to prove that the sum is direct. Let 
	\[
		\mathbf{v} \in V_{\lambda_j} \cap \left( \sum_{j\neq j} V_{\lambda_i} \right),
	\]
	applying $\pi_j$ to $\mathbf{v}$ gives 
	\begin{align*}
		\mathbf{v}\in V_{\lambda_j} &\implies \pi_j(\mathbf{v}) = \prod_{i\neq j} \frac{\lambda_j-\lambda_i}{\lambda_j-\lambda_i}\mathbf{v} = \mathbf{v}\\ 
		\mathbf{v}\in \sum_{j\neq j} V_{\lambda_i} &\implies \pi_j(\mathbf{w}) = 0 \text{ for } \mathbf{w} \in V_{\lambda_i},\ i\neq j\\ 
		&\implies \pi_j(\mathbf{v}) = 0. 
	\end{align*}
	This implies $ \mathbf{v} = \pi_j(\mathbf{v}) = 0 $ and thus the sum is distinct. 

	Hence, $ B = (B_1, \dots, B_k) $ is a basis of $ V $, where the $ B_i $ are bases of $ V_{\lambda_i} $.
	Then $ [\alpha]_B $ is diagonal, and this completes the proof. 
\end{proof} 

\begin{remark}
	We have shown further that if $ \lambda_1, \dots, \lambda_k $ are distinct eigenvalues of $ \alpha $, then
	\[
		\sum_{i=1}^k V_{\lambda_i} = \bigoplus_{i=1}^k V_{\lambda_i}
	\]
	Therefore, the only way that diagonalisation fails is when this sum is not direct, i.e.
	\[
		\sum_{i=1}^k V_{\lambda_i} \le V
	\]
\end{remark}

\begin{example}
	Let $ \bbF = \mathbb C $.
	Let $ A \in \mcM_n(\bbF) $ such that $ A $ has finite order; there exists $ m \in \mathbb N $ such that $ A^m = I $.
	Then $ A $ is diagonalisable.
	This is because
	\[
		t^m - 1 = p(t) = \prod_{j=1}^m (t - \xi_m^j);\quad \xi_m = e^{2 \pi i/m}
	\]
	and $ p(A) = 0 $.
\end{example}

\begin{theorem}[Simultaneous diagonalisation]
	Let $ \alpha, \beta \in \mcL(V)$ that are diagonalisable.
	Then $ \alpha, \beta $ are \textbf{simultaneously diagonalisable} (there exists a basis $ B $ of $ V $ such that $ [\alpha]_B, [\beta]_B $ are diagonal) if and only if $ \alpha $ and $ \beta $ commute.
\end{theorem}
\begin{proof}
	($ \Rightarrow $)
	Suppose there exists a basis $B$ such that 
	\[
		[\alpha]_{B} = D_1,\ [\beta]_B = D_2,\quad D_1,D_2 \text{ diagonal}, 
	\]
	then $ D_1D_2 = D_2D_1 $, i.e. $ \alpha \beta = \beta \alpha $. 

	($ \Leftarrow $)
	Suppose $\alpha,\beta$ are diagonalisable and commute. We have
	\[
		V = \bigoplus_{i=1}^k V_{\lambda_i},\quad \lambda_i, \dots, \lambda_k \text{ distinct eigenvalues of } \alpha. 
	\]
	\begin{claim}
		$ \beta(V_{\lambda_j}) \le V_{\lambda_j} $. 
	\end{claim}
	Indeed, for $ v \in V_{\lambda_j} $,
	\[
		\alpha \beta(v) = \beta \alpha(v) = \beta(\lambda_j v) = \lambda_j \beta(v) \implies \alpha(\beta(v)) = \lambda_j \beta(v)
	\]
	Hence, $ \beta(v) \in V_{\lambda_j} $. 

	By assumption, $ \beta $ is diagonalisable, so there exists a polynomial $ p $ with distinct linear factors such that $ p(\beta) = 0 $. This implies that
	\[
		p(\beta \restriction_{V_{\lambda_i}}) = 0 \implies \beta \restriction_{V_{\lambda_i}}\in \mcL(V_{\lambda_i}) \text{ is diagonalisable.}
	\]
	Let $ B_i $ be the basis of $ V_{\lambda_i} $ in which $ \beta\restriction_{V_{\lambda_j}} $ is diagonal.
	Since $ V = \bigoplus V_{\lambda_i} $, $ B = (B_1, \dots, B_k) $ is a basis of $ V $.
	Then the matrices of $ \alpha $ and $ \beta $ in $ V $ are diagonal.
\end{proof}

\subsection{Minimal polynomials}
{\color{blue}Recall from IB Groups, Rings and Modules} (\textit{Wow good job Professor Raphael}) the Euclidean algorithm for dividing polynomials.
Given $ a, b $ polynomials over $ \bbF $ with $ b $ nonzero, there exist polynomials $ q, r $ over $ \bbF $ with $ \deg r < \deg b $ and $ a = qb + r $.
\begin{definition}
	Let $ V $ be a finite dimensional $ \bbF $-vector space.
	Let $ \alpha $ be an endomorphism on $ V $.
	The \textbf{minimal polynomial} $ m_\alpha $ of $ \alpha $ is the nonzero polynomial with smallest degree such that $ m_\alpha(\alpha) = 0 $.
\end{definition}
\begin{remark}
	If $ \dim V = n < \infty $, then $ \dim \mcL(V) = n^2 $.
	In particular, the family $ \qty{I, \alpha, \dots, \alpha^{n^2}} $ cannot be linearly independent since it has $ n^2+1 $ entries.
	This generates a polynomial in $ \alpha $ which evaluates to zero.
	Hence, a minimal polynomial always exists.
\end{remark}
\begin{lemma}
	Let $ \alpha \in \mcL(V) $ and $ p \in \bbF[t] $ be a polynomial.
	Then $ p(\alpha) = 0 $ if and only if $ m_\alpha $ is a factor of $ p $.
	In particular, $ m_\alpha $ is well-defined and unique up to a constant multiple.
\end{lemma}
\begin{proof}
	Let $ p \in \bbF[t] $ such that $ p(\alpha) = 0 $.
	If $ m_\alpha(\alpha) = 0 $ and $ \deg m_\alpha < \deg p $, we can perform the division $ p = m_\alpha q + r $ for $ \deg r < \deg m_\alpha $.
	Then $ p(\alpha) = m_\alpha(\alpha) q(\alpha) + r(\alpha) $.
	But $ m_\alpha(\alpha) = 0 $.
	But $ \deg r < \deg m_\alpha $ and $ m_\alpha $ is the smallest degree polynomial which evaluates to zero for $ \alpha $, so $ r \equiv 0 $ so $ p = m_\alpha q $.
	In particular, if $ m_1, m_2 $ are both minimal polynomials that evaluate to zero for $ \alpha $, we have $ m_1 $ divides $ m_2 $ and $ m_2 $ divides $ m_1 $.
	Hence they are equivalent up to a constant.
\end{proof}
\begin{example}
	Let $ V = \bbF^2 $ and
	\[
		A= \begin{pmatrix}
			1 & 0 \\
			0 & 1
		\end{pmatrix};\quad B = \begin{pmatrix}
			1 & 1 \\
			0 & 1
		\end{pmatrix}
	\]
	We can check $ p(t) = (t-1)^2 $ gives $ p(A) = p(B) = 0 $.
	So the minimal polynomial of $ A $ or $ B $ must be either $ (t-1) $ or $ (t-1)^2 $.
	For $ A $, we can find the minimal polynomial is $ (t-1) $, and for $ B $ we require $ (t-1)^2 $.
	So $ B $ is not diagonalisable, since its minimal polynomial is not a product of distinct linear factors.
\end{example}

\subsection{Cayley-Hamilton Theorem}
For $ \alpha\in \mcL(V), p\in \mathbb{F}[t] $, recall that the minimal polynomial satisfies $ p(\alpha) = 0 \iff m_\alpha | p $. 

\begin{theorem}[Cayley-Hamilton]
	Let $V$ be an $\mathbb{F}$-vector space with finite dimension. Let $\alpha\in \mcL(V)$ with characteristic polynomial $ \chi_\alpha(t) = \det (\alpha - t I). $
	Then $ \chi_\alpha(\alpha) = 0 $. 
\end{theorem}
\begin{corollary}
	$ m_\alpha | \chi_\alpha $. We now have a way to find minimal polynomials.
\end{corollary}
We give two proofs. 
\begin{proof}[Proof 1]
	We first restrict to $ \mathbb{F} = \mathbb{C} $. 
	
	Let $ B = \qty{\bfv_1, \dots, \bfv_n} $ be a basis of $ V $ such that $ [\alpha]_B $ is triangular. This can be done by theorem \ref{thm:test if triangulable}. If the diagonal entries in this basis are $ a_i $,
	\[
		\chi_\alpha(t) = \prod_{i=1}^n (a_i - t) \implies \chi_\alpha(\alpha) = (\alpha - a_1 I) \dots (\alpha - a_n I)
	\]
	We want to show that this expansion evaluates to zero.
	Let $ U_j = \spn \qty{\bfv_1, \dots, \bfv_j} $.
	Let $ \bfv \in V = U_n $.
	We want to compute $ \chi_\alpha(\alpha)(\bfv) $.
	Note, by construction of the triangular matrix,
	\begin{align*}
		\chi_\alpha(\alpha)(\bfv) & = (\alpha - a_1 I) \dots \underbrace{(\alpha - a_n I)(\bfv)}_{\in U_{n-1}}                     \\
		                       & = (\alpha - a_1 I) \dots \underbrace{(\alpha - a_{n-1} I)(\alpha - a_n I)(\bfv)}_{\in U_{n-2}} \\
		                       & = \dots                                                                                     \\
		                       & \in U_0 = 0. 
	\end{align*}
	Hence this evaluates to zero.
\end{proof}

\begin{proof}[Proof 2]
	Now we prove this on any field $\mathbb{F}$. 

	Let $ A\in \mathcal{M}_n(\mathbb{F}) $ be a matrix of $\alpha$. Note that 
	\[
		(-1)^n \chi_A(t) = \det (t I - A) = t^n + a_{n-1} t^{n-1} + \cdots + a_1 t + a_0.
 	\]
	Recall that for any matrix $B$, $ B \adj B = (\det B) I. $ We apply this to $B = t I - A$. We can check that
	\[
		\adj B = \adj(tI - A) = B_{n-1} t^{n-1} + \dots + B_1 t + B_0
	\]
	since adjugate matrices are degree $ (n-1) $ polynomials for each element.
	Then, by applying $ B \adj B = (\det B) I $,
	\[
		(tI - A) [ B_{n-1} t^{n-1} + \dots + B_1 t + B_0 ] = (\det B) I = (t^n + \dots + a_0) I
	\]
	Since this is true for all $ t $, we can equate coefficients.
	This gives
	\begin{align*}
		t^n     & :       I          = B_{n-1}            \\
		t^{n-1} & :       a_{n-1} I  = B_{n-2} - AB_{n-1} \\
		        & \vdots                           \\
		t^0     & :       a_0 I      = -A B_1
	\end{align*}
	Then, substituting $ A $ for $ t $ in each relation will give
	\begin{align*}
		A^n & \times (I = B_{n-1}) \\ 
		A^{n-1} & \times (a_{n-1} I = B _{n-2} - AB_{n-1}) \\ 
		A^{n-2} & \times (a_{n-2} I = B_{n-3} - AB_{n-2})\\
		& \vdots \\ 
		A^{0} & \times (a_0 I = - AB_0)
	\end{align*}
	Summing up the above equations, RHS forms a telescoping series and cancels out to $0$, giving
	\[
		A^n + a_{n-1} A^{n-1} + \dots + a_0 I = 0 \implies \chi_A(A) = 0
	\]
	as required. 
\end{proof}

\subsection{Algebraic and geometric multiplicity}
We look closer at eigenvalues on how the behaviours will effect diagonalisation. 
\begin{definition}
	Let $ V $ be a finite dimensional $ \bbF $-vector space.
	Let $ \alpha \in \mcL(V) $ and let $ \lambda $ be an eigenvalue of $ \alpha $.
	Then
	\[
		\chi_\alpha(t) = (t-\lambda)^{a_\lambda} q(t)
	\]
	where $ q(t) $ is a polynomial over $ \bbF $ such that $ (t-\lambda) $ does not divide $ q $.

	$ a_\lambda $ is known as the \textbf{algebraic multiplicity} of the eigenvalue $ \lambda $.
	
	We define the \textbf{geometric multiplicity} $ g_\lambda $ of $ \lambda $ to be the dimension of the eigenspace associated with $ \lambda $, i.e. $ g_\lambda = \dim \ker (\alpha - \lambda I) $.
\end{definition}

\begin{remark}
	$ \lambda $ is an eigenvalue $\iff$ $\alpha-\lambda I$ is singular $\iff$ $\chi_a(\lambda) = \det (\alpha - \lambda I ) = 0$.  
\end{remark}

\begin{lemma}
	If \( \lambda \) is an eigenvalue of \( \alpha \in \mcL(V) \), then \( 1 \leq g_\lambda \leq a_\lambda \).
\end{lemma}
\begin{proof}
	We have \( g_\lambda = \dim \ker (\alpha - \lambda I) \).
	There exists a nontrivial vector \( \bfv \in V \) such that \( \bfv \in \ker(\alpha - \lambda I) \) since \( \lambda \) is an eigenvalue.
	Hence \( g_\lambda \geq 1 \).

	We now show that \( g_\lambda \leq a_\lambda \).
	Indeed, let \( \{\bfv_1, \dots, \bfv_{g_\lambda}\} \) be a basis of \( V_\lambda \equiv \ker (\alpha - \lambda I) \).
	We complete this into a basis \( B \equiv \qty(\bfv_1, \dots, \bfv_{g_\lambda}, \bfv_{g_\lambda + 1}, \dots, \bfv_n) \) of \( V \).
	Then note that
	\[
		[\alpha]_B = \begin{pmatrix}
			\lambda I_{g_\lambda} & * \\
			0                     & A_1
		\end{pmatrix}
	\]
	for some matrix \( A_1 \).
	Now,
	\[
		\det (\alpha - tI) = \det \begin{pmatrix}
			(\lambda - t) I_{g_\lambda} & *     \\
			0                           & A_1 - t I
		\end{pmatrix}
	\]
	By the formula for determinants of block matrices with a zero block on the off diagonal,
	\[
		\det (\alpha - tI) = (\lambda-t)^{g_\lambda} \det(A_1 - t I)
	\]
	Hence \( g_\lambda \leq a_\lambda \) since the determinant is a polynomial that could have more factors of the same form.
\end{proof}

\begin{lemma}
	Let \( V \) be a finite-dimensional \( \bbF \)-vector space.
	Let \( \alpha \in \mcL(V) \) and let \( \lambda \) be an eigenvalue of \( \alpha \).
	Let \( c_\lambda \) be the multiplicity of \( \lambda \) as a root of the \textit{minimal polynomial} of \( \alpha \).
	Then \( 1 \leq c_\lambda \leq a_\lambda \).
\end{lemma}
\begin{proof}
	By Cayley-Hamilton theorem, $ m_\alpha | \chi_\alpha $, and hence $ c_\lambda \le a_\lambda $. It remains to show that $ c_\lambda\ge 1 $. 
	
	Indeed, since $ \lambda $ is an eigenvalue, there exists $ \mathbf{v}\neq 0 $ such that $ \alpha(\mathbf{v}) = \lambda \mathbf{v} $. For such an eigenvector, $ \alpha^n (\mathbf{v}) = \lambda^n \mathbf{v} $ for any $ n \in \mathbb{N} $. 
	It follows that for all $ p\in \mathbb{F}[t] $, $ p(\alpha)(\mathbf{v}) = p(\lambda)\mathbf{v} $. 

	Take $p = m_\alpha$, 
	\[
		m_\alpha(\alpha)(\mathbf{v}) = m_\alpha(\lambda) \mathbf{v} = 0 \implies (t-\lambda) | m_\alpha \implies c_\lambda\ge 1. \qedhere
	\]
\end{proof}

\begin{example}
	Let
	\[
		A = \begin{pmatrix}
			1 & 0 & -2 \\
			0 & 1 & 1  \\
			0 & 0 & 2
		\end{pmatrix}
	\]
	The minimal polynomial can be computed by considering the characteristic polynomial
	\[
		\chi_A(t) = (t-1)^2(t-2)
	\]
	So the minimal polynomial is either \( (t-1)^2(t-2) \) or \( (t-1)(t-2) \).
	We check \( (t-1)(t-2) \).
	\( (A - I)(A - 2I) \) can be found to be zero.
	So \( m_A(t) = (t-1)(t-2) \).
	Since this is a product of distinct linear factors, \( A \) is diagonalisable.
\end{example}

\begin{example}
	Let \( A \) be a \textbf{Jordan block} of size \( n \geq 2 \): 
	\[
		A = \begin{pmatrix}
			\lambda & 1 &  &  &  0 \\
			 & \lambda & 1  &  &   \\
			 &  & \ddots & \ddots &   \\
			 &  &  &  & 1  \\
			0 &  &  &  &  \lambda \\
		\end{pmatrix}
	\]
	Then \( g_\lambda = \ker(A-\lambda I) =1 \), \( a_\lambda = n \), and \( c_\lambda = n \).
\end{example}

\begin{example}
	Let $ A = \lambda I $, thjen $ g_\lambda = n, a_\lambda = n, c_\lambda = 1 $. 
\end{example}

\begin{lemma}[Characterisation of diagonalisable complex endomorphisms]
	Let \( \bbF = \mathbb C \).
	Let \( V \) be a finite-dimensional \( \mathbb C \)-vector space.
	Let \( \alpha \) be an endomorphism of \( V \).
	Then the following are equivalent.
	\begin{enumerate}[(i)]
		\item \( \alpha \) is diagonalisable;
		\item for all \( \lambda \) eigenvalues of \( \alpha \), we have \( a_\lambda = g_\lambda \);
		\item for all \( \lambda \) eigenvalues of \( \alpha \), \( c_\lambda = 1 \).
	\end{enumerate}
\end{lemma}

\begin{proof}
	(i) $\iff$ (iii): We have already shown this. 

	(i) $\iff$ (ii): Let \( \lambda_1, \dots, \lambda_k \) be the distinct eigenvalues of \( \alpha \).
	We have already found that \( \alpha \) is diagonalisable if and only if \( V = \bigoplus V_{\lambda_i} \).
	The sum was found to be always direct, regardless of diagonalisability.
	We will compute the dimension of \( V \) in two ways;
	\[
		n = \dim V = \deg \chi_\alpha;\quad n = \dim V = \sum_{i=1}^k a_{\lambda_i}
	\]
	since \( \chi_\alpha \) is a product of \( (t-\lambda_i) \) factors as \( \bbF = \mathbb C \).
	Since the sum is direct,
	\[
		\dim \qty(\bigoplus_{i=1}^k V_{\lambda_i}) = \sum_{i=1}^k g_{\lambda_i}
	\]
	\( \alpha \) is diagonalisable if and only if the dimensions are equal, so
	\[
		\sum_{i=1}^k g_{\lambda_i} = \sum_{i=1}^k a_{\lambda_i}
	\]
	We know that for all eigenvalues \( \lambda_i \), \( g_{\lambda_i} \leq a_{\lambda_i} \).
	Hence, \( \sum_{i=1}^k g_{\lambda_i} = \sum_{i=1}^k a_{\lambda_i} \) holds if and only if \( g_{\lambda_i} = a_{\lambda_i} \) for all \( i \).
\end{proof}

In summary, over $ \mathbb{C} $ we have 
\begin{align*}
	\chi_\alpha(t) & = (t-\lambda_1)^{a_{\lambda_1}}\cdots (t-\lambda_k)^{a_{\lambda_k}}\\ 
	m_\alpha(t) & = (t-\lambda_1)^{c_{\lambda_1}}\cdots (t-\lambda_k)^{c_{\lambda_k}}\\ 
	g_{\lambda_i} &= \dim \ker (\alpha- \lambda_i I)
\end{align*}
Note that $ m_\alpha $ must include all linear factors with degree at least 1. Indeed, if it is not the case then there is some eigenvalue $ \lambda_j $ such that $ (t-\lambda_j) $ does not divide $p$, and $ m_\alpha(\alpha)(\mathbf{v}_j)\neq 0 $, where $\mathbf{v}_j$ is an eigenvector associated with $\lambda_j$.

\section{Jordan normal form}
For this section, let \( \bbF = \mathbb C \).
\subsection{Definition}
A weaker form compared to diagonalisation, but more general. 
\begin{definition}
	Let \( A \in \bbM_n(\mathbb C) \).
	We say that \( A \) is in \textbf{Jordan normal form} if it is a block diagonal matrix
	\[
		\begin{pmatrix}
		  J_{n_1}(\lambda_1) & & & 0\\
		  & J_{n_2}(\lambda_2)\\
		  & & \ddots\\\
		  0 & & & J_{n_k} (\lambda_k)
		\end{pmatrix}
	\]
	where $k \geq 1$, $n_1, \cdots, n_k \in \bbN$ such that $n = \sum n_i$, $\lambda_1, \cdots, \lambda_k$ not necessarily distinct, and
	\[
		J_{n_i}(\lambda) =
		\begin{pmatrix}
			\lambda & 1       & 0       & \cdots & 0       \\
			0       & \lambda & 1       & \cdots & 0       \\
			0       & 0       & \lambda & \cdots & 0       \\
			\vdots  & \vdots  & \vdots  & \ddots & \vdots  \\
			0       & 0       & 0       & \cdots & \lambda
		\end{pmatrix},\quad J_{n_i}\in \mathcal{M}_{n_i\times n_i}(\mathbb{C})
	\]
	We say that \( J_{n_i}(\lambda) \in M_{n_i}(\mathbb C) \) are \textbf{Jordan blocks}.
\end{definition}
For example, it might look something like
\[\arraycolsep=1.4pt
  \begin{pmatrix}
    \lambda_1 & 1\\
    & \lambda_1 & 1\\
    & & \lambda_1 & 0\\
    & & & \lambda_2 & 0\\
    & & & & \lambda_3 & 1\\
    & & & & & \lambda_3 & 0\\
    & & & & & & \ddots & \ddots\\
    & & & & & & & \lambda_n & 1\\
    & & & & & & & & \lambda_n
  \end{pmatrix}
\]
\begin{remark}
	In three dimensions,
	\[
		A =
		\begin{pmatrix}
			\lambda & 0       & 0       \\
			0       & \lambda & 0       \\
			0       & 0       & \lambda
		\end{pmatrix}
	\]
	is in Jordan normal form, with three one-dimensional Jordan blocks with the same \( \lambda \) value.
\end{remark}

\subsection{Similarity to Jordan normal form}
We can convert \textit{any} matrix over $\mathbb{C}$ to a JNF. 
\begin{theorem}
	Any complex matrix \( A \in \bbM_n(\mathbb C) \) is similar to a matrix in Jordan normal form, which is unique up to reordering the Jordan blocks.
\end{theorem}

The proof is non-examinable.
This follows from IB Groups, Rings and Modules.

\begin{example}
	Let \( \dim V = 2 \).
	Then any matrix is similar to one of (with corresponding minimal polynomials) 
	\begin{center}
		\begin{tabular}{cccc}
		  \toprule
		  Jordan normal form & $\chi_A$ & $m_A$\\
		  \midrule
		  $\begin{pmatrix} \lambda & 0\\ 0 & \lambda \end{pmatrix}$ & $(t - \lambda)^2$ & $(t - \lambda)$\\\addlinespace
		  $\begin{pmatrix} \lambda & 0\\ 0 & \mu \end{pmatrix}$ & $(t - \lambda)(t - \mu)$ & $(t - \lambda)(t - \mu)$\\\addlinespace
		  $\begin{pmatrix} \lambda & 1\\ 0 & \lambda \end{pmatrix}$ & $(t - \lambda)^2$ & $(t - \lambda)^2$\\
		  \bottomrule
		\end{tabular}
	  \end{center}
	  with $\lambda, \mu \in \bbC$ distinct. We see that $m_A$ determines the Jordan normal form of $A$, but $\chi_A$ does not.
\end{example}

\begin{example}
	Every $3\times 3$ matrix in Jordan normal form is one of the six types. Here $\lambda_1, \lambda_2$ and $\lambda_3$ are distinct complex numbers.
  \begin{center}
    \begin{tabular}{ccccccc}
      \toprule
      Jordan normal form & $\chi_A$ & $m_A$\\
      \midrule
      $\begin{pmatrix} \lambda_1 & 0 & 0\\ 0 & \lambda_2 & 0\\ 0 & 0 & \lambda_3 \end{pmatrix}$ & $(t - \lambda_1)(t - \lambda_2)(t- \lambda_3)$ & $(t - \lambda_1)(t - \lambda_2)(t- \lambda_3)$\\\addlinespace
      $\begin{pmatrix} \lambda_1 & 0 & 0\\ 0 & \lambda_1 & 0\\ 0 & 0 & \lambda_2 \end{pmatrix}$ & $(t - \lambda_1)^2 (t - \lambda_2)$ & $(t - \lambda_1) (t - \lambda_2)$\\\addlinespace
      $\begin{pmatrix} \lambda_1 & 1 & 0\\ 0 & \lambda_1 & 0\\ 0 & 0 & \lambda_2 \end{pmatrix}$ & $(t - \lambda_1)^2 (t - \lambda_2)$ & $(t - \lambda_1)^2 (t - \lambda_2)$\\\addlinespace
      $\begin{pmatrix} \lambda_1 & 0 & 0\\ 0 & \lambda_1 & 0\\ 0 & 0 & \lambda_1 \end{pmatrix}$ & $(t - \lambda_1)^3$ & $(t - \lambda_1)$\\\addlinespace
      $\begin{pmatrix} \lambda_1 & 1 & 0\\ 0 & \lambda_1 & 0\\ 0 & 0 & \lambda_1 \end{pmatrix}$ & $(t - \lambda_1)^3$ & $(t - \lambda_1)^2$\\\addlinespace
      $\begin{pmatrix} \lambda_1 & 1 & 0\\ 0 & \lambda_1 & 1\\ 0 & 0 & \lambda_1 \end{pmatrix}$ & $(t - \lambda_1)^3$ & $(t - \lambda_1)^3$\\
      \bottomrule
    \end{tabular}
  \end{center}
  Notice that $\chi_A$ and $m_A$ together determine the Jordan normal form of a $3\times 3$ complex matrix. We do indeed need $\chi_A$ in the second case, since if we are given $m_A = (t - \lambda_1)(t - \lambda_2)$, we know one of the roots is double, but not which one.
\end{example}

\subsection{Generalised eigenspace decomposition}
Generalise the direct sum of eigenspaces in JNF case. 
\begin{theorem}[Generalised eigenspace decomposition]
	Let $ V $ be a finite-dimensional vector space over $\mathbb{C}$, and let $ \alpha\in \mcL(V) $ with
	\[
		m_\alpha(t) = \prod_{i=1}^k (t - \lambda_i)^{c_i}
	\]
	where $ (\lambda_i)_{1\le i\le k} $ are \textit{distinct} eigenvalues. Then 
	\[
		V = \bigoplus_{j=1}^k V_j
	\]
	where \( V_j = \ker[(\alpha - \lambda_j I)^{c_j}] \) is called a \textbf{generalised eigenspace} associated with \( \lambda_j \).	
\end{theorem}
\begin{definition}
	We say $\alpha \in \mathcal{L}(V)$ is \textbf{nilpotent} if there is some $r$ such that $\alpha^r = 0$.
  \end{definition}

  \begin{remark}
	Note that \( V_j \) is stable by \( \alpha \), that is, \( \alpha(V_j) = V_j \).
	Note further that \( {(\alpha - \lambda_j I)}\restriction_{V_j} = \mu_j \) gives that \( \mu_j \) is a nilpotent endomorphism; \( \mu_j^{c_j} = 0 \).
	So the Jordan normal form theorem is a statement about nilpotent matrices.

	The decomposition can be used to reduce the proof of JNF to a single eigenvalue. 

	Note, when \( \alpha \) is diagonalisable, \( c_j = 1 \) and hence we recover \( V_j = \ker(\alpha - \lambda_j I) \) and \( V = \bigoplus V_j \).
\end{remark}

\begin{proof}
	Projectors onto $V_j$ are explicit. Indeed: 
	\[
		p_j(t) = \prod_{i\neq j} (t-\lambda_i)^{c_i}
	\]
	Then $p_j$ have no common factors, so by Euclid's algorithm, we can find polynomial $ q_1,\dots,q_k $ over $\mathbb{C}$ such that
	\[
		\sum_{i=1}^{k}p_i q_i = 1
	\]
	Define $ \pi_j = q_j p_j (\alpha) $. 
	\begin{enumerate}
		\item By construction, for all \( \bfv \in V \), we have
		\[
			\sum_{j=1}^k \pi_j(\bfv) = \sum_{j=1}^k q_j p_j(\alpha(\bfv)) = I(\bfv) = \bfv
		\]
		Hence,
		\[
			\bfv = \sum_{i=1}^k \pi_i(\bfv)
		\]
		\item Observe further that \( \pi_j(\bfv) \in V_j \).
		Indeed,
		\[
			(\alpha - \lambda_j I)^{c_j} \pi_j(\bfv) = (\alpha - \lambda_j I)^{c_j} q_j p_j(\alpha(\bfv)) = q_j m_\alpha (\alpha(\bfv)) = 0
		\]
		Hence \( \pi_j(\bfv) \in V_j \).
		In particular, \( V = \sum_{j=1}^k V_j \).
		\item Note, for \( i \neq j \), \( \pi_i \pi_j = 0 \) from the definition of \( \pi \).
		Hence, observe that
		\[
			\pi_i = \pi_i I =  \pi_i \qty(\sum_{j=1}^k \pi_j) \implies \pi_i = \pi_i \pi_i
		\]
		Thus, \( \pi \) is a projector.
		In particular, this implies that \( {\pi_i}\restriction_{V_j} \) is the identity if \( i = j \) and zero if \( i \neq j \).
		This immediately implies that th sum is direct:
		\[
			V = \bigoplus_{j=1}^k V_j
		\]
		Indeed, suppose
		\[
			\sum_{j=1}^k \alpha_j \bfv_j = 0;\quad \bfv_j \in V_j;\quad \alpha_i \neq 0
		\]
		Then
		\[
			\bfv_1 = -\frac{1}{\alpha_1} \sum_{j=2}^k \alpha_j \bfv_j
		\]
		Applying \( \pi_1 \),
		\[
			\bfv_1 = -\frac{1}{\alpha_1} \sum_{j=2}^k \alpha_j \pi_1(\bfv_j) = 0
		\]
		Iterating, we find \( \bfv = 0 \).\qedhere
	\end{enumerate}
\end{proof}

We can compute the quantities \( a_\lambda, g_\lambda, c_\lambda \) on the Jordan normal form of a matrix.
Indeed, let \( m \geq 2 \) and consider a Jordan block \( J_m(\lambda) \).
\[
	\begin{pmatrix}
		\lambda & 1       & 0       & \cdots & 0       \\
		0       & \lambda & 1       & \cdots & 0       \\
		0       & 0       & \lambda & \cdots & 0       \\
		\vdots  & \vdots  & \vdots  & \ddots & \vdots  \\
		0       & 0       & 0       & \cdots & \lambda
	\end{pmatrix}
\]
Then \( J_m(\lambda) - \lambda I \) is the zero matrix with ones on the off-diagonal.
\[
	\begin{pmatrix}
		0 & 1       & 0       & \cdots & 0       \\
		0       & 0 & 1       & \cdots & 0       \\
		0       & 0       & 0 & \cdots & 0       \\
		\vdots  & \vdots  & \vdots  & \ddots & \vdots  \\
		0       & 0       & 0       & \cdots & 0
	\end{pmatrix}
\]
\( (J_m(\lambda) - \lambda I)^k \) pushes the ones onto the next line iteratively, so
\[
	(J_m(\lambda) - \lambda I)^k = \begin{pmatrix}
		0 & I_{m-k} \\
		0 & 0
	\end{pmatrix}
\]
If $k \geq n$, then we have $(J_n(\lambda) - \lambda I)^k = 0$.
Hence we can summarize this as
\[
\nullity((J_m(\lambda) - \lambda I_m)^r) = \min\{r, m\}.
\]
Hence \( J_m(\lambda) - \lambda I \) is nilpotent of order \( m \).

Note that if $A = J_n(\lambda)$, then $\chi_A(t) = m_A(t) = (t - \lambda)^n$. So $\lambda$ is the only eigenvalue of $A$. So $a_\lambda = c_\lambda = n$. We also know that $\nullity(A - \lambda I) = \nullity - \rank(A - \lambda I) = 1$. So $g_\lambda = 1$.

If $A$ is the block diagonal matrix
\[
  A =
  \begin{pmatrix}
    A_1\\
    & A_2\\
    & & \ddots\\
    & & & A_k
  \end{pmatrix},
\]
then
\[
  \chi_A(t) = \prod_{i = 1}^k \chi_{A_i}(k).
\]
Moreover, if $p \in \bbF[t]$, then
\[
  p (A) =
  \begin{pmatrix}
    p(A_1)\\
    & p(A_2)\\
    & & \ddots\\
    & & & p(A_k)
  \end{pmatrix}.
\]
Hence
\[
  m_A(t) = \lcm (m_{A_1}(t), \cdots, m_{A_k} (t)).
\]
By rank-nullity theorem, we have
\[
  \nullity(p(A)) = \sum_{i = 1}^k \nullity(p(A_i)).
\]
Thus if $A$ is in Jordan normal form, we get the following:
\begin{itemize}
  \item $g_\lambda$ is the number of Jordan blocks in $A$ with eigenvalue $\lambda$.
  \item $a_\lambda$ is the sum of sizes of the Jordan blocks of $A$ with eigenvalue $\lambda$. This is the amount of times \( \lambda \) is seen on the diagonal.
  \item $c_\lambda$ is the size of the largest Jordan block with eigenvalue $\lambda$.
\end{itemize}

\begin{example}
	Let
	\[
		A = \begin{pmatrix}
			0 & -1 \\
			1 & 2
		\end{pmatrix}
	\]
	We wish to convert this matrix into Jordan normal form; so we seek a basis for which this matrix becomes Jordan normal form.
	\[
		\chi_A(t) = (t-1)^2
	\]
	Hence there exists only one eigenvalue, \( \lambda = 1 \).
	\( A - I \neq 0 \) hence \( m_\alpha(t) = (t-1)^2 \).
	By previous result, the Jordan normal form of \( A \) is of the form
	\[
		B = \begin{pmatrix}
			1 & 1 \\
			0 & 1
		\end{pmatrix}
	\]
	Now,
	\[
		\ker(A - I) = \genset{\bfv_1};\quad \bfv_1 = \begin{pmatrix}
			1 \\ -1
		\end{pmatrix}
	\]
	Further, we seek a \( \bfv_2 \) such that
	\[
		(A - I)\bfv_2 = \bfv_1 \implies \bfv_2 = \begin{pmatrix}
			-1 \\ 0
		\end{pmatrix}
	\]
	Such a \( \bfv_2 \) is not unique.
	Now,
	\[
		A = \begin{pmatrix}
			1  & -1 \\
			-1 & 0
		\end{pmatrix}
		\begin{pmatrix}
			1 & 1 \\
			0 & 1
		\end{pmatrix}
		\begin{pmatrix}
			1  & -1 \\
			-1 & 0
		\end{pmatrix}^{-1}
	\]
	This is how we find a basis. 
\end{example}

\begin{example}
	Let
	\[
	  A =
	  \begin{pmatrix}
		3 & -2 & 0\\
		1 & 0 & 0\\
		1 & 0 & 1
	  \end{pmatrix}
	\]
	We know we can find the Jordan normal form by just computing the minimal polynomial and characteristic polynomial. But we can do better and try to find a $P$ such that $P^{-1}AP$ is in Jordan normal form.
  
	We first compute the eigenvalues of $A$. The characteristic polynomial is
	\[
	  \det \begin{pmatrix}
		t - 3 & -2 & 0\\
		1 & t & 0\\
		1 & 0 & t - 1
	  \end{pmatrix} = (t - 1)((t - 3)t + 2) = (t - 1)^2 (t - 2).
	\]
	We now compute the eigenspaces of $A$. We have
	\[
	  A - I =
	  \begin{pmatrix}
		2 & -2 & 0\\
		1 & -1 & 0\\
		1 & 0 & 0
	  \end{pmatrix}
	\]
	We see this has rank $2$ and hence nullity $1$, and the eigenspace is the kernel
	\[
	  V_1 = \genset{
	  \begin{pmatrix}
		0\\0\\1
	  \end{pmatrix}}
	\]
	We can also compute the other eigenspace. We have
	\[
	  A - 2I =
	  \begin{pmatrix}
		1 & -2 & 0\\
		1 & -2 & 0\\
		1 & 0 & -1
	  \end{pmatrix}
	\]
	This has rank $2$ and
	\[
	  V_2 = \genset{
	  \begin{pmatrix}
		2 \\ 1 \\ 2
	  \end{pmatrix}}
	\]
	Since
	\[
	  \dim V_1 + \dim V_2 = 2 < 3,
	\]
	this is not diagonalizable. So the minimal polynomial must also be $m_A(t) = \chi_A(t) = (t - 1)^2 (t - 2)$. From the classificaion last time, we know that $A$ is similar to
	\[
	  \begin{pmatrix}
		1 & 1 & 0\\
		0 & 1 & 0\\
		0 & 0 & 2
	  \end{pmatrix}
	\]
	We now want to compute a basis that transforms $A$ to this. We want a basis $(\mathbf{v}_1, \mathbf{v}_2, \mathbf{v}_3)$ of $\bbC^3$ such that
	\[
	  A\mathbf{v}_1 = \mathbf{v}_1,\quad A\mathbf{v}_2 = \mathbf{v}_1 + \mathbf{v}_2,\quad A \mathbf{v}_3 = 2\mathbf{v}_3.
	\]
	Equivalently, we have
	\[
	  (A - I)\mathbf{v}_1 = 0,\quad (A - I)\mathbf{v}_2 = \mathbf{v}_1, \quad (A - 2I)\mathbf{v}_3 = 0.
	\]
	There is an obvious choices $\mathbf{v}_3$, namely the eigenvector of eigenvalue $2$.
  
	To find $\mathbf{v}_1$ and $\mathbf{v}_2$, note that 
	\[
		\ker (A-I) = \genset{\mathbf{v}_1},\quad \mathbf{v}_1 = \begin{pmatrix}
			 0 \\
			 0 \\
			 1 \\
		\end{pmatrix}
	\]
	Solving $ (A-\lambda I)\mathbf{v}_2 = \mathbf{v}_1 $ we get 
	\[
		\mathbf{v}_2 =
		\begin{pmatrix}
		  1\\1\\0
		\end{pmatrix},\quad
		\mathbf{v}_1 =
		\begin{pmatrix}
		  0\\0\\1
		\end{pmatrix},\quad
		\mathbf{v}_3 =
		\begin{pmatrix}
		  2\\1\\2
		\end{pmatrix}.
	  \]
	Hence we have
	\[
	  P =
	  \begin{pmatrix}
		0 & 1 & 2\\
		0 & 1 & 1\\
		1 & 0 & 2
	  \end{pmatrix}
	\]
	and
	\[
	  P^{-1} AP = \begin{pmatrix}
		1 & 1 & 0\\
		0 & 1 & 0\\
		0 & 0 & 2
	  \end{pmatrix}.
	\]
  \end{example}

\section{Properties of bilinear forms}
\subsection{Changing basis}
Let \( \phi \colon V \times V \to \mathbb \bbF \) be a bilinear form.
Let \( V \) be a finite-dimensional \( \bbF \)-vector space.
Let \( B \) be a basis of \( V \) and let \( [\phi]_B = [\phi]_{BB} \) be the matrix with entries \( \phi(\bfe_i, \bfe_j) \).
\begin{lemma}
	Let \( \phi \) be a bilinear form \( V \times V \to \bbF \).
	Then if \( B, B' \) are bases for \( V \), and \( P = [I]_{B', B} \) we have
	\[
		[\phi]_{B'} = P^\top [\phi]_B P
	\]
\end{lemma}
\begin{proof}
	This is a special case of the general change of basis formula.
\end{proof}
\begin{definition}
	Let \( A, B \in M_n(\bbF) \) be square matrices.
	We say that \( A, B \) are \textbf{congruent} if there exists \( P \in M_n(\bbF) \) such that \( A = P^\top B P \).
\end{definition}
\begin{remark}
	Congruence is an equivalence relation.
\end{remark}
\begin{definition}
	A bilinear form \( \phi \) on \( V \) is \textbf{symmetric} if, for all \( \bfu, \bfv \in V \), we have
	\[
		\phi(\bfu,\bfv) = \phi(\bfv,\bfu)
	\]
\end{definition}
\begin{remark}
	If \( A \) is a square matrix, we say \( A \) is symmetric if \( A = A^\top \).
	Equivalently, \( A_{ij} = A_{ji} \) for all \( i, j \).
	So \( \phi \) is symmetric if and only if \( [\phi]_B \) is symmetric for any basis \( B \).
	Note further that to represent \( \phi \) by a diagonal matrix in some basis \( B \), it must necessarily be symmetric, since
	\[
		P^\top A P = D \implies D = D^\top = \qty(P^\top A P)^\top = P^\top A^\top P \implies A = A^\top
	\]
\end{remark}

\subsection{Quadratic forms}
We have a special name for $ \phi(\mathbf{u},\mathbf{u}) $. 
\begin{definition}
	A map \( Q \colon V \to \bbF \) is a \textbf{quadratic form} if there exists a bilinear form \( \phi \colon V \times V \to \bbF \) such that, for all \( \bfu \in V \),
	\[
		Q(\bfu) = \phi(\bfu,\bfu)
	\]
	So a quadratic form is the restriction of a bilinear form to the diagonal.
\end{definition}
\begin{remark}
	Let \( B = (\bfe_i) \) be a basis of \( V \).
	Let \( A = [\phi]_B = (\phi(\bfe_i, \bfe_j)) = (a_{ij}) \).
	Then, for \( \bfu = \sum_i x_i \bfe_i \in V \),
	\begin{align*}
		Q(\bfu) &= \phi(\bfu,\bfu) = \phi\qty(\sum_i x_i \bfe_i, \sum_j x_j \bfe_j) \\ 
		&= \sum_i \sum_j x_i x_j \phi(\bfe_i, \bfe_j) = \sum_i \sum_j x_i x_j a_{ij}
	\end{align*}
	We can check that this is equal to
	\[
		Q(\bfu) = \bfx^\top A \bfx
	\]
	where \( [\bfu]_B = \bfx \).
	Note further that
	\begin{align*}
		\bfx^\top A \bfx &= \sum_i \sum_j a_{ij} x_i x_j = \sum_i \sum_j a_{ji} x_i x_j \\ 
		&= \sum_i \sum_j \frac{a_{ij} + a_{ji}}{2} x_i x_j = \bfx^\top \qty(\underbrace{\frac{A + A^\top}{2}}_{\mathclap{\text{symmetric}}})\bfx
	\end{align*}
	So we can always express the quadratic form as a symmetric matrix in any basis.
\end{remark}
\begin{proposition}
	If \( Q \colon V \to \bbF \) is a quadratic form, then there exists a unique symmetric bilinear form \( \phi \colon V \times V \to \bbF \) such that \( Q(\bfu) = \phi(\bfu,\bfu) \).
\end{proposition}
\begin{proof}
	Let \( \psi \) be a bilinear form on \( V \) such that for all \( \bfu \in V \), we have \( Q(\bfu) = \psi(\bfu,\bfu) \).
	Then, let
	\[
		\phi(\bfu,\bfv) = \frac{1}{2}\qty[\psi(\bfu,\bfv) + \psi(\bfv,\bfu)]
	\]
	Certainly \( \phi \) is a bilinear form and symmetric.
	Further, \( \phi(\bfu,\bfu) = \psi(\bfu,\bfu) = Q(\bfu) \).
	So there exists a symmetric bilinear form \( \phi \) such that \( Q(\bfu) = \phi(\bfu,\bfu) \), so it suffices to prove uniqueness.
	Let \( \phi \) be a symmetric bilinear form such that for all \( \bfu \in V \) we have \( Q(\bfu) = \phi(\bfu,\bfu) \).
	Then, we can find
	\[
		Q(\bfu + \bfv) = \phi(\bfu + \bfv, \bfu + \bfv) = \phi(\bfu,\bfu) + \phi(\bfv,\bfv) + 2\phi(\bfu,\bfv)
	\]
	Thus \( \phi(\bfu,\bfv) \) is defined uniquely by \( Q \), since
	\[
		2 \phi(\bfu,\bfv) = Q(\bfu+\bfv) - Q(\bfu) - Q(\bfv)
	\]
	So \( \phi \) is unique (when \( 2 \) is invertible in \( \bbF \)).
	This identity for \( \phi(\bfu,\bfv) \) is known as the polarisation identity.
\end{proof}

\subsection{Diagonalisation of symmetric bilinear forms}
\ \vspace*{-1.5em}
\begin{theorem}
	Let \( \phi \colon V \times V \to \bbF \) be a symmetric bilinear form, where \( V \) is finite-dimensional.
	Then there exists a basis \( B \) of \( V \) such that \( [\phi]_B \) is diagonal.
\end{theorem}
\begin{proof}
	By induction on the dimension, suppose the theorem holds for all dimensions less than \( n \) for \( n \geq 2 \).
	If \( \phi(\bfu,\bfu) = 0 \) for all \( \bfu \in V \), then \( \phi = 0 \) by the polarisation identity, which is diagonal.
	Otherwise \( \phi(\bfe_1, \bfe_1) \neq 0 \) for some \( \bfe_1 \in V \).
	Let
	\[
		U = \qty(\genset{e_1})^\perp = \qty{\bfv \in V \colon \phi(\bfe_1, \bfv) = 0}
	\]
	This is a vector subspace of \( V \), which is in particular
	\[
		\ker \qty{ \phi(e_1, \cdot) \colon V \to \bbF }
	\]
	By the rank-nullity theorem, \( \dim U = n - 1 \).
	We now claim that \( U + \genset{\bfe_1} \) is a direct sum.
	Indeed, for \( \bfv = \genset{\bfe_1} \cap U \), we have \( \bfv = \lambda \bfe_1 \) and \( \phi(\bfe_1, \bfv) = 0 \).
	Hence \( \lambda = 0 \), since by assumption \( \phi(\bfe_1, \bfe_1) \neq 0 \).
	So we find a basis \( B' = (\bfe_2, \dots, \bfe_n) \) of \( U \), which we extend by \( \bfe_1 \) to \( B = (\bfe_1, \bfe_2, \dots, \bfe_n) \).
	Since \( U \oplus \genset{\bfe_1} \) has dimension \( n \), this is a basis of \( V \).
	Under this basis, we find
	\[
		[\phi]_B = \begin{pmatrix}
			\phi(\bfe_1, \bfe_1) & 0                          \\
			0              & \qty[{\phi}\restriction_{U}]_{B'}
		\end{pmatrix}
	\]
	because
	\[
		\phi(\bfe_1, \bfe_j) = \phi(\bfe_j, \bfe_1) = 0
	\]
	for all \( j \geq 2 \).
	By the inductive hypothesis we can take a basis \( B' \) such that the restricted \( \phi \) to be diagonal, so \( [\phi]_B \) is diagonal in this basis.
\end{proof}
\begin{example}
	Let \( V = \mathbb R^3 \) and choose the canonical basis \( (\bfe_i) \).
	Let
	\[
		Q(x_1, x_2, x_3) = x_1^2 + x_2^2 + 2x_3^2 + 2x_1 x_2 + 2x_1 x_3 - 2x_2 x_3
	\]
	Then, if \( Q(x_1, x_2, x_3) = \bfx^\top A \bfx \), we have
	\[
		A = \begin{pmatrix}
			1 & 1  & 1  \\
			1 & 1  & -1 \\
			1 & -1 & 2
		\end{pmatrix}
	\]
	Note that the off-diagonal terms are halved from their coefficients since in the expansion of \( \bfx^\top A \bfx \) they are included twice.
	Then, we can find a basis in which \( A \) is diagonal.
	We could use the above algorithm to find a basis, or complete the square in each component.
	We can write
	\[
		Q(x_1, x_2, x_3) = (x_1 + x_2 + x_3)^2 + x_3^2 - 4 x_2 x_3 = (x_1 + x_2 + x_3)^2 + (x_3 - 2x_2)^2 - (2x_2)^2
	\]
	This yields a new coordinate basis \( x_1', x_2', x_3' \).
	Then \( P^{-1} A P \) is diagonal.
	\( P \) is given by
	\[
		\begin{pmatrix} x_1' \\ x_2' \\ x_3' \end{pmatrix} = \underbrace{\begin{pmatrix} 1 & 1 & 1 \\ 0 & -2 & 1 \\ 0 & -2 & 0 \end{pmatrix}}_{P^{-1}} \begin{pmatrix} x_1 \\ x_2 \\ x_3 \end{pmatrix}
	\]
	With this basis, 
	\[
		P ^\top A P = \begin{pmatrix}
			1 & 0 &  0 \\
			0 & 1 &  0 \\
			0 & 0 &  -1 \\
		\end{pmatrix}
	\]
\end{example}

\subsection{Sylvester's law}
\ \vspace*{-1.5em}
\begin{corollary}
	If \( \bbF = \mathbb C \), for any symmetric bilinear form \( \phi \) there exists a basis of \( V \) such that \( [\phi]_B \) is
	\[
		\begin{pmatrix}
			I_r & 0 \\
			0   & 0
		\end{pmatrix}
	\]
	where $ r = \rank \phi $. 
\end{corollary}
\begin{proof}
	Since any symmetric bilinear form \( \phi \) in a finite-dimensional \( \bbF \)-vector space \( V \) can be diagonalised, let \( E = (\bfe_1, \dots, \bfe_n) \) such that \( [\phi]_E \) is diagonal with diagonal entries \( a_i \).

	Reorder the \( a_i \) such that \( a_i \) is nonzero for \( 1 \leq i \leq r \), and the remaining values (if any) are zero.
	For \( i \leq r \), let \( \sqrt{a_i} \) be a choice of a complex root for \( a_i \).
	Then \( \bfv_i = \frac{\bfe_i}{\sqrt{a_i}} \) for \( i \leq r \) and \( \bfv_i = \bfe_i \) for \( i > r \) gives the basis \( B \) as required.
\end{proof}
\begin{corollary}
	Every symmetric matrix of \( M_n(\mathbb C) \) is congruent to a unique matrix of the form
	\[
		\begin{pmatrix}
			I_r & 0 \\
			0   & 0
		\end{pmatrix}
	\]
	where \( r \) is the rank of the matrix.
\end{corollary}
\begin{corollary}
	Let \( \bbF = \mathbb R \), and let \( V \) be a finite-dimensional \( \mathbb R \)-vector space.
	Let \( \phi \) be a symmetric bilinear form on \( V \).
	Then there exists a basis \( B = (\bfv_1, \dots, \bfv_n) \) of \( V \) such that
	\[
		[\phi]_B = \begin{pmatrix}
			I_p & 0    & 0 \\
			0   & -I_q & 0 \\
			0   & 0    & 0
		\end{pmatrix}
	\]
	for some integers \( p, q ,\ p+q = \rank \phi\).
\end{corollary}
\begin{proof}
	Since square roots do not necessarily exist in \( \mathbb R \), we cannot use the form above.
	We first diagonalise the bilinear form in some basis \( E \).
	Then, reorder and group the \( a_i \) into a positive group of size \( p \), a negative group of size \( q \), and a zero group.
	Then,
	\[
		\bfv_i = \begin{cases}
			\frac{\bfe_i}{\sqrt{a_i}}  & i \in \qty{1, \dots, p}     \\
			\frac{\bfe_i}{\sqrt{-a_i}} & i \in \qty{p+1, \dots, p+q} \\
			\bfe_i                     & i \in \qty{p+q+1, \dots, n}
		\end{cases}
	\]
	This gives a new basis as required.
\end{proof}
\begin{definition}
	Let \( \bbF = \mathbb R \).
	The \textbf{signature} of a bilinear form \( \phi \) is
	\[
		s(\phi) = p - q
	\]
	where \( p \) and \( q \) are defined as in the corollary above.
\end{definition}
It is well defined, i.e. it is independent of basis:
\begin{theorem}[Sylvester's law]
	Let \( \bbF = \mathbb R \).
	Let \( V \) be a finite-dimensional \( \mathbb R \)-vector space.
	If a real symmetric bilinear form is represented by some matrix
	\[
		\begin{pmatrix}
			I_p & 0    & 0 \\
			0   & -I_q & 0 \\
			0   & 0    & 0
		\end{pmatrix}
	\]
	in some basis \( B \), and some other matrix
	\[
		\begin{pmatrix}
			I_{p'} & 0       & 0 \\
			0      & -I_{q'} & 0 \\
			0      & 0       & 0
		\end{pmatrix}
	\]
	in another basis \( B' \), then \( p = p' \) and \( q = q' \).
	Thus, the signature of the matrix is well defined.
\end{theorem}
\begin{definition}
	Let \( \phi \) be a symmetric bilinear form on a real vector space \( V \).
	We say that
	\begin{enumerate}
		\item \( \phi \) is \textbf{positive definite} if \( \phi(\bfu,\bfu) > 0 \) for all nonzero \( \bfu \in V \);
		\item \( \phi \) is \textbf{positive semidefinite} if \( \phi(\bfu,\bfu) \geq 0 \) for all \( \bfu \in V \);
		\item \( \phi \) is \textbf{negative definite} or \textbf{negative semidefinite} if \( \phi(\bfu,\bfu) < 0 \) or \( \phi(\bfu,\bfu) \leq 0 \) respectively for all nonzero \( \bfu \in V \).
	\end{enumerate}
\end{definition}
\begin{example}
	The matrix
	\[
		\begin{pmatrix}
			I_r & 0 \\
			0   & 0
		\end{pmatrix}
	\]
	is positive definite for \( r = n \), and positive semidefinite for \( r < n \).
\end{example}
\noindent We now prove Sylvester's law.
\begin{proof}
	In order to prove uniqueness of \( p \), we will characterise the matrix in a way that does not depend on the basis.
	In particular, we will show that \( p \) is the largest dimension of a vector subspace of \( V \) such that the restriction of \( \phi \) on this subspace is positive definite.
	Suppose we have \( B = (\bfv_1, \dots, \bfv_n) \) and
	\[
		[\phi]_B = \begin{pmatrix}
			I_p & 0    & 0 \\
			0   & -I_q & 0 \\
			0   & 0    & 0
		\end{pmatrix}
	\]
	We consider
	\[
		X = \genset{\bfv_1, \dots, \bfv_p}
	\]
	Then we can easily compute that \({\phi}\restriction_X \) is positive definite.
	Let
	\[
		Y = \genset{\bfv_{p+1}, \dots, \bfv_n}
	\]
	Then, as above, \( {\phi}\restriction_Y \) is negative semidefinite.
	Suppose that \( \phi \) is positive definite on another subspace \( X' \).
	In this case, \( Y \cap X' = \qty{0} \), since if \( \bfy \in Y \cap X' \) we must have \( Q(\bfy) \leq 0 \), but since \( \bfy \in X' \) we have \( \bfy = 0 \).

	Thus, \( Y + X' = Y \oplus X' \), so \( n = \dim V \geq \dim Y + \dim X' \).
	But \( \dim Y = n - p \), so \( \dim X' \leq p \).
	The same argument can be executed for \( q \), hence both \( p \) and \( q \) are independent of basis.
\end{proof}

\subsection{Kernels of bilinear forms}
\ \vspace*{-1.5em}
\begin{definition}
	Let \( K = \qty{ \bfv \in V \colon \forall \bfu \in V, \phi(\bfu,\bfv) = 0 } \).
	This is the \textbf{kernel} of the bilinear form.
\end{definition}
\begin{remark}
	By the rank-nullity theorem,
	\[
		\dim K + \rank \phi = n
	\]
	Using the above notation, we can show that there exists a subspace \( T \) of dimension \( n - (p+q) + \min\qty{p,q} \) such that \( {\phi}\restriction_T = 0 \).
	Indeed, let \( B = (\bfv_1, \dots, \bfv_n) \) such that
	\[
		[\phi]_B = \begin{pmatrix}
			I_p & 0    & 0 \\
			0   & -I_q & 0 \\
			0   & 0    & 0
		\end{pmatrix}
	\]
	The quadratic form has a zero subspace of dimension \( n - (p+q) \) in the bottom right.
	But by setting
	\[
		T = \qty{\bfv_1 + \bfv_{p+1}, \dots, \bfv_q + \bfv_{p+q}, \bfv_{p+q+1}, \dots, \bfv_n}
	\]
	we can combine the positive and negative blocks (assuming here that \( p \geq q \)) to produce more linearly independent elements of the kernel.
	In particular, \( \dim T \) is the largest possible dimension of a subspace \( T' \) of \( V \) such that \( {\phi}\restriction_{T'} = 0 \).
\end{remark}

\subsection{Sesquilinear forms}
Let \( \bbF = \mathbb C \).
The standard inner product on \( \mathbb C^n \) is defined to be
\[
	\inner{\begin{pmatrix} x_1 \\ \vdots \\ v_n \end{pmatrix}, \begin{pmatrix} y_1 \\ \vdots \\ y_n \end{pmatrix}} = \sum_{i=1}^n x_i \overline y_i
\]
This is not a bilinear form on \( \mathbb C \) due to the complex conjugate, it is linear in the first entry.
\begin{definition}
	Let \( V, W \) be \( \mathbb C \)-vector spaces.
	A form \( \phi \colon V \times W \to \mathbb C \) is called \textbf{sesquilinear} if it is linear in the first entry, and
	\[
		\phi(\bfv, \lambda_1 \bfw_1 + \lambda_2 \bfw_2) = \overline \lambda_1 \phi(\bfv,\bfw_1) + \overline \lambda_2 \phi(\bfv,\bfw_2)
	\]
	so it is antilinear with respect to the second entry.
\end{definition}
\begin{lemma}
	Let \( B = (\bfv_1, \dots, \bfv_m) \) be a basis of \( V \) and \( C = (\bfw_1, \dots, \bfw_n) \) be a basis of \( W \).
	Let \( [\phi]_{B,C} = \qty(\phi(\bfv_i, \bfw_j)) \).
	Then,
	\[
		\phi(\bfv,\bfw) = [\bfv]_B^\top [\phi]_{B,C} \overline{[\bfw]_C}
	\]
\end{lemma}
\begin{proof}
	Let \( B, B' \) be bases of \( V \) and \( C, C' \) be bases of \( W \).
	Let \( P = [I]_{B', B} \) and \( Q = [I]_{C', C} \).
	Then
	\[
		[\phi]_{B', C'} = P^\top [\phi]_{B,C} \overline Q. \qedhere
	\]
\end{proof}

\subsection{Hermitian forms}
\ \vspace*{-1.5em}
\begin{definition}
	Let \( V \) be a finite-dimensional \( \mathbb C \)-vector space.
	Let \( \phi \) be a sesquilinear form on \( V \).
	Then \( \phi \) is \textbf{Hermitian} if, for all \( \bfu, \bfv \in V \),
	\[
		\phi(\bfu, \bfv) = \overline{\phi(\bfv,\bfu)}
	\]
\end{definition}
\begin{remark}
	If \( \phi \) is Hermitian, then \( \phi(\bfu,\bfu) = \overline{\phi(\bfu,\bfu)} \in \mathbb R \).
	Further, \( \phi(\lambda \bfu, \lambda \bfu) = \abs{\lambda}^2 \phi(\bfu,\bfu) \).
	This allows us to define positive and negative definite Hermitian forms.
\end{remark}
\begin{lemma}
	A sesquilinear form \( \phi \colon V \times V \to \mathbb C \) is Hermitian if and only if, for any basis \( B \) of \( V \),
	\[
		[\phi]_B = [\phi]_B^\dagger
	\]
\end{lemma}
\begin{proof}
	Let \( A = [\phi]_B = (a_{ij}) \).
	Then \( a_{ij} = \phi(\bfe_i, \bfe_j) \), and \( a_{ji} = \phi(\bfe_j, \bfe_i) = \overline{\phi(\bfe_i, \bfe_j)} = \overline{a_{ij}} \).
	So \( \overline A^\top = A \).
	Conversely suppose that \( [\phi]_B = A = \overline A^\top \).
	Now let
	\[
		\bfu = \sum_{i=1}^n \lambda_i \bfe_i;\quad v = \sum_{i=1}^n \mu_i \bfe_i
	\]
	Then,
	\[
		\phi(\bfu,\bfv) = \phi\qty(\sum_{i=1}^n \lambda_i \bfe_i, \sum_{i=1}^n \mu_i \bfe_i) = \sum_{i=1}^n \sum_{j=1}^n \lambda_i \overline{\mu_j} a_{ij}
	\]
	Further,
	\[
		\overline{\phi(\bfv,\bfu)} = \overline{\phi\qty(\sum_{i=1}^n \mu_i \bfe_i, \sum_{i=1}^n \lambda_i \bfe_i)} = \sum_{i=1}^n \sum_{j=1}^n \overline{\mu_j \overline{\lambda_i}} \overline{a_{ij}}
	\]
	which is equivalent.
	Hence \( \phi \) is Hermitian.
\end{proof}

\subsection{Polarisation identity}
A Hermitian form \( \phi \) on a complex vector space \( V \) is entirely determined by a quadratic form \( Q \colon V \to \mathbb R \) such that \( \bfv \mapsto \phi(\bfv,\bfv) \) by the formula
\[
	\phi(\bfu,\bfv) = \frac{1}{4} \qty[ Q(\bfu+\bfv) - Q(\bfu-\bfv) + iQ(\bfu+i\bfv) - iQ(\bfu-i\bfv) ]
\]

\subsection{Hermitian formulation of Sylvester's law}
\ \vspace*{-1.5em}
\begin{theorem}[Hermitian formulation of Sylvester's law]
	Let \( V \) be a finite-dimensional \( \mathbb C \)-vector space.
	Let \( \phi \colon V \times V \to \mathbb C \) be a Hermitian form on \( V \).
	Then there exists a basis \( B = (\bfv_1, \dots, \bfv_n) \) of \( V \) such that
	\[
		[\phi]_B = \begin{pmatrix}
			I_p & 0    & 0 \\
			0   & -I_q & 0 \\
			0   & 0    & 0
		\end{pmatrix}
	\]
	where \( p, q \) depend only on \( \phi \) and not \( B \).
\end{theorem}
\begin{proof}
	The following is a sketch proof; it is nearly identical to the case of real symmetric bilinear forms.

	If \( \phi = 0 \), existence is trivial.
	Otherwise, using the polarisation identity there exists \( \bfe_1 \neq 0 \) such that \( \phi(\bfe_1, \bfe_1) \neq 0 \).
	Let
	\[
		\bfv_1 = \frac{\bfe_1}{\sqrt{\abs{\phi(\bfe_1, \bfe_1)}}} \implies \phi(\bfv_1, \bfv_1) = \pm 1
	\]
	Consider the orthogonal space \( W = \qty{w \in V \colon \phi(\bfv_1, \bfw) = 0} \).
	We can check, arguing analogously to the real case, that \( V = \genset{\bfv_1} \oplus W \).
	Hence, we can inductively diagonalise \( \phi \).

	\( p, q \) are unique.
	Indeed, we can prove that \( p \) is the maximal dimension of a subspace on which \( \phi \) is positive definite (which is well-defined since \( \phi(\bfu,\bfu) \in \mathbb R \)).
	The geometric interpretation of \( q \) is similar.
\end{proof}

\subsection{Skew-symmetric forms}
\ \vspace*{-1.5em}
\begin{definition}
	Let \( V \) be a finite-dimensional \( \mathbb R \)-vector space.
	Let \( \phi \) be a bilinear form on \( V \).
	Then \( \phi \) is \textbf{skew-symmetric} if, for all \( \bfu,\bfv \in V \),
	\[
		\phi(\bfu,\bfv) = -\phi(\bfv,\bfu)
	\]
\end{definition}
\begin{remark}
	\( \phi(\bfu,\bfu) = -\phi(\bfu,\bfu) = 0 \).
	Also, in any basis \( B \) of \( V \), we have \( [\phi]_B = -[\phi]_B^\top \).
	Any real matrix can be decomposed as the sum
	\[
		A = \frac{1}{2}\qty(A + A^\top) + \frac{1}{2}\qty(A - A^\top)
	\]
	where the first term is symmetric and the second is skew-symmetric.
\end{remark}

\subsection{Skew-symmetric formulation of Sylvester's law}
\ \vspace*{-1.5em}
\begin{theorem}[Skew-symmetric formulation of Sylvester's law]
	Let \( V \) be a finite-dimensional \( \mathbb R \)-vector space.
	Let \( \phi \colon V \times V \to \mathbb R \) be a skew-symmetric form on \( V \).
	Then there exists a basis
	\[
		B = (\bfv_1, \bfw_1, \bfv_2, \bfw_2, \dots, \bfv_m, \bfw_m, \bfv_{2m+1}, \bfv_{2m+2}, \dots, \bfv_n)
	\]
	of \( V \) such that
	\[
		[\phi]_B = \begin{pmatrix}
			0  & 1                       \\
			-1 & 0                       \\
			   &   & 0  & 1              \\
			   &   & -1 & 0              \\
			   &   &    &   & \ddots     \\
			   &   &    &   &        & 0
		\end{pmatrix}
	\]
\end{theorem}
\begin{corollary}
	Skew-symmetric matrices have an even rank.
\end{corollary}
\begin{proof}
	This is again very similar to the previous case.
	We will perform an inductive step on the dimension of \( V \).
	If \( \phi \neq 0 \), there exist \( \bfv_1, \bfw_1 \) such that \( \phi_1(\bfv_1, \bfw_1) \neq 0 \).
	After scaling one of the vectors, we can assume \( \phi(\bfv_1, \bfw_1) = 1 \).
	Since \( \phi \) is skew-symmetric, \( \phi(\bfw_1, \bfv_1) = -1 \).
	Then \( \bfv_1, \bfw_1 \) are linearly independent; if they were linearly dependent we would have \( \phi(\bfv_1, \bfw_1) = \phi(\bfv_1, \lambda \bfv_1) = 0 \).
	Let \( U = \genset{\bfv_1, \bfw_1} \) and let \( W = \qty{v \in V \colon \phi(\bfv_1, \bfv) = \phi(\bfw_1, \bfv) = 0} \) and we can show \( V = U \oplus W \).
	Then induction gives the required result.
\end{proof}

\section{Inner product spaces}
\ \vspace*{-1.5em}
\begin{definition}
	Let \( V \) be a vector space over \( \mathbb R \) or \( \mathbb C \).
	A \textbf{scalar product} or \textbf{inner product} is a positive-definite symmetric (respectively Hermitian) bilinear form \( \phi \) on \( V \).
	We write
	\[
		\phi(\bfu,\bfv) = \inner{\bfu,\bfv}
	\]
	\( V \), when equipped with this inner product, is called a real (respectively complex) \textbf{inner product space}.
\end{definition}
\begin{example}
	In \( \mathbb C^n \), we define
	\[
		\inner{\bfx,\bfy} = \sum_{i=1}^n x_i \overline y_i
	\]
\end{example}
\begin{example}
	Let \( V = C^0([0,1], \mathbb C) \).
	Then we can define
	\[
		\inner{f,g} = \int_0^1 f(t) \overline g(t) \dd{t}
	\]
	This is the \( L^2 \) scalar product.
\end{example}
\begin{example}
	Let \( \omega \colon [0,1] \colon \mathbb R^*_+ \) where \( \mathbb R^*_+ = \mathbb R_+ \setminus \qty{0} \) and define
	\[
		\inner{f,g} = \int_0^1 f(t) \overline g(t) w(t) \dd{t}
	\]
\end{example}
\begin{remark}
	Typically it suffices to check \( \inner{\bfu,\bfu} = 0 \implies \bfu = 0 \) since linearity and positivity are usually trivial.
\end{remark}
\begin{definition}
	Let \( V \) be an inner product space.
	Then for \( \bfv \in V \), the \textbf{norm} of \( \bfv \) induced by the inner product is defined by
	\[
		\norm{\bfv} = \qty(\inner{\bfv,\bfv})^{1/2}
	\]
	This is real, and positive if \( \bfv \neq 0 \).
\end{definition}
It allows us to define a notion of length. 

\subsection{Cauchy-Schwarz inequality}
\ \vspace*{-1.5em}
\begin{lemma}
	For an inner product space,
	\[
		\abs{\inner{\bfu,\bfv}} \leq \norm{\mathbf{u}} \cdot \norm{\mathbf{v}}
	\]
\end{lemma}
\begin{proof}
	Let \( t \in \bbF \).
	Then,
	\[
		0 \leq \norm{t \bfu - \bfv} = \inner{t\bfu - \bfv, t\bfu - \bfv} = t \overline t \inner{\bfu,\bfu} - t \inner{\bfu,\bfv} - \overline t \inner{\bfv,\bfu} + \norm{\bfv}^2
	\]
	Since the inner product is Hermitian,
	\[
		0 \leq \abs{t}^2 \norm{\bfu}^2 + \norm{\bfv}^2 - 2 \Re(t \inner{\bfu,\bfv})
	\]
	By choosing
	\[
		t = \frac{\overline{\inner{\bfu,\bfv}}}{\norm{\bfu}^2}
	\]
	we have
	\[
		0 \leq \frac{\abs{\inner{\bfu,\bfv}}^2}{\norm{\bfu}^2} + \norm{\bfv}^2 - 2 \Re\qty(\frac{\abs{\inner{\bfu,\bfv}}^2}{\norm{\bfu}^2})
	\]
	Since the term under the real part operator is real, we have 
	\[
		0\le \left\| \mathbf{v} \right\|^2 - \frac{\abs{\inner{\mathbf{u},\mathbf{v}}}^2}{\left\| \mathbf{u} \right\|^2}
	\]and the result follows.
\end{proof}
\noindent Note that equality implies collinearity in the Cauchy-Schwarz inequality.
\begin{corollary}[triangle inequality]
	In an inner product space,
	\[
		\norm{\bfu+\bfv} \leq \norm{\bfu} + \norm{\bfv}
	\]
\end{corollary}
\begin{proof}
	We have
	\begin{align*}
		\norm{\bfu+\bfv}^2 &= \inner{\bfu+\bfv, \bfu+\bfv} = \norm{\bfu^2} + 2 \Re(\inner{\bfu,\bfv}) + \norm{\bfv}^2 \\ 
		&\leq \norm{\bfu}^2 + \norm{\bfv}^2 + 2 \norm{\bfu} \cdot \norm{\bfv} = (\norm{\bfu} + \norm{\bfv})^2. \qedhere
	\end{align*}
\end{proof}
\begin{remark}
	Any inner product induces a norm, but not all norms derive from scalar products.
\end{remark}

\subsection{Orthogonal and orthonormal sets}
\ \vspace*{-1.5em}
\begin{definition}
	A set \( (\bfe_1, \dots, \bfe_k) \) of vectors of \( V \) is said to be \textbf{orthogonal} if \( \inner{\bfe_i, \bfe_j} = 0 \) for all \( i \neq j \).
	The set is said to be \textbf{orthonormal} if it is orthogonal and \( \norm{\bfe_i} = 1 \) for all \( i \).
	In this case, \( \inner{\bfe_i, \bfe_j} = \delta_{ij} \).
\end{definition}
\begin{lemma}
	If \( (\bfe_1, \dots, \bfe_k) \) are orthogonal and nonzero, then they are linearly independent.
	Further, let \( \bfv \in \genset{\qty{\bfe_i}} \).
	Then,
	\[
		\bfv = \sum_{j=1}^k \lambda_j \bfe_j \implies \lambda_j = \frac{\inner{\bfv, \bfe_j}}{\norm{\bfe_j}^2}
	\]
\end{lemma}
\begin{proof}
	Suppose
	\[
		\sum_{i=1}^k \lambda_i \bfe_i = 0
	\]
	Then,
	\[
		0 = \inner{\sum_{i=1}^k \lambda_i, \bfe_j} \implies 0 = \sum_{i=1}^k \lambda_i \inner{\bfe_i, \bfe_j}
	\]
	Thus \( \lambda_j = 0 \) for all \( j \).
	Further, for \( \bfv \) in the span of these vectors,
	\[
		\inner{\bfv, \bfe_j} = \sum_{i=1}^k \lambda_i \inner{\bfe_i, \bfe_j} = \lambda_j \norm{\bfe_j}^2 \qedhere
	\]
\end{proof}
\begin{corollary}[Parseval's identity]
	Let \( V \) be a finite-dimensional inner product space.
	Let \( (\bfe_1, \dots, \bfe_n) \) be an orthonormal basis.
	Then, for any vectors \( \bfu, \bfv \in V \), we have
	\[
		\inner{\bfu, \bfv} = \sum_{i=1}^n \inner{\bfu, \bfe_i} \overline{\inner{\bfv, \bfe_i}}
	\]
	Hence,
	\[
		\norm{\bfu}^2 = \sum_{i=1}^n \abs{\inner{\bfu,\bfe_i}}^2
	\]
\end{corollary}
\begin{proof}
	By orthonormality,
	\[
		\bfu = \sum_{i=1}^n \inner{\bfu, \bfe_i} \bfe_i;\quad \bfv = \sum_{i=1}^n \inner{\bfv, \bfe_i} \bfe_i
	\]
	Hence, by sesquilinearity,
	\[
		\inner{\bfu,\bfv} = \sum_{i=1}^n \inner{\bfu, \bfe_i} \overline{\inner{\bfv, \bfe_i}}
	\]
	By taking \( \bfu = \bfv \) we find
	\[
		\norm{\bfu}^2 = \inner{\bfu,\bfu} = \sum_{i=1}^n \abs{\inner{\bfu,\bfe_i}}^2\qedhere
	\]
\end{proof}

\subsection{Gram-Schmidt orthogonalisation process}
We can orthonormalise a set of vectors by:
\begin{theorem}[Gram-Schmidt orthogonalisation process]
	Let \( V \) be an inner product space.
	Let \( (\bfv_i)_{i \in I} \) be a linearly independent family of vectors such that \( I \) is countable.
	Then there exists a family \( (\bfe_i)_{i \in I} \) of orthonormal vectors such that for all \( k \geq 1 \),
	\[
		\genset{\bfv_1, \dots, \bfv_k} = \genset{\bfe_1, \dots, \bfe_k}
	\]
\end{theorem}
\begin{proof}
	This proof is an explicit algorithm to compute the family \( (\bfe_i) \), which will be computed by induction on \( k \).
	
	For \( k = 1 \), take \( \bfe_1 = \frac{\bfv_1}{\norm{\bfv_1}} \).

	Inductively, suppose \( (\bfe_1, \dots, \bfe_k) \) satisfy the conditions as above.
	Then we will find a valid \( \bfe_{k+1} \).
	We define
	\[
		\bfe_{k+1}' = \bfv_{k+1} - \sum_{i=1}^k \inner{\bfv_{k+1}, \bfe_i} \bfe_i
	\]
	This ensures that the inner product between \( \bfe_{k+1}' \) and any basis vector \( \bfe_j \) is zero, while maintaining the same span.
	Suppose \( \bfe_{k+1}' = 0 \).
	Then, \( \bfv_{k+1} \in \genset{\bfe_1, \dots, \bfe_k} = \genset{\bfv_1, \dots, \bfv_k} \) which contradicts the fact that the family is free.
	Thus,
	\[
		\bfe_{k+1} = \frac{\bfe_{k+1}'}{\norm{\bfe_{k+1}'}}
	\]
	satisfies the requirements.
\end{proof}
\begin{corollary}
	In finite-dimensional inner product spaces, there always exists an orthonormal basis.
	In particular, any orthonormal set of vectors can be extended into an orthonormal basis.
\end{corollary}
\begin{remark}
	Let \( A \in \mcM_n(\mathbb R) \) be a real-valued (or complex-valued) matrix.
	Then, the column vectors of \( A \) are orthogonal if \( A^\top A = I \) (or \( A^\top \overline A = I \) in the complex-valued case).
\end{remark}

\begin{definition}
	A matrix \( A \in \mcM_n(\mathbb R) \) is \textbf{orthogonal} if \( A^\top A = I \), hence \( A^\top = A^{-1} \).
	A matrix \( A \in \mcM_n(\mathbb C) \) is \textbf{unitary} if \( A^\top \overline A = I \), hence \( A^\dagger = A^{-1} \).
\end{definition}
\begin{proposition}
	Let \( A \) be a square, non-singular, real-valued (or complex-valued) matrix.
	Then \( A \) can be written as \( A = RT \) where \( T \) is upper triangular and \( R \) is orthogonal (or respectively unitary).
\end{proposition}
\begin{proof}
	We apply the Gram-Schmidt process to the column vectors of the matrix.
	This gives us an orthonormal set of vectors, which gives an upper triangular matrix in this new basis.
\end{proof}

\subsection{Orthogonal complement and projection}
\ \vspace*{-1.5em}
\begin{definition}
	Let \( V \) be an inner product space.
	Let \( V_1, V_2 \leq V \).
	Then we say that \( V \) is the \textbf{orthogonal direct sum} of \( V_1 \) and \( V_2 \) if \( V = V_1 \oplus V_2 \) and for all vectors \( \bfv_1\in V_1, \bfv_2\in V_2 \) we have \( \inner{\bfv_1, \bfv_2} = 0 \).
	When this holds, we write \( V = V_1 \overset{\perp}{\oplus} V_2 \).
\end{definition}
\begin{remark}
	If for all vectors \( \bfv_1, \bfv_2 \) we have \( \inner{\bfv_1, \bfv_2} = 0 \), then \( \bfv \in V_1 \cap V_2 \implies \norm{\bfv}^2 = 0 \implies \bfv = 0 \).
	Hence the sum is always direct if the subspaces are orthogonal.
\end{remark}
\begin{definition}
	Let \( V \) be an inner product space and let \( W \leq V \).
	We define the \textbf{orthogonal} of \( W \) to be
	\[
		W^\perp = \qty{\bfv \in V \colon \forall \bfw \in W, \inner{\bfv,\bfw} = 0}
	\]
\end{definition}
\begin{lemma}
	For any inner product space \( V \) and any subspace \( W \leq V \), we have \( V = W \overset{\perp}{\oplus} W^\perp \).
\end{lemma}
\begin{proof}
	First note that \( W^\perp \leq V \).
	Then, if \( \bfw \in W \cap W^\perp \), we have
	\[
		\norm{\bfw}^2 = \inner{\bfw, \bfw} = 0
	\]
	since they are orthogonal, so the vector subspaces intersect only in the zero vector.
	Now, we need to show \( V = W + W^\perp \).
	Let \( (\bfe_1, \dots, \bfe_k) \) be an orthonormal basis of \( W \) and extend it into \( (\bfe_1, \dots, \bfe_k, \bfe_{k+1}, \dots, \bfe_n) \) which can be made orthonormal.
	Then, \( (\bfe_{k+1}, \dots, \bfe_n) \) are elements of \( W^\perp \) and form a basis.
\end{proof}

\begin{definition}
	Suppose \( V = U \oplus W \), so \( U \) is a complement of \( W \) in \( V \).
	Then, we define \( \pi \colon V \to W \) which maps \( \bfv = \bfu + \bfw \) to \( \bfw \).
	This is well defined, since the sum is direct.
	\( \pi \) is linear, and \( \pi^2 = \pi \).

	We say that \( \pi \) is the \textbf{projection} operator onto \( W \).
\end{definition}
\begin{remark}
	The map \( \iota - \pi \) is the projection onto \( U \), where \( \iota \) is the identity map.
\end{remark}
\begin{lemma}
	Let \( V \) be an inner product space.
	Let \( W \leq V \) be a finite-dimensional subspace.
	Let \( (\bfe_1, \dots, \bfe_k) \) be an orthonormal basis for \( W \).
	Then,
	\begin{enumerate}[(a)]
		\item \( \pi(\bfv) = \sum_{i=1}^k \inner{\bfv, \bfe_i} \bfe_i \); and
		\item for all \( \bfv \in V, \bfw \in W \), \( \norm{\bfv - \pi(\bfv)} \leq \norm{\bfv - \bfw} \) with equality if and only if \( \bfw = \pi(\bfv) \), hence \( \pi(\bfv) \) is the point in \( W \) closest to \( \bfv \).
	\end{enumerate}
\end{lemma}
\begin{proof}
	We define \( \pi(\bfv) = \sum_{i=1}^k \inner{\bfv, \bfe_i} \bfe_i \).
	Since \( W = \genset{\qty{\bfe_k}} \), \( \pi(\bfv) \in W \) for all \( \bfv \in V \).
	Then, \( \bfv = (\bfv - \pi(\bfv)) + \pi(\bfv) \) has a term in \( W \).
	We claim that the remaining term is in the orthogonal; \( \bfv - \pi(\bfv) \in W^\perp \).

	Indeed, we must show \( \inner{\bfv - \pi(\bfv), \bfw} = 0 \) for all \( \bfw \in W \).
	Equivalently, 
	\[
		\inner{\bfv - \pi(\bfv), \bfe_i} = 0
	\]
	for all basis vectors \( \bfe_i \) of \( W \).
	We can explicitly compute
	\begin{align*}
		\inner{\bfv - \pi(\bfv), \bfe_j} &= \inner{\bfv, \bfe_j} - \inner{\sum_{i=1}^k \inner{\bfv, \bfe_i} \bfe_i, \bfe_j} \\ 
		&= \inner{\bfv, \bfe_j} - \sum_{i=1}^k \inner{\bfv, \bfe_i} \inner{\bfe_i, \bfe_j} = \inner{\bfv, \bfe_j} - \inner{\bfv, \bfe_j} = 0
	\end{align*}
	Hence, \( \bfv = (\bfv - \pi(\bfv)) + \pi(\bfv) \) is a decomposition into \( W \) and \( W^\perp \).
	Since \( W \cap W^\perp = \qty{0} \), we have \( V = W \overset{\perp}{\oplus} W^\perp \).
	For the second part, let \( \bfv \in V \), \( \bfw \in W \), and we compute
	\begin{align*}
		\norm{\bfv - \bfw}^2 &= \|\underbrace{\bfv - \pi(\bfv)}_{\in W^\perp} + \underbrace{\pi(\bfv) - \bfw}_{\in W}\|^2 \\ 
		&= \norm{\bfv - \pi(\bfv)}^2 + \norm{\pi(\bfv) - \bfw}^2 \geq \norm{\bfv - \pi(\bfv)}^2
	\end{align*}
	with equality if and only if \( \bfw = \pi(\bfv) \).
\end{proof}

\subsection{Adjoint maps}
\ \vspace*{-1.5em}
\begin{definition}
	Let \( V, W \) be finite-dimensional inner product spaces.
	Let \( \alpha \in \mcL(V, W) \).
	Then there exists a unique linear map \( \alpha^* \colon W \to V \) such that for all \( \bfv, \bfw \in V, W \),
	\[
		\inner{\alpha(\bfv), \bfw} = \inner{\bfv, \alpha^*(\bfw)}
	\]
	Moreover, if \( B \) is an orthonormal basis of \( V \), and \( C \) is an orthonormal basis of \( W \), then
	\[
		[\alpha^*]_{C, B} = \qty(\overline{[\alpha]_{B, C}})^\top = [\alpha]_{B, C}^\dagger
	\]
\end{definition}
\begin{proof}
	Let \( B = (\bfv_1, \dots, \bfv_n) \) and \( C = (\bfw_1, \dots, \bfw_m) \) and \( A = [\alpha]_{B, C} = (a_{ij}) \).
	To check existence, we define \( [\alpha^*]_{C, B} = \overline{A}^\top = (c_{ij}) \) and explicitly check the definition.
	By orthogonality,
	\[
		\inner{\alpha\qty(\sum \lambda_i \bfv_i), \sum \mu_j \bfw_j} = \inner{\sum_{i,k} \lambda_i a_{ki} \bfw_k, \sum_j \mu_j \bfw_j} = \sum_{i,j} \lambda_i a_{ji} \overline{\mu_j}
	\]
	Then,
	\[
		\inner{\sum \lambda_i \bfv_i, \alpha^*\qty(\sum \mu_j \bfw_j)} = \inner{\sum_i \lambda_i \bfv_i, \sum_{j,k} \mu_j c_{kj} \bfv_k} = \sum_{i,j} \lambda_i \overline{c_{ij}} \overline{\mu_j}
	\]
	So equality requires \( \overline{c_{ij}} = a_{ji} \).
	Uniqueness follows from the above; the expansions are equivalent for any vector if and only if \( \overline{c_{ij}} = a_{ji} \).
\end{proof}

\begin{remark}
	The same notation, \( \alpha^* \), is used for the adjoint as just defined, and the dual map as defined before.

	Let $ V,W $ be real inner product spaces. Let $ \alpha\in \mathcal{L}(V,W) $ and define 
	\[
		\psi_{R,V}: \begin{matrix}
			 V \xrightarrow{\cong} V^*\\
			 \mathbf{v} \mapsto \inner{\cdot, \mathbf{v}} \\
		\end{matrix},\quad \psi_{R,W}: \begin{matrix}
			 W \to W^*  \\
			 \mathbf{w} \mapsto \inner{\cdot,\mathbf{w}} \\
		\end{matrix}
	\]
	Then the adjoint of $\alpha$ is given by $ \alpha^* = \psi_{R,V}^{-1} \alpha^* \psi_{R,W}  $, here $\alpha^* $ on RHS is the dual. 
	\[
		W \xrightarrow[\psi_{R,W}]{} W^* \xrightarrow[\alpha^*]{} V^* \xrightarrow[\psi_{R,V}^{-1}]{} V.
	\]
\end{remark}

\subsection{Self-adjointness and isometries}
\ \vspace*{-1.5em}
\begin{definition}
	Let \( V \) be a finite-dimensional inner product space, and \( \alpha \) be an endomorphism of \( V \).
	Let \( \alpha^* \in \mcL(V) \) be the adjoint map.
	Then,
	\begin{enumerate}[(i)]
		\item the condition \( \inner{\alpha \bfv, \bfw} = \inner{\bfv, \alpha \bfw} \) is equivalent to the condition \( \alpha = \alpha^* \), and such an \( \alpha \) is called \textit{self-adjoint} (for \( \mathbb R \) we call such endomorphisms \textbf{symmetric}, and for \( \mathbb C \) we call such endomorphisms Hermitian);
		\item the condition \( \inner{\alpha \bfv, \alpha \bfw} = \inner{\bfv, \bfw} \) is equivalent to the condition \( \alpha^* = \alpha^{-1} \), and such an \( \alpha \) is called an \textbf{isometry} (for \( \mathbb R \) it is called \textbf{orthogonal}, and for \( \mathbb C \) it is called \textbf{unitary}).
	\end{enumerate}
\end{definition}
\begin{proposition}
	The conditions for isometries defined as above are equivalent.
\end{proposition}
\begin{proof}
	Suppose \( \inner{\alpha \bfv, \alpha \bfw} = \inner{\bfv,\bfw} \).
	Then for \( \bfv = \bfw \), we find \( \norm{\alpha \bfv}^2 = \norm{\bfv}^2 \), so \( \alpha \) preserves the norm.
	In particular, this implies \( \ker \alpha = \qty{0} \).
	Since \( \alpha \) is an endomorphism and \( V \) is finite-dimensional, \( \alpha \) is bijective.
	Then for all \( \bfv, \bfw \in V \),
	\[
		\inner{\bfv, \alpha^*(\bfw)} = \inner{\alpha \bfv, \bfw} = \inner{\alpha \bfv, \alpha\qty(\alpha^{-1}(\bfw))} = \inner{\bfv, \alpha^{-1}(\bfw)}
	\]
	Hence \( \alpha^* = \alpha^{-1} \).
	Conversely, if \( \alpha^* = \alpha^{-1} \) we have
	\[
		\inner{\alpha \bfv, \alpha \bfw} = \inner{\bfv, \alpha^*(\alpha \bfw)} = \inner{\bfv, \bfw}
	\]
	as required.
\end{proof}
\begin{remark}
	Using the polarisation identity, we can show that \( \alpha \) is isometric if and only if for all \( \bfv \in V \), \( \norm{\alpha(\bfv)} = \norm{\bfv} \).
\end{remark}
\begin{lemma}
	Let \( V \) be a finite-dimensional real (or complex) inner product space.
	Then for \( \alpha \in \mcL(V) \),
	\begin{enumerate}[(i)]
		\item \( \alpha \) is self-adjoint if and only if for all orthonormal bases \( B \) of \( V \), we have \( [\alpha]_B \) is symmetric (or Hermitian);
		\item \( \alpha \) is an isometry if and only if for all orthonormal bases \( B \) of \( V \), we have \( [\alpha]_B \) is orthogonal (or unitary).
	\end{enumerate}
\end{lemma}
\begin{proof}
	Let \( B \) be an orthonormal basis for \( V \).
	Then we know \( [\alpha^*]_B = [\alpha]_B^\dagger \).
	We can then check that \( [\alpha]_B^\dagger = [\alpha]_B \) and \( [\alpha]_B^\dagger = [\alpha]_B^{-1} \) respectively.
\end{proof}
\begin{definition}
	For \( F = \mathbb R \), we define the \textbf{orthogonal group} of \( V \) by
	\[
		O(V) = \qty{ \alpha \in L(V) \colon \alpha \text{ is an isometry} }
	\]
	Note that \( O(V) \) is bijective with the set of orthogonal bases of \( V \).
	For \( F = \mathbb C \), we define the \textbf{unitary group} of \( V \) by
	\[
		U(V) = \qty{ \alpha \in L(V) \colon \alpha \text{ is an isometry} }
	\]
	Again, note that \( U(V) \) is bijective with the set of orthogonal bases of \( V \).
\end{definition}

Let $V$ be a finite-dimensional real inner product space and $(\mathbf{e}_1, \cdots, \mathbf{e}_n)$ is an orthonormal basis of $V$. Then there is a bijection
\begin{align*}
O(V) &\to \{\text{orthonormal basis for }V\}\\
\alpha &\mapsto (\alpha(\mathbf{e}_1, \cdots, \mathbf{e}_n)).
\end{align*}

Let $V$ be a finite-dimensional complex inner product space. Then there is a bijection
\begin{align*}
U(V) &\to \{\text{orthonormal basis of } V\}\\
\alpha &\mapsto \{\alpha(\mathbf{e}_1), \cdots, \alpha (\mathbf{e}_n)\}.
\end{align*}

\subsection{Spectral theory}

Spectral theory is the study of the spectrum of operators.
Recall that in finite-dimensional inner product spaces \( V, W \), \( \alpha \in \mcL(V, W) \) yields the adjoint \( \alpha^* \in \mcL(W, V) \) such that for all \( \bfv \in V, \bfw \in W \), we have \( \inner{\alpha(\bfv), \bfw} = \inner{\bfv, \alpha^*(\bfw)} \).
\begin{lemma}
	Let \( V \) be a finite-dimensional inner product space.
	Let \( \alpha \in \mcL(V) \) be a self-adjoint endomorphism.
	Then \( \alpha \) has real eigenvalues, and eigenvectors of \( \alpha \) with respect to different eigenvalues are orthogonal.
\end{lemma}
\begin{proof}
	Suppose \( \lambda \in \mathbb C \), \( \bfv \in V \) nonzero such that \( \alpha(\bfv) = \lambda \bfv \).
	Then, \( \inner{\lambda \bfv, \bfv} = \lambda \norm{\bfv}^2 \) and also
	\[
		\inner{\alpha \bfv, \bfv} = \inner{\bfv, \alpha \bfv} = \inner{\bfv, \lambda \bfv} = \overline{\lambda} \norm{\bfv}^2
	\]
	Hence \( \lambda = \overline{\lambda} \) since \( \bfv \neq 0 \).
	Now, suppose \( \mu \neq \lambda \) and \( \bfw \in V \) nonzero such that \( \alpha(\bfw) = \mu \bfw \).
	Then,
	\[
		\lambda \inner{\bfv, \bfw} = \inner{\alpha \bfv, \bfw} = \inner{\bfv, \alpha \bfw} = \overline{\mu} \inner{\bfv, \bfw} = \mu \inner{\bfv,\bfw}
	\]
	So if \( \lambda \neq \mu \) we must have \( \inner{\bfv,\bfw} = 0 \).
\end{proof}
\begin{theorem}[Spectral theorem for self-adjoint maps]
	Let \( V \) be a finite-dimensional inner product space.
	Let \( \alpha \in \mcL(V) \) be self-adjoint.
	Then \( V \) has an orthonormal basis of eigenvectors of \( \alpha \).
	Hence \( \alpha \) is diagonalisable in an orthonormal basis.
\end{theorem}
\begin{proof}
	We argue by induction on the dimension of $V$. 
	\begin{itemize}
		\item $n=1$, this is obvious. 
		\item $ n-1\to n $. Suppose \( A = [\alpha]_B \) with respect to the fundamental basis \( B \). By the fundamental theorem of algebra, we know that \( \chi_A(\lambda) \) has a (complex) root.
		But since \( \lambda \) is an eigenvalue of \( \alpha \) and \( \alpha \) is self-adjoint, \( \lambda \in \mathbb R \).

		Now, we choose an eigenvector \( \bfv_1 = V \setminus \qty{0} \) such that \( \alpha(\bfv_1) = \lambda \bfv_1 \).
		We can set \( \norm{\bfv_1} = 1 \) by linearity.
		Let \( U = \genset{\bfv_1}^\perp \leq V \).
		We then observe that \( U \) is stable by \( \alpha \); \( \alpha(U) \leq U \).
		Indeed, let \( \bfu \in U \).
		Then \( \inner{\alpha(\bfu), \bfv_1} = \inner{\bfu, \alpha(\bfv_1)} = \lambda \inner{\bfu, \bfv_1} = 0 \) by orthogonality.
		Hence \( \alpha(\bfu) \in U \).

		We can then restrict \( \alpha \) to the domain \( U \). Note that by construction, $ \dim U = \dim V - 1 $, and by induction we can then choose an orthonormal basis of eigenvectors for \( U \).
		Since \( V = \genset{\bfv_1} \overset{\perp}{\oplus} U \) we have an orthonormal basis of eigenvectors for \( V \) when including \( \bfv_1 \).\qedhere
	\end{itemize}
\end{proof}
\begin{corollary}
	Let \( V \) be a finite-dimensional inner product space.
	Let \( \alpha \in \mcL(V) \) be self-adjoint.
	Then \( V \) is the orthogonal direct sum of the eigenspaces of \( \alpha \).
\end{corollary}

\subsection{Spectral theory for unitary maps}
\ \vspace*{-1.5em}
\begin{lemma}
	Let \( V \) be a complex inner product space.
	Let \( \alpha \) be unitary, so \( \alpha^* = \alpha^{-1} \).
	Then all eigenvalues of \( \alpha \) have unit norm.
	Eigenvectors corresponding to different eigenvalues are orthogonal.
\end{lemma}
\begin{proof}
	Let \( \lambda \in \mathbb C \), \( \bfv \in V \setminus \qty{0} \) such that \( \alpha(\bfv) = \lambda \bfv \).
	First, \( \lambda \neq 0 \) since \( \alpha \) is invertible, and in particular \( \ker \alpha = \qty{0} \).
	Since \( \bfv = \lambda \alpha^{-1}(\bfv) \), we can compute
	\[
		\lambda \inner{\bfv,\bfv} = \inner{\lambda \bfv, \bfv} = \inner{\alpha \bfv, \bfv} = \inner{\bfv, \alpha^{-1} \bfv} = \inner{\bfv, \frac{1}{\lambda} \bfv} = \frac{1}{\overline \lambda} \inner{\bfv, \bfv}
	\]
	Hence \( (\lambda \overline \lambda - 1) \norm{\bfv}^2 = 0 \) giving \( \abs{\lambda} = 1 \).

	Further, suppose \( \mu \in \mathbb C \) and \( \bfw \in V \setminus \qty{0} \) such that \( \alpha(\bfw) = \mu \bfw, \lambda \neq \mu \).
	Then
	\[
		\lambda \inner{\bfv,\bfw} = \inner{\lambda \bfv, \bfw} = \inner{\alpha \bfv, \bfw} = \inner{\bfv, \alpha^{-1} \bfw} = \inner{\bfv, \frac{1}{\mu} \bfw} = \frac{1}{\overline \mu} \inner{\bfv,\bfw} = \mu \inner{\bfv,\bfw}
	\]
	since \( \mu \overline \mu = 1 \). So $ \langle \mathbf{v},\mathbf{w} \rangle = 0 $.
\end{proof}
\begin{theorem}[spectral theorem for unitary maps]
	Let \( V \) be a finite-dimensional complex inner product space.
	Let \( \alpha \in \mcL(V) \) be unitary.
	Then \( V \) has an orthonormal basis of eigenvectors of \( \alpha \).
	Hence \( \alpha \) is diagonalisable in an orthonormal basis.
\end{theorem}
\begin{proof}
	Let \( A = [\alpha]_B \) where \( B \) is an orthonormal basis.
	Then \( \chi_A(\lambda) \) has a complex root \( \lambda \).
	As before, let \( \bfv_1 \neq 0 \) such that \( \alpha(\bfv_1) = \lambda \bfv_1 \) and \(\norm{\bfv_1} = 1 \).
	Let \( U = \genset{\bfv_1}^\perp \), and we claim that \( \alpha(U) = U \).
	Indeed, let \( \bfu \in U \), and we find
	\[
		\inner{\alpha(\bfu), \bfv_1} = \inner{\bfu, \alpha^{-1}(\bfv_1)} = \inner{\bfu, \frac{1}{\lambda} \bfv_1} = \frac{1}{\overline \lambda} \inner{\bfu,\bfv_1}
	\]
	Since \( \inner{\bfu, \bfv_1} = 0 \), we have \( \alpha(\bfu) \in U \).
	Hence, \( \alpha \) restricted to \( U \) is a unitary endomorphism of \( U \).
	By induction we have an orthonormal basis of eigenvectors of \( \alpha \) for \( U \) and hence for \( V \).
\end{proof}
\begin{remark}
	We used the fact that the field is complex to find an eigenvalue.
	In general, a real-valued orthonormal matrix \( A \) giving \( A A^\top = I \) cannot be diagonalised over \( \mathbb R \).
	For example, consider
	\[
		A = \begin{pmatrix}
			\cos\theta & -\sin\theta \\
			\sin\theta & \cos\theta
		\end{pmatrix}
	\]
	This is orthogonal and normalised.
	However, \( \chi_A(\lambda) = 1 + 2\lambda \cos\theta + \lambda^2 \) hence \( \lambda = e^{\pm i \theta} \) which are complex in the general case.
\end{remark}

\subsection{Application to bilinear forms}
We wish to extend the previous statements about spectral theory into statements about bilinear forms.
\begin{corollary}
	Let \( A \in \mcM_n(\mathbb R) \) (or \( \mcM_n(\mathbb C) \)) be a symmetric (or respectively Hermitian) matrix.
	Then there exists an orthonormal (respectively unitary) matrix \( P \) such that \( P^\top A P \) (or \( P^\dagger A P \)) is diagonal with real-valued entries.
\end{corollary}
\begin{proof}
	Using the standard inner product, \( A \in \mcL(\bbF^n) \) is self-adjoint and hence there exists an orthonormal basis \( B \) of \( F^n \) such that \( A \) is diagonal in this basis.
	Let \( P = (\bfv_1, \dots, \bfv_n) \) be the matrix of this basis.
	Since \( B \) is orthonormal, \( P \) is orthogonal (or unitary).
	The result follows from the fact that \( P^{-1} A P \) is diagonal.
	The eigenvalues are real, hence the diagonal matrix is real.
\end{proof}
\begin{corollary}
	Let \( V \) be a finite-dimensional real (or complex) inner product space.
	Let \( \phi \colon V \times V \to \bbF \) be a symmetric (or Hermitian) bilinear form.
	Then, there exists an orthonormal basis \( B \) of \( V \) such that \( [\phi]_B \) is diagonal.
\end{corollary}
\begin{proof}
	\( A^\top = A \) (or respectively \( A^\dagger = A \)), hence there exists an orthogonal (respectively unitary) matrix \( P \) such that \( P^{-1} A P \) is diagonal.
	Let \( (v_i) \) be the \( i \)th row of \( P^{-1} = P^\top \) (or \( P^\dagger \)).
	Then \( (\bfv_1, \dots, \bfv_n) \) is an orthonormal basis \( B \) of \( V \) such that \( [\phi]_V \) is this diagonal matrix.
\end{proof}
\begin{remark}
	The diagonal entries of \( P^{-1} A P \) are the eigenvalues of \( A \).
	Moreover, we can define the signature \( s(\phi) \) to be the difference between the number of positive eigenvalues of \( A \) and the number of negative eigenvalues of \( A \).
\end{remark}

\subsection{Simultaneous diagonalisation}
\ \vspace*{-1.5em}
\begin{corollary}
	Let \( V \) be a finite-dimensional real (or complex) vector space.
	Let \( \phi, \psi \) be symmetric (or Hermitian) bilinear forms on \( V \).
	Let \( \phi \) be positive definite.
	Then there exists a basis \( (\bfv_1, \dots, \bfv_n) \) of \( V \) with respect to which \( \phi \) and \( \psi \) are represented with a diagonal matrix.
\end{corollary}
\begin{proof}
	Since \( \phi \) is positive definite, \( V \) equipped with \( \phi \) is a finite-dimensional inner product space where \( \inner{\bfu,\bfv} = \phi(\bfu,\bfv) \).
	Hence, there exists a basis of \( V \) in which \( \psi \) is represented by a diagonal matrix, which is orthonormal with respect to the inner product defined by \( \phi \).
	Then, \( \phi \) in this basis is represented by the identity matrix given by \( \phi(\bfv_i, \bfv_j) = \inner{\bfv_i, \bfv_j} = \delta_{ij} \), which is diagonal.
\end{proof}
\begin{corollary}
	Let \( A, B \in \mcM_n(\mathbb R) \) (or \( \mathbb C \)) which are symmetric (or Hermitian).
	Suppose for all \( \bfx \neq 0 \) we have \( \bfx^\dagger A \bfx > 0 \), so \( A \) is positive definite.
	Then there exists an invertible matrix \( Q \in M_n(\mathbb R) \) (or \( \mathbb C \)) such that \( Q^\top A Q \) (or \( Q^\top A \overline{Q} \)) and \( Q^\top B Q \) (or \( Q^\top B \overline{Q} \)) are diagonal.
\end{corollary}
\begin{proof}
	\( A \) induces a quadratic form \( Q(\bfx) = x^\dagger A x \) which is positive definite by assumption.
	Similarly, \( \widetilde Q(\bfx) = \bfx^\dagger B \bfx \) is induced by \( B \).
	Then we can apply the previous corollary and change basis.
\end{proof}

\end{document}